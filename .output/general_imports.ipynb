{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052e3eb4",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [7]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad9b11",
   "metadata": {
    "papermill": {
     "duration": 0.06575,
     "end_time": "2021-10-22T12:23:59.340251",
     "exception": false,
     "start_time": "2021-10-22T12:23:59.274501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## general_import.py transposed to a .ipynb file for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8a44c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T12:23:59.442960Z",
     "iopub.status.busy": "2021-10-22T12:23:59.442124Z",
     "iopub.status.idle": "2021-10-22T12:23:59.446004Z",
     "shell.execute_reply": "2021-10-22T12:23:59.446517Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.059003,
     "end_time": "2021-10-22T12:23:59.446813",
     "exception": false,
     "start_time": "2021-10-22T12:23:59.387810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e202d2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T12:23:59.540836Z",
     "iopub.status.busy": "2021-10-22T12:23:59.540345Z",
     "iopub.status.idle": "2021-10-22T12:24:00.682369Z",
     "shell.execute_reply": "2021-10-22T12:24:00.683411Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.1905,
     "end_time": "2021-10-22T12:24:00.684031",
     "exception": false,
     "start_time": "2021-10-22T12:23:59.493531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mnanoHUB - Serving Students, Researchers & Instructors\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained Salesforce access token ...... True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from nanoHUB.application import Application\n",
    "from googleapiclient.discovery import build\n",
    "from apiclient import errors\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from httplib2 import Http\n",
    "from nanoHUB.logger import logger\n",
    "application = Application.get_instance()\n",
    "# nanohub_db = application.new_db_engine('nanohub')\n",
    "# nanohub_metrics_db = application.new_db_engine('nanohub_metrics')\n",
    "# wang159_myrmekes_db = application.new_db_engine('wang159_myrmekes')\n",
    "\n",
    "salesforce = application.new_salesforce_engine()\n",
    "db_s = salesforce\n",
    "log = logger('nanoHUB:google_imports')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551cd45",
   "metadata": {
    "papermill": {
     "duration": 0.050123,
     "end_time": "2021-10-22T12:24:00.793676",
     "exception": false,
     "start_time": "2021-10-22T12:24:00.743553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup GDrive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee9aacd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T12:24:00.892253Z",
     "iopub.status.busy": "2021-10-22T12:24:00.891308Z",
     "iopub.status.idle": "2021-10-22T12:24:00.893272Z",
     "shell.execute_reply": "2021-10-22T12:24:00.893644Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.05212,
     "end_time": "2021-10-22T12:24:00.893879",
     "exception": false,
     "start_time": "2021-10-22T12:24:00.841759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import os\n",
    "import errno\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "## stuff that's rather hard to find from documentation\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f19ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T12:24:00.995865Z",
     "iopub.status.busy": "2021-10-22T12:24:00.995332Z",
     "iopub.status.idle": "2021-10-22T12:24:01.005682Z",
     "shell.execute_reply": "2021-10-22T12:24:01.006198Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.06472,
     "end_time": "2021-10-22T12:24:01.006438",
     "exception": false,
     "start_time": "2021-10-22T12:24:00.941718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'Salesforce_Imports'\n",
    "TO_IMPORT_FOLDER_NAME = 'To_Import'\n",
    "FAILURES_FOLDER_NAME = 'Import_Issues'\n",
    "IMPORTED_FOLDER_NAME = 'Imported'\n",
    "\n",
    "service = application.new_google_api_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34db7533",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-22T12:24:01.106877Z",
     "iopub.status.busy": "2021-10-22T12:24:01.106322Z",
     "iopub.status.idle": "2021-10-22T12:24:01.108398Z",
     "shell.execute_reply": "2021-10-22T12:24:01.108943Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.055031,
     "end_time": "2021-10-22T12:24:01.109139",
     "exception": false,
     "start_time": "2021-10-22T12:24:01.054108",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_dir = os.getenv('APP_DIR') + '/.cache/SF_Imports'\n",
    "Path(import_dir + '/' + FOLDER_NAME).mkdir(parents=True, exist_ok=True)\n",
    "Path(import_dir + '/' + TO_IMPORT_FOLDER_NAME).mkdir(parents=True, exist_ok=True)\n",
    "Path(import_dir + '/' + FAILURES_FOLDER_NAME).mkdir(parents=True, exist_ok=True)\n",
    "Path(import_dir + '/' + IMPORTED_FOLDER_NAME).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9596545",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-22T12:24:01.213617Z",
     "iopub.status.busy": "2021-10-22T12:24:01.212910Z",
     "iopub.status.idle": "2021-10-22T12:24:01.215367Z",
     "shell.execute_reply": "2021-10-22T12:24:01.214962Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.057476,
     "end_time": "2021-10-22T12:24:01.215671",
     "exception": false,
     "start_time": "2021-10-22T12:24:01.158195",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_folder_id(service, folder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + folder_name + \"'\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def get_subfolder_id(service, parent_folder_id: str, subfolder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + subfolder_name + \"'\" + \" and '\" + parent_folder_id + \"' in parents\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def change_folder_for_file(file_id: str, old_folder_id: str, new_folder_id: str):\n",
    "    return service.files().update(\n",
    "        fileId=file_id,\n",
    "        removeParents=old_folder_id,\n",
    "        addParents=new_folder_id,\n",
    "        fields='id, parents'\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a23b07",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c012fc",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-22T12:24:01.321516Z",
     "iopub.status.busy": "2021-10-22T12:24:01.320913Z",
     "iopub.status.idle": "2021-10-22T12:25:01.573663Z",
     "shell.execute_reply": "2021-10-22T12:25:01.574212Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 60.309762,
     "end_time": "2021-10-22T12:25:01.574449",
     "exception": true,
     "start_time": "2021-10-22T12:24:01.264687",
     "status": "failed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "timeout",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mtimeout\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/64/2zrjfqsn7x3fxjzpq46kqcyr0000gp/T/ipykernel_6093/191429788.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mfolder_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_folder_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mservice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFOLDER_NAME\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Found folder: %s (%s)'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mFOLDER_NAME\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfolder_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/64/2zrjfqsn7x3fxjzpq46kqcyr0000gp/T/ipykernel_6093/2836040266.py\u001B[0m in \u001B[0;36mget_folder_id\u001B[0;34m(service, folder_name)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mget_folder_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mservice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfolder_name\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     response = service.files().list(\n\u001B[0m\u001B[1;32m      3\u001B[0m         \u001B[0mq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"mimeType = 'application/vnd.google-apps.folder' and name = '\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mfolder_name\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"'\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mspaces\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'drive'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mfields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"files(id, name)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/googleapiclient/_helpers.py\u001B[0m in \u001B[0;36mpositional_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    129\u001B[0m                 \u001B[0;32melif\u001B[0m \u001B[0mpositional_parameters_enforcement\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mPOSITIONAL_WARNING\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m                     \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 131\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mpositional_wrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/googleapiclient/http.py\u001B[0m in \u001B[0;36mexecute\u001B[0;34m(self, http, num_retries)\u001B[0m\n\u001B[1;32m    920\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    921\u001B[0m         \u001B[0;31m# Handle retries for server-side errors.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 922\u001B[0;31m         resp, content = _retry_request(\n\u001B[0m\u001B[1;32m    923\u001B[0m             \u001B[0mhttp\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    924\u001B[0m             \u001B[0mnum_retries\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/googleapiclient/http.py\u001B[0m in \u001B[0;36m_retry_request\u001B[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001B[0m\n\u001B[1;32m    219\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mexception\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mretry_num\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mnum_retries\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 221\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mexception\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    222\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m                 \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/googleapiclient/http.py\u001B[0m in \u001B[0;36m_retry_request\u001B[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001B[0m\n\u001B[1;32m    188\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m             \u001B[0mexception\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0mresp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhttp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;31m# Retry on SSL errors and socket timeout errors.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0m_ssl_SSLError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mssl_error\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/google_auth_httplib2.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    217\u001B[0m         \u001B[0;31m# Make the request.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 218\u001B[0;31m         response, content = self.http.request(\n\u001B[0m\u001B[1;32m    219\u001B[0m             \u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m             \u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/httplib2/__init__.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, uri, method, body, headers, redirections, connection_type)\u001B[0m\n\u001B[1;32m   1709\u001B[0m                     \u001B[0mcontent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mb\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1710\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1711\u001B[0;31m                     (response, content) = self._request(\n\u001B[0m\u001B[1;32m   1712\u001B[0m                         \u001B[0mconn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauthority\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequest_uri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mredirections\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcachekey\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1713\u001B[0m                     )\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/httplib2/__init__.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001B[0m\n\u001B[1;32m   1425\u001B[0m             \u001B[0mauth\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequest_uri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1426\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1427\u001B[0;31m         \u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontent\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conn_request\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequest_uri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1428\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1429\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mauth\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/httplib2/__init__.py\u001B[0m in \u001B[0;36m_conn_request\u001B[0;34m(self, conn, request_uri, method, body, headers)\u001B[0m\n\u001B[1;32m   1347\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1348\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msock\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1349\u001B[0;31m                     \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1350\u001B[0m                 \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequest_uri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1351\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0msocket\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/nanoHUB/lib/python3.9/site-packages/httplib2/__init__.py\u001B[0m in \u001B[0;36mconnect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1137\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mhas_timeout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1138\u001B[0m                     \u001B[0msock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msettimeout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1139\u001B[0;31m                 \u001B[0msock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhost\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mport\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1140\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1141\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msock\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrap_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mserver_hostname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhost\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mtimeout\u001B[0m: timed out"
     ]
    }
   ],
   "source": [
    "folder_id = get_folder_id(service, FOLDER_NAME)\n",
    "log.info('Found folder: %s (%s)' % (FOLDER_NAME, folder_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66bab9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subfolder_id = get_subfolder_id(service, folder_id, TO_IMPORT_FOLDER_NAME)\n",
    "log.info('Found subfolder: %s (%s)' % (TO_IMPORT_FOLDER_NAME, subfolder_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2330fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mimetypes = \"\"\"\n",
    "mimeType = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' or\n",
    "mimeType = 'application/vnd.ms-excel' or\n",
    "mimeType = 'text/csv'\n",
    "\"\"\"\n",
    "file_ids = []\n",
    "file_names = []\n",
    "page_token = None\n",
    "# q = \"(\" + mimetypes + \") and '\" + folder.get('id') + \"' in parents\"\n",
    "while True:\n",
    "    query = \"(\" + mimetypes + \") and '\" + subfolder_id + \"' in parents\"\n",
    "    response = service.files().list(\n",
    "        q = query,\n",
    "        spaces='drive',\n",
    "        pageToken=page_token,\n",
    "        fields=\"nextPageToken, files(id, name)\"\n",
    "    ).execute()\n",
    "    files = response.get('files', [])\n",
    "    for file in files:\n",
    "        log.info('Found file: %s (%s)' % (file.get('name'), file.get('id')))\n",
    "        file_names.append(file['name'])\n",
    "        file_ids.append(file['id'])\n",
    "    page_token = response.get('nextPageToken', None)\n",
    "    if page_token is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e89aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io \n",
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd186c41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for temp_index,f_tbd_id in enumerate(file_ids):\n",
    "        request = service.files().get_media(fileId=f_tbd_id) #,mimeType='text/csv') #if not .csv, then do .export()\n",
    "        fh = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "            log.info(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "\n",
    "        # The file has been downloaded into RAM, now save it in a file\n",
    "        # https://stackoverflow.com/questions/60111361/how-to-download-a-file-from-google-drive-using-python-and-the-drive-api-v3\n",
    "        fh.seek(0)\n",
    "        download_filepath = import_dir + '/' + file_names[temp_index]\n",
    "        if not os.path.exists(os.path.dirname(download_filepath)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(download_filepath))\n",
    "            except OSError as exc:\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "        with open(download_filepath, 'wb') as f:\n",
    "            log.info(f)\n",
    "            shutil.copyfileobj(fh, f) #, length=131072)\n",
    "\n",
    "    log.info(\"Finished downloading files\")\n",
    "except Exception as e:\n",
    "    log.info(\"Error downloading file: \" + str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd30a1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73e05f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_files = [name for name in os.listdir(import_dir)] #if os.path.isfile(name)]\n",
    "log.info(list_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded9a96",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Perform Sequential Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb4593",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0bcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_files = []\n",
    "fail_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc63220",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chardet import detect\n",
    "import cchardet\n",
    "# get file encoding type\n",
    "def get_encoding_type(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a5bfe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from charset_normalizer import CharsetNormalizerMatches as CnM\n",
    "\n",
    "\n",
    "log.info(\"\\n\")\n",
    "for file in file_names:\n",
    "    file_path = import_dir + '/' + file\n",
    "    log.info(\"Processing file: \" + file + '---->')\n",
    "\n",
    "    from_chardet = get_encoding_type(file_path)\n",
    "    log.info('From chardet ----> ' + from_chardet['encoding'])\n",
    "\n",
    "    from_normalizer = CnM.from_path(file_path).best().first().encoding\n",
    "    log.info('From Normalizer ----> ' + from_normalizer)\n",
    "\n",
    "\n",
    "\n",
    "from charset_normalizer import from_path\n",
    "\n",
    "def change_encoding(file_path: str):\n",
    "    try:\n",
    "        results = from_path(file_path)\n",
    "        with open(file_path, 'w') as filetowrite:\n",
    "            filetowrite.write(results)\n",
    "    except IOError as e:\n",
    "        log.info('Sadly, we are unable to perform charset normalization.', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b18aaf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for file in file_names:\n",
    "    idf = pd.DataFrame()\n",
    "    log.info(\"Reading %s\" % file)\n",
    "    try:\n",
    "        f_type = file.split('.')[-1]\n",
    "        log.info(\"File type %s\" % f_type)\n",
    "        #pandas import # add exception handling - UnicodeError\n",
    "        if f_type == 'csv':\n",
    "            try:\n",
    "                idf = pd.read_csv(import_dir + '/' +file,encoding='utf-8')\n",
    "                log.info('Loaded csv with encoding utf-8.')\n",
    "            except:\n",
    "                try:\n",
    "                    idf = pd.read_csv(import_dir + '/' +file,encoding='cp1252')\n",
    "                    log.info('Loaded csv with encoding cp1252 now.')\n",
    "                except:\n",
    "                    try:\n",
    "                        idf = pd.read_csv(import_dir + '/' +file,encoding='utf-16')\n",
    "                        log.info('Loaded csv with encoding utf-16 now.')\n",
    "                    except:\n",
    "                        try:\n",
    "                            idf = pd.read_csv(import_dir + '/' +file,encoding='cp1254')\n",
    "                            log.info('Loaded csv with encoding cp1254 now.')\n",
    "                        except:\n",
    "                            try:\n",
    "                                idf = pd.read_csv(import_dir + '/' +file,encoding='cp775')\n",
    "                                log.info('Loaded csv with encoding cp775 now.')\n",
    "                            except:\n",
    "                                try:\n",
    "                                    idf = pd.read_csv(import_dir + '/' +file,encoding='cp1252',sep='\\t')\n",
    "                                    log.info('Loaded csv with encoding cp1252 now.')\n",
    "                                except:\n",
    "                                    try:\n",
    "                                        idf = pd.read_csv(import_dir + '/' +file,encoding='utf-16',sep='\\t')\n",
    "                                        log.info('Loaded csv with encoding utf-16 now.')\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            idf = pd.read_csv(import_dir + '/' +file,encoding='cp1254',sep='\\t')\n",
    "                                            log.info('Loaded csv with encoding cp1254 now.')\n",
    "                                        except:\n",
    "                                            try:\n",
    "                                                idf = pd.read_csv(import_dir + '/' +file,encoding='cp775',sep='\\t')\n",
    "                                                log.info('Loaded csv with encoding cp775 now.')\n",
    "                                            except:\n",
    "                                                log.info('CSV import failed. Possible encoding issues with file %s.' % file)\n",
    "                                                raise TypeError\n",
    "        elif f_type == 'xlsx' or f_type == 'xls':\n",
    "            try:\n",
    "                xl = pd.ExcelFile(import_dir + '/' +file)\n",
    "                log.info(\"sheet names: \" + xl.sheet_names)# see all sheet names\n",
    "                sheet_names = xl.parse(xl.sheet_names) #this already performs an import\n",
    "                idf = pd.read_excel(import_dir + '/' +file,sheet_name=xl.sheet_names[0],header=0)#,skiprows=1)\n",
    "            except:\n",
    "                log.info('error bad lines, csv/xls/xlsx import failed')\n",
    "                raise TypeError\n",
    "\n",
    "        ## remove leading and trailing spaces ## add str.strip spaces for all columns and rename\n",
    "        log.info(idf.columns)\n",
    "        prev_idf_cols = idf.columns\n",
    "        idf_cols = [i.strip() for i in idf.columns]\n",
    "        idf.columns = idf.columns.str.strip()\n",
    "        log.info(idf.columns)\n",
    "        idf = idf.rename(columns={j:idf_cols[i] for i,j in enumerate(prev_idf_cols)})\n",
    "    #     display(idf.columns)\n",
    "    #     # log.info(prev_idf_cols)\n",
    "    #     # log.info(idf_cols)\n",
    "\n",
    "        # if engagement venue does not exist, then create a flag with that entry\n",
    "        try:\n",
    "            #if engagement venue is specified\n",
    "            log.info(idf['Engagement Venue'])\n",
    "            log.info(idf['First Name'])\n",
    "            idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "            # idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "        except:\n",
    "            #decide the event\n",
    "            #event_extract = xl.sheet_names[0]\n",
    "            #idf['Engagement Ve nue'] = event_extract\n",
    "            idf['Venue__c'] = file.split('.')[0]\n",
    "\n",
    "            try:\n",
    "                #name extract\n",
    "                names = idf['Name'].to_list()\n",
    "                from copy import deepcopy?\n",
    "                fname = deepcopy(names)\n",
    "                lname = deepcopy(names)\n",
    "                for ind, val in enumerate(names):\n",
    "                    val = val.split(' ')\n",
    "                    fname[ind] = val[-1]\n",
    "                    lname[ind] = val[0]\n",
    "\n",
    "                idf['firstname'] = fname\n",
    "                idf['lastname'] = lname\n",
    "                name_flag = True\n",
    "            except:\n",
    "                name_flag = False\n",
    "                log.info('no name')\n",
    "\n",
    "        #rename columns\n",
    "        idf = idf.rename(columns={'email':'Email','EMAIL':'Email','E-mail Address':'Email',\\\n",
    "                    'Email Address':'Email','Recipient Email':'Email'})\n",
    "        idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "        idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname','FirstName':'firstname','LastName':'lastname'})\n",
    "\n",
    "        idf = idf.rename(columns={0:'Email'})\n",
    "        if name_flag == True:\n",
    "            idf = idf.drop(columns='Name')#['NAME','LAST NAME','FIRST NAME'])            \n",
    "        idf = idf.dropna(subset=['Email'])\n",
    "\n",
    "        log.info(idf.columns)\n",
    "        #email check rows\n",
    "        grows = []\n",
    "        brows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            if '@' in val:\n",
    "                grows.append(ind)\n",
    "            else:\n",
    "                brows.append(ind)    \n",
    "        idf = idf.iloc[grows,:].reset_index().iloc[:,1:]        \n",
    "\n",
    "        log.info(len(grows))\n",
    "        log.info(len(brows))\n",
    "\n",
    "        ## Import in contacts\n",
    "        os_name = os.name\n",
    "        sys_name = platform.system() #Linux, Darwin, Windows    \n",
    "\n",
    "        # salesforce queries for contact data\n",
    "        # deciding the queries\n",
    "        import_df_cols = deepcopy(idf.columns)\n",
    "        nh_id_flag = False\n",
    "        email_flag = False\n",
    "        if 'nanoHUB_user_ID__c' in import_df_cols:\n",
    "            nh_id_flag = True\n",
    "\n",
    "        if 'Email' in import_df_cols:\n",
    "            email_flag = True    \n",
    "\n",
    "        if nh_id_flag == True and email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)\n",
    "        elif email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)    \n",
    "\n",
    "        # find all existing contacts\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        grows = []\n",
    "        brows = [] #dont need the sf_bad_rows as send to leads\n",
    "        sf_grows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                grows.append(ind)\n",
    "                sf_grows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                brows.append(ind)   \n",
    "\n",
    "        # pull the matching SF entries and the matching import df entries\n",
    "        sf_df_match = sf_df.iloc[sf_grows,:].reset_index().iloc[:,1:]\n",
    "        idf_match = idf.iloc[grows,:].reset_index().iloc[:,1:]\n",
    "        lead_df = idf.iloc[brows,:].reset_index().iloc[:,1:] #use this in next section\n",
    "\n",
    "        log.info(sf_df_match.head(2))\n",
    "        log.info(idf_match.head(2))\n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                val.append(idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val\n",
    "\n",
    "        log.info(sf_df_match.head(5))\n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind]=';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## encoding correction for dashes\n",
    "        venues = sf_df_match['Venue__c'].apply(lambda x: x.replace('창\\x80\\x93','-'))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        sf_df_match = sf_df_match.drop_duplicates()\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        sf_df_match = sf_df_match[['Email','Venue__c','Id']]\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## send to SF\n",
    "        # rebuild api object\n",
    "        db_s_c = deepcopy(db_s)\n",
    "\n",
    "        # send data to SF\n",
    "        db_s_c.object_id = 'Contact'\n",
    "        # db_s_c.external_id = 'nanoHUB_user_ID__c'\n",
    "        db_s_c.external_id = 'Id'\n",
    "\n",
    "        db_s_c.send_data(sf_df_match)\n",
    "\n",
    "        ## find leads and send them to SF as well\n",
    "        #pull all current leads\n",
    "        sf_df = db_s.query_data('SELECT Id, Email, Venue__c, SF_indexer__c FROM Lead')    \n",
    "        # find the max sf_indexer\n",
    "        indexers = sf_df['SF_indexer__c'].fillna(0).to_list()\n",
    "        max_ind = max(indexers)    \n",
    "\n",
    "        # find all existing leads\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        m_rows = []\n",
    "        nm_rows = [] #don't need sf no match rows\n",
    "        sf_mrows = []    \n",
    "\n",
    "        for ind,val in enumerate(lead_df['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                m_rows.append(ind)\n",
    "                sf_mrows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                nm_rows.append(ind)    \n",
    "\n",
    "        # filter the matches\n",
    "        sf_df_match = sf_df.iloc[sf_mrows,:].reset_index().iloc[:,1:]\n",
    "        join_idf = lead_df.iloc[m_rows,:].reset_index().iloc[:,1:]\n",
    "        new_idf = lead_df.iloc[nm_rows,:].reset_index().iloc[:,1:]    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                #if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #    val.remove('MSE Summer Webinar Series 2020')\n",
    "                #    if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #        val.remove('MSE Summer Webinar Series 2020')\n",
    "                val.append(join_idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = join_idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val    \n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind] = ';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        # assign new SF_indexers for the new leads\n",
    "        new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "        new_idf['SF_indexer__c'] = range(int(max_ind)+1,new_max_ind+1)    \n",
    "\n",
    "        # need non-empty company field\n",
    "        new_idf['Company'] = '-'    \n",
    "\n",
    "        # ensure '창\\x80\\x93' has been replaced\n",
    "        sf_df_match['Venue__c'] = sf_df_match['Venue__c'].apply(lambda x: x.replace('창\\x80\\x93','-'))\n",
    "        new_idf['Venue__c'] = new_idf['Venue__c'].apply(lambda x: x.replace('창\\x80\\x93','-'))\n",
    "\n",
    "        sf_df_match['Company'] = '-'\n",
    "        new_idf['Company'] = '-'\n",
    "        log.info(new_idf)\n",
    "        # sys.exit()\n",
    "\n",
    "        # populate the company fields for sf_df_match and new_idf by comparing email addresses\n",
    "        if 'Company' in idf.columns:\n",
    "            log.info(\"company exist, using it\")\n",
    "            # comparison for sf_df_match\n",
    "\n",
    "            sf_df_match_comp_ind = sf_df_match.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in sf_df_match['Email'].to_list():\n",
    "                    sf_df_match_ind = sf_df_match['Email'].to_list().index(val)\n",
    "                    sf_df_match.iloc[sf_df_match_ind,sf_df_match_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "\n",
    "            # sf_df_match sf_indexer if not available\n",
    "            sf_df_match_sf_ind = sf_df_match.columns.to_list().index('SF_indexer__c')\n",
    "            new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "            for ind,val in enumerate(sf_df_match['SF_indexer__c'].to_list()):\n",
    "                if type(val) != int and type(val) != float:\n",
    "                    sf_df_match.iloc[ind,sf_df_match_sf_ind] = new_max_ind+1\n",
    "                    new_max_ind += 1\n",
    "\n",
    "            new_idf_comp_ind = new_idf.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in new_idf['Email'].to_list():\n",
    "                    new_idf_ind = new_idf['Email'].to_list().index(val)\n",
    "                    new_idf.iloc[new_idf_ind,new_idf_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "        else:\n",
    "            log.info('no company in import list, set to -')    \n",
    "        \n",
    "        if sf_df_match.shape[0] == 0 and new_idf.shape[0] == 0: \n",
    "            log.info('no leads to import, they were all contacts')\n",
    "        else:\n",
    "            sf_df_match = sf_df_match.fillna('-')\n",
    "            new_idf = new_idf.fillna('-')\n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "            except:\n",
    "                log.info('names are good')          \n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                new_idf['Company'] = '-'\n",
    "            except: # names are not present\n",
    "                try:\n",
    "                    tempnames = new_idf['Faculty'].apply(lambda x: x.split(' '))\n",
    "                    tf_names = [i[0] for i in tempnames]\n",
    "                    tl_names = [i[-1] if len(i[-1]) > 0 else '-' for i in tempnames]\n",
    "                    new_idf['firstname'] = tf_names\n",
    "                    new_idf['lastname'] = tl_names\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'    \n",
    "                except:\n",
    "                    tempnames = new_idf['Name'].to_list()\n",
    "                    temp_fname = []\n",
    "                    temp_lname = []\n",
    "                    for t_ind,t_val in enumerate(tempnames):\n",
    "                        t_val = t_val.split(' ')\n",
    "                        temp_fname.append(t_val[0])\n",
    "                        if len(t_val[-1]) > 0:\n",
    "                            temp_lname.append(t_val[-1])\n",
    "                        else:\n",
    "                            temp_lname.append('-')\n",
    "\n",
    "                    new_idf['firstname'] = temp_fname\n",
    "                    new_idf['lastname'] = temp_lname\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'\n",
    "\n",
    "            #drop duplicate rows\n",
    "            sf_df_match = sf_df_match.drop_duplicates(subset='SF_indexer__c')\n",
    "            new_idf = new_idf.drop_duplicates()    \n",
    "\n",
    "            sf_df_match['Company'] = sf_df_match['Company'].replace('  ','-')\n",
    "            sf_df_match = sf_df_match.drop(columns='SF_indexer__c')\n",
    "\n",
    "            #send the matching ones\n",
    "            db_s_l1 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l1.object_id = 'Lead'\n",
    "            # db_s_l1.external_id = 'SF_indexer__c'\n",
    "            db_s_l1.external_id = 'Id'\n",
    "\n",
    "            db_s_l1.send_data(sf_df_match)\n",
    "\n",
    "            #send the new ones\n",
    "            db_s_l2 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l2.object_id = 'Lead'\n",
    "            db_s_l2.external_id = 'SF_indexer__c'\n",
    "\n",
    "            db_s_l2.send_data(new_idf)\n",
    "\n",
    "        success_files.append(file)\n",
    "    except Exception as e:\n",
    "        log.info(\"Error with file: \" + str(e))\n",
    "        fail_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d19bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## move success files from To_Import to Imported on Google Drive \n",
    "## move failure files from To_Import to Import_Issues on GDrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbb7b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log.info(\"success files: \")\n",
    "success_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367ff30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log.info(\"fail files: \")\n",
    "fail_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54261ad6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tbd_imp_names = file_names\n",
    "tbd_imp_ids = file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3cd19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## match success and failures to their appropriate ids\n",
    "success_fids = []\n",
    "failure_fids = []\n",
    "\n",
    "for i in success_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    success_fids.append(tbd_imp_ids[t_index])#[0][0]])\n",
    "\n",
    "for i in fail_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    failure_fids.append(tbd_imp_ids[t_index])#[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6cd68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log.info(\"success file ids: \")\n",
    "success_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74124299",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log.info(\"failure file ids: \")\n",
    "failure_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ffe62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## start with success (To_Import -> Imported)\n",
    "imported_folder_id = get_subfolder_id(service, folder_id, IMPORTED_FOLDER_NAME)\n",
    "log.info('Found imported sub-folder: %s (%s)' % (IMPORTED_FOLDER_NAME, imported_folder_id))\n",
    "\n",
    "for sid in success_fids:\n",
    "    log.info('Deleting success file with id: ' + sid)\n",
    "    change_folder_for_file(sid, subfolder_id, imported_folder_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f4131",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## now do failures (To_Import -> Import_Issues)\n",
    "# failure_folder_id = get_subfolder_id(service, folder_id, FAILURES_FOLDER_NAME)\n",
    "# log.info('Found failure subfolder: %s (%s)' % (FAILURES_FOLDER_NAME, failure_folder_id))\n",
    "#\n",
    "# for fid in failure_fids:\n",
    "#     log.info('Deleting failed file with id: ' + fid)\n",
    "#     change_folder_for_file(fid, subfolder_id, failure_folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f8536",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove files from local directory\n",
    "for file in list_files:\n",
    "    os.remove(import_dir + '/' +file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.580919,
   "end_time": "2021-10-22T12:25:01.949708",
   "environment_variables": {},
   "exception": true,
   "input_path": "/Users/saxenap/Documents/Dev/nanoHUB/nanoHUB/pipeline/SF_dataimports/general_imports.ipynb",
   "output_path": "./.output/general_imports.ipynb",
   "parameters": {},
   "start_time": "2021-10-22T12:23:58.368789",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}