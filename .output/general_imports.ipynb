{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b06aef",
   "metadata": {
    "papermill": {
     "duration": 0.068649,
     "end_time": "2021-09-01T18:50:26.165310",
     "exception": false,
     "start_time": "2021-09-01T18:50:26.096661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## general_import.py transposed to a .ipynb file for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8096c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:26.275100Z",
     "iopub.status.busy": "2021-09-01T18:50:26.274042Z",
     "iopub.status.idle": "2021-09-01T18:50:26.277705Z",
     "shell.execute_reply": "2021-09-01T18:50:26.278310Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.061794,
     "end_time": "2021-09-01T18:50:26.278706",
     "exception": false,
     "start_time": "2021-09-01T18:50:26.216912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb40045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:26.396156Z",
     "iopub.status.busy": "2021-09-01T18:50:26.395513Z",
     "iopub.status.idle": "2021-09-01T18:50:28.032683Z",
     "shell.execute_reply": "2021-09-01T18:50:28.033554Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.698821,
     "end_time": "2021-09-01T18:50:28.034109",
     "exception": false,
     "start_time": "2021-09-01T18:50:26.335288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mnanoHUB - Serving Students, Researchers & Instructors\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained Salesforce access token ...... True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from nanoHUB.application import Application\n",
    "from googleapiclient.discovery import build\n",
    "from apiclient import errors\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from httplib2 import Http\n",
    "from nanoHUB.logger import logger\n",
    "application = Application.get_instance()\n",
    "# nanohub_db = application.new_db_engine('nanohub')\n",
    "# nanohub_metrics_db = application.new_db_engine('nanohub_metrics')\n",
    "# wang159_myrmekes_db = application.new_db_engine('wang159_myrmekes')\n",
    "\n",
    "salesforce = application.new_salesforce_engine()\n",
    "db_s = salesforce\n",
    "log = logger('nanoHUB:google_imports')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618c297",
   "metadata": {
    "papermill": {
     "duration": 0.05412,
     "end_time": "2021-09-01T18:50:28.155144",
     "exception": false,
     "start_time": "2021-09-01T18:50:28.101024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup GDrive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e3f4f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:28.273555Z",
     "iopub.status.busy": "2021-09-01T18:50:28.272688Z",
     "iopub.status.idle": "2021-09-01T18:50:28.276103Z",
     "shell.execute_reply": "2021-09-01T18:50:28.275443Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.061791,
     "end_time": "2021-09-01T18:50:28.276298",
     "exception": false,
     "start_time": "2021-09-01T18:50:28.214507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "## stuff that's rather hard to find from documentation\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ed2f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:28.390889Z",
     "iopub.status.busy": "2021-09-01T18:50:28.390169Z",
     "iopub.status.idle": "2021-09-01T18:50:28.400049Z",
     "shell.execute_reply": "2021-09-01T18:50:28.400435Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.069636,
     "end_time": "2021-09-01T18:50:28.400598",
     "exception": false,
     "start_time": "2021-09-01T18:50:28.330962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'Salesforce_Imports'\n",
    "TO_IMPORT_FOLDER_NAME = 'To_Import'\n",
    "TO_IMPORT_FOLDER_NAME = 'Test'\n",
    "FAILURES_FOLDER_NAME = 'Import_Issues'\n",
    "IMPORTED_FOLDER_NAME = 'Imported'\n",
    "\n",
    "import_dir = os.getenv('APP_DIR') + '/.cache/SF_Imports'\n",
    "\n",
    "service = application.new_google_api_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564e2eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:28.513497Z",
     "iopub.status.busy": "2021-09-01T18:50:28.512627Z",
     "iopub.status.idle": "2021-09-01T18:50:28.515095Z",
     "shell.execute_reply": "2021-09-01T18:50:28.515621Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.059709,
     "end_time": "2021-09-01T18:50:28.515871",
     "exception": false,
     "start_time": "2021-09-01T18:50:28.456162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NCN_master_id = '1OjlcHKkyKe9Uw_3iTFxIUafdtD1NNXKC' \n",
    "sf_import_master_id = '1Hg6UQmf5z2ZRqOwVnOHMqjLwZkZN-jOh' \n",
    "tbd_import_id = '1gIbrfPnNs8TZrhTarhWOaCl8Bts5PNX8' \n",
    "failure_id = '1WMU4A6IMQhtIZ4rehpFiKiyllc718U_1' \n",
    "success_id = '1lf2vTUWiZHLEhfzM8m3Tym4j9UGgdhMZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3f7a89",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:28.631992Z",
     "iopub.status.busy": "2021-09-01T18:50:28.631241Z",
     "iopub.status.idle": "2021-09-01T18:50:28.633024Z",
     "shell.execute_reply": "2021-09-01T18:50:28.633564Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.062364,
     "end_time": "2021-09-01T18:50:28.633818",
     "exception": false,
     "start_time": "2021-09-01T18:50:28.571454",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_folder_id(service, folder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + folder_name + \"'\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def get_subfolder_id(service, parent_folder_id: str, subfolder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + subfolder_name + \"'\" + \" and '\" + parent_folder_id + \"' in parents\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def change_folder_for_file(file_id: str, old_folder_id: str, new_folder_id: str):\n",
    "    return service.files().update(\n",
    "        fileId=file_id,\n",
    "        removeParents=old_folder_id,\n",
    "        addParents=new_folder_id,\n",
    "        fields='id, parents'\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f0626b",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:28.750177Z",
     "iopub.status.busy": "2021-09-01T18:50:28.749529Z",
     "iopub.status.idle": "2021-09-01T18:50:29.245730Z",
     "shell.execute_reply": "2021-09-01T18:50:29.246176Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.557334,
     "end_time": "2021-09-01T18:50:29.246381",
     "exception": false,
     "start_time": "2021-09-01T18:50:28.689047",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:29,240 - [INFO] nanoHUB:google_imports [191429788.<module>:2]: Found folder: Salesforce_Imports (1xwtRsGEcBNW7LItcgoekxxZADk7Ghy-d)\n"
     ]
    }
   ],
   "source": [
    "folder_id = get_folder_id(service, FOLDER_NAME)\n",
    "log.info('Found folder: %s (%s)' % (FOLDER_NAME, folder_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf061e8",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:29.359061Z",
     "iopub.status.busy": "2021-09-01T18:50:29.358426Z",
     "iopub.status.idle": "2021-09-01T18:50:29.659881Z",
     "shell.execute_reply": "2021-09-01T18:50:29.660321Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.358872,
     "end_time": "2021-09-01T18:50:29.660497",
     "exception": false,
     "start_time": "2021-09-01T18:50:29.301625",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:29,654 - [INFO] nanoHUB:google_imports [285691732.<module>:2]: Found subfolder: Test (1pY2deL-Xk6EsRzgthcjpV5TtoFVlthVw)\n"
     ]
    }
   ],
   "source": [
    "subfolder_id = get_subfolder_id(service, folder_id, TO_IMPORT_FOLDER_NAME)\n",
    "log.info('Found subfolder: %s (%s)' % (TO_IMPORT_FOLDER_NAME, subfolder_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85356b8a",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:29.809003Z",
     "iopub.status.busy": "2021-09-01T18:50:29.807852Z",
     "iopub.status.idle": "2021-09-01T18:50:30.035288Z",
     "shell.execute_reply": "2021-09-01T18:50:30.032476Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.301944,
     "end_time": "2021-09-01T18:50:30.035530",
     "exception": false,
     "start_time": "2021-09-01T18:50:29.733586",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:30,010 - [INFO] nanoHUB:google_imports [2186677928.<module>:20]: Found file: Copy of Copy of UMC Mailing list 4.27.2020.csv (1ItxpX9921VMn37RQ2q6MkG4rvM5pSezQ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:30,012 - [INFO] nanoHUB:google_imports [2186677928.<module>:20]: Found file: Copy of UMC Mailing list 4.27.2020.csv (1ycGfM3i1gKifju_-wdYtWyQ7-xvgN-QM)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:30,013 - [INFO] nanoHUB:google_imports [2186677928.<module>:20]: Found file: Test2.csv (1XxYsmn-_DWM3vhWpOrWQsZ8LAc3IEfnT)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:30,015 - [INFO] nanoHUB:google_imports [2186677928.<module>:20]: Found file: Test 3 UMC Mailing list 4_27_2020.csv (13dG8MfeOrGGAnCIlQK5560nq-EZfhLVL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:30,016 - [INFO] nanoHUB:google_imports [2186677928.<module>:20]: Found file: UMC Mailing list 4_27_2020.csv (1n--JvkyCFWzKjtGAKXwnGegXir83J-hI)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:30,018 - [INFO] nanoHUB:google_imports [2186677928.<module>:20]: Found file: Test ML Debugging Neural Networks - A.csv (1BwhFciNHYIZGfDKU1AI_dplSgH75LSae)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:30,020 - [INFO] nanoHUB:google_imports [2186677928.<module>:20]: Found file: ML Debugging Neural Networks - A.csv (1lvYmcO5TkHcadWBBo-56Gh9oFGqBFLrq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 14:50:30,022 - [INFO] nanoHUB:google_imports [2186677928.<module>:20]: Found file: UMC Mailing list 4.27.2020.csv (1Qzh8QPy16se0f0pIUV6zyum0AvyV0ypc)\n"
     ]
    }
   ],
   "source": [
    "mimetypes = \"\"\"\n",
    "mimeType = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' or\n",
    "mimeType = 'application/vnd.ms-excel' or\n",
    "mimeType = 'text/csv'\n",
    "\"\"\"\n",
    "file_ids = []\n",
    "file_names = []\n",
    "page_token = None\n",
    "# q = \"(\" + mimetypes + \") and '\" + folder.get('id') + \"' in parents\"\n",
    "while True:\n",
    "    query = \"(\" + mimetypes + \") and '\" + subfolder_id + \"' in parents\"\n",
    "    response = service.files().list(\n",
    "        q = query,\n",
    "        spaces='drive',\n",
    "        pageToken=page_token,\n",
    "        fields=\"nextPageToken, files(id, name)\"\n",
    "    ).execute()\n",
    "    files = response.get('files', [])\n",
    "    for file in files:\n",
    "        log.info('Found file: %s (%s)' % (file.get('name'), file.get('id')))\n",
    "        file_names.append(file['name'])\n",
    "        file_ids.append(file['id'])\n",
    "    page_token = response.get('nextPageToken', None)\n",
    "    if page_token is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc2221a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:30.186293Z",
     "iopub.status.busy": "2021-09-01T18:50:30.185368Z",
     "iopub.status.idle": "2021-09-01T18:50:30.188103Z",
     "shell.execute_reply": "2021-09-01T18:50:30.188607Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.076685,
     "end_time": "2021-09-01T18:50:30.188791",
     "exception": false,
     "start_time": "2021-09-01T18:50:30.112106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io \n",
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "408a2d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:30.326244Z",
     "iopub.status.busy": "2021-09-01T18:50:30.325510Z",
     "iopub.status.idle": "2021-09-01T18:50:32.717891Z",
     "shell.execute_reply": "2021-09-01T18:50:32.718304Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 2.462364,
     "end_time": "2021-09-01T18:50:32.718502",
     "exception": false,
     "start_time": "2021-09-01T18:50:30.256138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "<_io.BufferedWriter name='/Users/saxenap/Documents/Dev/nanoHUB/.cache/SF_Imports/Copy of Copy of UMC Mailing list 4.27.2020.csv'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "<_io.BufferedWriter name='/Users/saxenap/Documents/Dev/nanoHUB/.cache/SF_Imports/Copy of UMC Mailing list 4.27.2020.csv'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "<_io.BufferedWriter name='/Users/saxenap/Documents/Dev/nanoHUB/.cache/SF_Imports/Test2.csv'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "<_io.BufferedWriter name='/Users/saxenap/Documents/Dev/nanoHUB/.cache/SF_Imports/Test 3 UMC Mailing list 4_27_2020.csv'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "<_io.BufferedWriter name='/Users/saxenap/Documents/Dev/nanoHUB/.cache/SF_Imports/UMC Mailing list 4_27_2020.csv'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "<_io.BufferedWriter name='/Users/saxenap/Documents/Dev/nanoHUB/.cache/SF_Imports/Test ML Debugging Neural Networks - A.csv'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "<_io.BufferedWriter name='/Users/saxenap/Documents/Dev/nanoHUB/.cache/SF_Imports/ML Debugging Neural Networks - A.csv'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "<_io.BufferedWriter name='/Users/saxenap/Documents/Dev/nanoHUB/.cache/SF_Imports/UMC Mailing list 4.27.2020.csv'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for temp_index,f_tbd_id in enumerate(file_ids):\n",
    "    request = service.files().get_media(fileId=f_tbd_id) #,mimeType='text/csv') #if not .csv, then do .export()\n",
    "    fh = io.BytesIO() \n",
    "    downloader = MediaIoBaseDownload(fh, request) \n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "        \n",
    "    # The file has been downloaded into RAM, now save it in a file\n",
    "    # https://stackoverflow.com/questions/60111361/how-to-download-a-file-from-google-drive-using-python-and-the-drive-api-v3\n",
    "    fh.seek(0)\n",
    "    with open(import_dir + '/' + file_names[temp_index], 'wb') as f:\n",
    "        print(f)\n",
    "        shutil.copyfileobj(fh, f) #, length=131072)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc2c69e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:32.856067Z",
     "iopub.status.busy": "2021-09-01T18:50:32.855404Z",
     "iopub.status.idle": "2021-09-01T18:50:32.857042Z",
     "shell.execute_reply": "2021-09-01T18:50:32.857651Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.068083,
     "end_time": "2021-09-01T18:50:32.857854",
     "exception": false,
     "start_time": "2021-09-01T18:50:32.789771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec1bae6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:33.013196Z",
     "iopub.status.busy": "2021-09-01T18:50:33.012252Z",
     "iopub.status.idle": "2021-09-01T18:50:33.016410Z",
     "shell.execute_reply": "2021-09-01T18:50:33.017029Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.082534,
     "end_time": "2021-09-01T18:50:33.017365",
     "exception": false,
     "start_time": "2021-09-01T18:50:32.934831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test 3 UMC Mailing list 4_27_2020.csv', 'Copy of UMC Mailing list 4.27.2020.csv', 'UMC Mailing list 4_27_2020.csv', 'ML Debugging Neural Networks - A.csv', 'Copy of Copy of UMC Mailing list 4.27.2020.csv', 'UMC Mailing list 4.27.2020.csv', 'Test2.csv', 'Test ML Debugging Neural Networks - A.csv']\n"
     ]
    }
   ],
   "source": [
    "list_files = [name for name in os.listdir(import_dir)] #if os.path.isfile(name)]\n",
    "print(list_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496dbc2",
   "metadata": {
    "papermill": {
     "duration": 0.084009,
     "end_time": "2021-09-01T18:50:33.170093",
     "exception": false,
     "start_time": "2021-09-01T18:50:33.086084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perform Sequential Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb4f1474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:33.309129Z",
     "iopub.status.busy": "2021-09-01T18:50:33.308314Z",
     "iopub.status.idle": "2021-09-01T18:50:33.311160Z",
     "shell.execute_reply": "2021-09-01T18:50:33.312056Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.073019,
     "end_time": "2021-09-01T18:50:33.312605",
     "exception": false,
     "start_time": "2021-09-01T18:50:33.239586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16dc6263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:33.471328Z",
     "iopub.status.busy": "2021-09-01T18:50:33.470672Z",
     "iopub.status.idle": "2021-09-01T18:50:33.473313Z",
     "shell.execute_reply": "2021-09-01T18:50:33.473883Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.081364,
     "end_time": "2021-09-01T18:50:33.474084",
     "exception": false,
     "start_time": "2021-09-01T18:50:33.392720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_files = []\n",
    "fail_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17eee5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:33.615139Z",
     "iopub.status.busy": "2021-09-01T18:50:33.614040Z",
     "iopub.status.idle": "2021-09-01T18:50:33.616859Z",
     "shell.execute_reply": "2021-09-01T18:50:33.616324Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.07163,
     "end_time": "2021-09-01T18:50:33.617123",
     "exception": false,
     "start_time": "2021-09-01T18:50:33.545493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chardet import detect\n",
    "import cchardet\n",
    "# get file encoding type\n",
    "def get_encoding_type(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f87a5f54",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-01T18:50:33.763601Z",
     "iopub.status.busy": "2021-09-01T18:50:33.762698Z",
     "iopub.status.idle": "2021-09-01T18:50:34.525220Z",
     "shell.execute_reply": "2021-09-01T18:50:34.524095Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.334969,
     "end_time": "2021-09-01T18:50:34.018960",
     "exception": false,
     "start_time": "2021-09-01T18:50:33.683991",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing file: Copy of Copy of UMC Mailing list 4.27.2020.csv---->\n",
      "From chardet ----> UTF-8-SIG\n",
      "From Normalizer ----> utf_8\n",
      "\n",
      "\n",
      "Processing file: Copy of UMC Mailing list 4.27.2020.csv---->\n",
      "From chardet ----> UTF-8-SIG\n",
      "From Normalizer ----> utf_8\n",
      "\n",
      "\n",
      "Processing file: Test2.csv---->\n",
      "From chardet ----> Windows-1252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/2zrjfqsn7x3fxjzpq46kqcyr0000gp/T/ipykernel_38371/3200663918.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'From chardet ----> '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfrom_chardet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfrom_normalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCnM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'From Normalizer ----> '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfrom_normalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nanohub/lib/python3.8/site-packages/charset_normalizer/api.py\u001b[0m in \u001b[0;36mfrom_path\u001b[0;34m(path, steps, chunk_size, threshold, cp_isolation, cp_exclusion, preemptive_behaviour, explain)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \"\"\"\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_fp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp_isolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp_exclusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreemptive_behaviour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nanohub/lib/python3.8/site-packages/charset_normalizer/api.py\u001b[0m in \u001b[0;36mfrom_fp\u001b[0;34m(fp, steps, chunk_size, threshold, cp_isolation, cp_exclusion, preemptive_behaviour, explain)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mWill\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclose\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m     return from_bytes(\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nanohub/lib/python3.8/site-packages/charset_normalizer/api.py\u001b[0m in \u001b[0;36mfrom_bytes\u001b[0;34m(sequences, steps, chunk_size, threshold, cp_isolation, cp_exclusion, preemptive_behaviour, explain)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mis_multi_byte_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_multi_byte_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_iana\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoding %s does not provide an IncrementalDecoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_iana\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nanohub/lib/python3.8/site-packages/charset_normalizer/utils.py\u001b[0m in \u001b[0;36mis_multi_byte_encoding\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[1;32m    218\u001b[0m     return name in {\"utf_8\", \"utf_8_sig\", \"utf_16\", \"utf_16_be\", \"utf_16_le\", \"utf_32\", \"utf_32_le\", \"utf_32_be\", \"utf_7\"} or issubclass(\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encodings.{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mMultibyteIncrementalDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from charset_normalizer import CharsetNormalizerMatches as CnM\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "for file in file_names:\n",
    "    file_path = import_dir + '/' + file\n",
    "    print(\"Processing file: \" + file + '---->')\n",
    "\n",
    "    from_chardet = get_encoding_type(file_path)\n",
    "    print('From chardet ----> ' + from_chardet['encoding'])\n",
    "\n",
    "    from_normalizer = CnM.from_path(file_path).best().first().encoding\n",
    "    print('From Normalizer ----> ' + from_normalizer)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d65ef7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for file in file_names:\n",
    "    idf = pd.DataFrame()\n",
    "    try:\n",
    "        f_type = file.split('.')[-1] \n",
    "\n",
    "        #pandas import # add exception handling - UnicodeError\n",
    "        if f_type == 'csv':\n",
    "            try:\n",
    "                idf = pd.read_csv(import_dir + '/' +file,encoding='utf-8')\n",
    "                log.info('Loading csv with encoding utf-8 now.')\n",
    "            except:\n",
    "                try:\n",
    "                    idf = pd.read_csv(import_dir + '/' +file,encoding='cp1252')\n",
    "                    log.info('Loading csv with encoding cp1252 now.')\n",
    "                except:\n",
    "                    try:\n",
    "                        idf = pd.read_csv(import_dir + '/' +file,encoding='utf-16')\n",
    "                        log.info('Loading csv with encoding utf-16 now.')\n",
    "                    except:\n",
    "                        try:\n",
    "                            idf = pd.read_csv(import_dir + '/' +file,encoding='cp1252',sep='\\t')\n",
    "                            log.info('Loading csv with encoding cp1252 now.')\n",
    "                        except:\n",
    "                            try:\n",
    "                                idf = pd.read_csv(import_dir + '/' +file,encoding='utf-16',sep='\\t')\n",
    "                                log.info('Loading csv with encoding utf-16 now.')\n",
    "                            except:\n",
    "                                print('error_bad_lines, csv import failed')\n",
    "                                raise TypeError\n",
    "        elif f_type == 'xlsx' or f_type == 'xls':\n",
    "            try:\n",
    "                xl = pd.ExcelFile(import_dir + '/' +file)\n",
    "                log.info(\"sheet names: \" + xl.sheet_names)# see all sheet names\n",
    "                sheet_names = xl.parse(xl.sheet_names) #this already performs an import\n",
    "                idf = pd.read_excel(import_dir + '/' +file,sheet_name=xl.sheet_names[0],header=0)#,skiprows=1)\n",
    "            except:\n",
    "                log.info('error bad lines, csv/xls/xlsx import failed')\n",
    "                raise TypeError\n",
    "\n",
    "        ## remove leading and trailing spaces ## add str.strip spaces for all columns and rename\n",
    "        prev_idf_cols = idf.columns\n",
    "        idf_cols = [i.strip() for i in idf.columns]\n",
    "\n",
    "        idf = idf.rename(columns={j:idf_cols[i] for i,j in enumerate(prev_idf_cols)})\n",
    "    #     display(idf.columns)\n",
    "    #     # print(prev_idf_cols)\n",
    "    #     # print(idf_cols)\n",
    "\n",
    "        # if engagement venue does not exist, then create a flag with that entry\n",
    "        try:\n",
    "            #if engagement venue is specified\n",
    "            log.info(idf['Engagement Venue'])\n",
    "            log.info(idf['First Name'])\n",
    "            idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "            # idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "        except:\n",
    "            #decide the event\n",
    "            #event_extract = xl.sheet_names[0]\n",
    "            #idf['Engagement Venue'] = event_extract\n",
    "            idf['Venue__c'] = file.split('.')[0]\n",
    "\n",
    "            try:\n",
    "                #name extract\n",
    "                names = idf['Name'].to_list()\n",
    "                from copy import deepcopy?\n",
    "                fname = deepcopy(names)\n",
    "                lname = deepcopy(names)\n",
    "                for ind, val in enumerate(names):\n",
    "                    val = val.split(' ')\n",
    "                    fname[ind] = val[-1]\n",
    "                    lname[ind] = val[0]\n",
    "\n",
    "                idf['firstname'] = fname\n",
    "                idf['lastname'] = lname\n",
    "                name_flag = True\n",
    "            except:\n",
    "                name_flag = False\n",
    "                log.info('no name')\n",
    "\n",
    "        #rename columns\n",
    "        idf = idf.rename(columns={'email':'Email','EMAIL':'Email','E-mail Address':'Email',\\\n",
    "                    'Email Address':'Email','Recipient Email':'Email'})\n",
    "        idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "        idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "\n",
    "        idf = idf.rename(columns={0:'Email'})\n",
    "        if name_flag == True:\n",
    "            idf = idf.drop(columns='Name')#['NAME','LAST NAME','FIRST NAME'])            \n",
    "        idf = idf.dropna(subset=['Email'])\n",
    "\n",
    "        #email check rows\n",
    "        grows = []\n",
    "        brows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            if '@' in val:\n",
    "                grows.append(ind)\n",
    "            else:\n",
    "                brows.append(ind)    \n",
    "        idf = idf.iloc[grows,:].reset_index().iloc[:,1:]        \n",
    "\n",
    "        print(len(grows))\n",
    "        print(len(brows))\n",
    "\n",
    "        ## Import in contacts\n",
    "        os_name = os.name\n",
    "        sys_name = platform.system() #Linux, Darwin, Windows    \n",
    "\n",
    "        # salesforce queries for contact data\n",
    "        # deciding the queries\n",
    "        import_df_cols = deepcopy(idf.columns)\n",
    "        nh_id_flag = False\n",
    "        email_flag = False\n",
    "        if 'nanoHUB_user_ID__c' in import_df_cols:\n",
    "            nh_id_flag = True\n",
    "\n",
    "        if 'Email' in import_df_cols:\n",
    "            email_flag = True    \n",
    "\n",
    "        if nh_id_flag == True and email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)\n",
    "        elif email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)    \n",
    "\n",
    "        # find all existing contacts\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        grows = []\n",
    "        brows = [] #dont need the sf_bad_rows as send to leads\n",
    "        sf_grows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                grows.append(ind)\n",
    "                sf_grows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                brows.append(ind)   \n",
    "\n",
    "        # pull the matching SF entries and the matching import df entries\n",
    "        sf_df_match = sf_df.iloc[sf_grows,:].reset_index().iloc[:,1:]\n",
    "        idf_match = idf.iloc[grows,:].reset_index().iloc[:,1:]\n",
    "        lead_df = idf.iloc[brows,:].reset_index().iloc[:,1:] #use this in next section\n",
    "\n",
    "    #     print(sf_df_match.head(2))\n",
    "    #     print(idf_match.head(2))    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                val.append(idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val\n",
    "\n",
    "    #     print(sf_df_match.head(5))\n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind]=';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## encoding correction for dashes\n",
    "        venues = sf_df_match['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        sf_df_match = sf_df_match.drop_duplicates()\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        sf_df_match = sf_df_match[['Email','Venue__c','Id']]\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## send to SF\n",
    "        # rebuild api object\n",
    "        db_s_c = deepcopy(db_s)\n",
    "\n",
    "        # send data to SF\n",
    "        db_s_c.object_id = 'Contact'\n",
    "        # db_s_c.external_id = 'nanoHUB_user_ID__c'\n",
    "        db_s_c.external_id = 'Id'\n",
    "\n",
    "        db_s_c.send_data(sf_df_match)\n",
    "\n",
    "        ## find leads and send them to SF as well\n",
    "        #pull all current leads\n",
    "        sf_df = db_s.query_data('SELECT Id, Email, Venue__c, SF_indexer__c FROM Lead')    \n",
    "        # find the max sf_indexer\n",
    "        indexers = sf_df['SF_indexer__c'].fillna(0).to_list()\n",
    "        max_ind = max(indexers)    \n",
    "\n",
    "        # find all existing leads\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        m_rows = []\n",
    "        nm_rows = [] #don't need sf no match rows\n",
    "        sf_mrows = []    \n",
    "\n",
    "        for ind,val in enumerate(lead_df['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                m_rows.append(ind)\n",
    "                sf_mrows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                nm_rows.append(ind)    \n",
    "\n",
    "        # filter the matches\n",
    "        sf_df_match = sf_df.iloc[sf_mrows,:].reset_index().iloc[:,1:]\n",
    "        join_idf = lead_df.iloc[m_rows,:].reset_index().iloc[:,1:]\n",
    "        new_idf = lead_df.iloc[nm_rows,:].reset_index().iloc[:,1:]    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                #if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #    val.remove('MSE Summer Webinar Series 2020')\n",
    "                #    if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #        val.remove('MSE Summer Webinar Series 2020')\n",
    "                val.append(join_idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = join_idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val    \n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind] = ';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        # assign new SF_indexers for the new leads\n",
    "        new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "        new_idf['SF_indexer__c'] = range(int(max_ind)+1,new_max_ind+1)    \n",
    "\n",
    "        # need non-empty company field\n",
    "        new_idf['Company'] = '-'    \n",
    "\n",
    "        # ensure 'â\\x80\\x93' has been replaced\n",
    "        sf_df_match['Venue__c'] = sf_df_match['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "        new_idf['Venue__c'] = new_idf['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "\n",
    "        sf_df_match['Company'] = '-'\n",
    "        new_idf['Company'] = '-'\n",
    "        print(new_idf)\n",
    "        sys.exit()\n",
    "\n",
    "        # populate the company fields for sf_df_match and new_idf by comparing email addresses\n",
    "        if 'Company' in idf.columns:\n",
    "            print(\"company exist, using it\")\n",
    "            # comparison for sf_df_match\n",
    "\n",
    "            sf_df_match_comp_ind = sf_df_match.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in sf_df_match['Email'].to_list():\n",
    "                    sf_df_match_ind = sf_df_match['Email'].to_list().index(val)\n",
    "                    sf_df_match.iloc[sf_df_match_ind,sf_df_match_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "\n",
    "            # sf_df_match sf_indexer if not available\n",
    "            sf_df_match_sf_ind = sf_df_match.columns.to_list().index('SF_indexer__c')\n",
    "            new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "            for ind,val in enumerate(sf_df_match['SF_indexer__c'].to_list()):\n",
    "                if type(val) != int and type(val) != float:\n",
    "                    sf_df_match.iloc[ind,sf_df_match_sf_ind] = new_max_ind+1\n",
    "                    new_max_ind += 1\n",
    "\n",
    "            new_idf_comp_ind = new_idf.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in new_idf['Email'].to_list():\n",
    "                    new_idf_ind = new_idf['Email'].to_list().index(val)\n",
    "                    new_idf.iloc[new_idf_ind,new_idf_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "        else:\n",
    "            print('no company in import list, set to -')    \n",
    "        \n",
    "        if sf_df_match.shape[0] == 0 and new_idf.shape[0] == 0: \n",
    "            print('no leads to import, they were all contacts')\n",
    "        else:\n",
    "            sf_df_match = sf_df_match.fillna('-')\n",
    "            new_idf = new_idf.fillna('-')\n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "            except:\n",
    "                print('names are good')          \n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                new_idf['Company'] = '-'\n",
    "            except: # names are not present\n",
    "                try:\n",
    "                    tempnames = new_idf['Faculty'].apply(lambda x: x.split(' '))\n",
    "                    tf_names = [i[0] for i in tempnames]\n",
    "                    tl_names = [i[-1] if len(i[-1]) > 0 else '-' for i in tempnames]\n",
    "                    new_idf['firstname'] = tf_names\n",
    "                    new_idf['lastname'] = tl_names\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'    \n",
    "                except:\n",
    "                    tempnames = new_idf['Name'].to_list()\n",
    "                    temp_fname = []\n",
    "                    temp_lname = []\n",
    "                    for t_ind,t_val in enumerate(tempnames):\n",
    "                        t_val = t_val.split(' ')\n",
    "                        temp_fname.append(t_val[0])\n",
    "                        if len(t_val[-1]) > 0:\n",
    "                            temp_lname.append(t_val[-1])\n",
    "                        else:\n",
    "                            temp_lname.append('-')\n",
    "\n",
    "                    new_idf['firstname'] = temp_fname\n",
    "                    new_idf['lastname'] = temp_lname\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'\n",
    "\n",
    "            #drop duplicate rows\n",
    "            sf_df_match = sf_df_match.drop_duplicates(subset='SF_indexer__c')\n",
    "            new_idf = new_idf.drop_duplicates()    \n",
    "\n",
    "            sf_df_match['Company'] = sf_df_match['Company'].replace('  ','-')\n",
    "            sf_df_match = sf_df_match.drop(columns='SF_indexer__c')\n",
    "\n",
    "            #send the matching ones\n",
    "            db_s_l1 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l1.object_id = 'Lead'\n",
    "            # db_s_l1.external_id = 'SF_indexer__c'\n",
    "            db_s_l1.external_id = 'Id'\n",
    "\n",
    "            db_s_l1.send_data(sf_df_match)\n",
    "\n",
    "            #send the new ones\n",
    "            db_s_l2 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l2.object_id = 'Lead'\n",
    "            db_s_l2.external_id = 'SF_indexer__c'\n",
    "\n",
    "            db_s_l2.send_data(new_idf)\n",
    "\n",
    "        success_files.append(file)\n",
    "    except Exception as e:\n",
    "        log.info(\"Error with file: \" + str(e))\n",
    "        fail_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6710f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## move success files from To_Import to Imported on Google Drive \n",
    "## move failure files from To_Import to Import_Issues on GDrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d59a2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"success files: \")\n",
    "success_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8fa806",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"fail files: \")\n",
    "fail_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be566272",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tbd_imp_names = file_names\n",
    "tbd_imp_ids = file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2ea27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## match success and failures to their appropriate ids\n",
    "success_fids = []\n",
    "failure_fids = []\n",
    "\n",
    "for i in success_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    success_fids.append(tbd_imp_ids[t_index])#[0][0]])\n",
    "\n",
    "for i in fail_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    failure_fids.append(tbd_imp_ids[t_index])#[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169a114",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"success file ids: \")\n",
    "success_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b747d44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"failure file ids: \")\n",
    "failure_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d946186",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## start with success (To_Import -> Imported)\n",
    "imported_folder_id = get_subfolder_id(service, folder_id, IMPORTED_FOLDER_NAME)\n",
    "log.info('Found imported sub-folder: %s (%s)' % (IMPORTED_FOLDER_NAME, imported_folder_id))\n",
    "\n",
    "for sid in success_fids:\n",
    "    print('Deleting success file with id: ' + sid)\n",
    "    change_folder_for_file(sid, subfolder_id, imported_folder_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b9d7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## now do failures (To_Import -> Import_Issues)\n",
    "# failure_folder_id = get_subfolder_id(service, folder_id, FAILURES_FOLDER_NAME)\n",
    "# log.info('Found failure subfolder: %s (%s)' % (FAILURES_FOLDER_NAME, failure_folder_id))\n",
    "#\n",
    "# for fid in failure_fids:\n",
    "#     print('Deleting failed file with id: ' + fid)\n",
    "#     change_folder_for_file(fid, subfolder_id, failure_folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd01602",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove files from local directory\n",
    "for file in list_files:\n",
    "    os.remove(import_dir + '/' +file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.910619,
   "end_time": "2021-09-01T18:50:34.811139",
   "environment_variables": {},
   "exception": null,
   "input_path": "nanoHUB/pipeline/SF_dataimports/general_imports.ipynb",
   "output_path": "./.output/general_imports.ipynb",
   "parameters": {},
   "start_time": "2021-09-01T18:50:24.900520",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}