{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general_import.py transposed to a .ipynb file for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:00.997452Z",
     "iopub.status.busy": "2021-11-16T20:00:00.996020Z",
     "iopub.status.idle": "2021-11-16T20:00:01.022114Z",
     "shell.execute_reply": "2021-11-16T20:00:01.021343Z",
     "shell.execute_reply.started": "2021-11-16T20:00:00.996381Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:01.025650Z",
     "iopub.status.busy": "2021-11-16T20:00:01.023478Z",
     "iopub.status.idle": "2021-11-16T20:00:05.139943Z",
     "shell.execute_reply": "2021-11-16T20:00:05.136195Z",
     "shell.execute_reply.started": "2021-11-16T20:00:01.025615Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mnanoHUB - Serving Students, Researchers & Instructors\u001b[0m\n",
      "Obtained Salesforce access token ...... True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from nanoHUB.application import Application\n",
    "from googleapiclient.discovery import build\n",
    "from apiclient import errors\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from httplib2 import Http\n",
    "from nanoHUB.logger import logger\n",
    "application = Application.get_instance()\n",
    "# nanohub_db = application.new_db_engine('nanohub')\n",
    "# nanohub_metrics_db = application.new_db_engine('nanohub_metrics')\n",
    "# wang159_myrmekes_db = application.new_db_engine('wang159_myrmekes')\n",
    "\n",
    "salesforce = application.new_salesforce_engine()\n",
    "db_s = salesforce\n",
    "log = logger('nanoHUB:google_imports')\n",
    "\n",
    "\n",
    "SHOULD_IMPORT_FRESH = False\n",
    "SHOULD_IMPORT_FRESH = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GDrive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:05.145693Z",
     "iopub.status.busy": "2021-11-16T20:00:05.144624Z",
     "iopub.status.idle": "2021-11-16T20:00:05.160837Z",
     "shell.execute_reply": "2021-11-16T20:00:05.157941Z",
     "shell.execute_reply.started": "2021-11-16T20:00:05.145496Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/saxenap/nanoHUB\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import os\n",
    "import errno\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "## stuff that's rather hard to find from documentation\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "\n",
    "print(os.environ['APP_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:05.164792Z",
     "iopub.status.busy": "2021-11-16T20:00:05.163843Z",
     "iopub.status.idle": "2021-11-16T20:00:05.197730Z",
     "shell.execute_reply": "2021-11-16T20:00:05.194650Z",
     "shell.execute_reply.started": "2021-11-16T20:00:05.164628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'Salesforce_Imports'\n",
    "TO_IMPORT_FOLDER_NAME = 'To_Import'\n",
    "FAILURES_FOLDER_NAME = 'Import_Issues'\n",
    "IMPORTED_FOLDER_NAME = 'Imported'\n",
    "\n",
    "service = application.new_google_api_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:05.201451Z",
     "iopub.status.busy": "2021-11-16T20:00:05.200600Z",
     "iopub.status.idle": "2021-11-16T20:00:05.238005Z",
     "shell.execute_reply": "2021-11-16T20:00:05.236573Z",
     "shell.execute_reply.started": "2021-11-16T20:00:05.201255Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/saxenap/nanoHUB/.cache/SF_Imports/To_Import\n"
     ]
    }
   ],
   "source": [
    "import_dir = os.getenv('APP_DIR') + '/.cache/SF_Imports'\n",
    "\n",
    "import_dir_path = Path(import_dir + '/' + FOLDER_NAME)\n",
    "import_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "to_import_dir_path = Path(import_dir + '/' + TO_IMPORT_FOLDER_NAME)\n",
    "to_import_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "failures_import_dir_path = Path(import_dir + '/' + FAILURES_FOLDER_NAME)\n",
    "failures_import_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "imported_dir_path = Path(import_dir + '/' + IMPORTED_FOLDER_NAME)\n",
    "imported_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(to_import_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:05.241643Z",
     "iopub.status.busy": "2021-11-16T20:00:05.240937Z",
     "iopub.status.idle": "2021-11-16T20:00:05.262802Z",
     "shell.execute_reply": "2021-11-16T20:00:05.258370Z",
     "shell.execute_reply.started": "2021-11-16T20:00:05.241584Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_folder_id(service, folder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + folder_name + \"'\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def get_subfolder_id(service, parent_folder_id: str, subfolder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + subfolder_name + \"'\" + \" and '\" + parent_folder_id + \"' in parents\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def change_folder_for_file(file_id: str, old_folder_id: str, new_folder_id: str):\n",
    "    return service.files().update(\n",
    "        fileId=file_id,\n",
    "        removeParents=old_folder_id,\n",
    "        addParents=new_folder_id,\n",
    "        fields='id, parents'\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:05.266478Z",
     "iopub.status.busy": "2021-11-16T20:00:05.265609Z",
     "iopub.status.idle": "2021-11-16T20:00:08.779391Z",
     "shell.execute_reply": "2021-11-16T20:00:08.778680Z",
     "shell.execute_reply.started": "2021-11-16T20:00:05.266411Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [3847485018 - nanoHUB:google_imports]: Found folder: Salesforce_Imports (1xwtRsGEcBNW7LItcgoekxxZADk7Ghy-d) [3847485018.<module>:3]\n",
      "[INFO] [3847485018 - nanoHUB:google_imports]: Found subfolder: To_Import (1vjnRjaPcEeK82Ye3_BmjMEmXU51Y9FL-) [3847485018.<module>:6]\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_IMPORT_FRESH == True:\n",
    "    folder_id = get_folder_id(service, FOLDER_NAME)\n",
    "    log.info('Found folder: %s (%s)' % (FOLDER_NAME, folder_id))\n",
    "    \n",
    "    subfolder_id = get_subfolder_id(service, folder_id, TO_IMPORT_FOLDER_NAME)\n",
    "    log.info('Found subfolder: %s (%s)' % (TO_IMPORT_FOLDER_NAME, subfolder_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:08.784910Z",
     "iopub.status.busy": "2021-11-16T20:00:08.782625Z",
     "iopub.status.idle": "2021-11-16T20:00:09.401485Z",
     "shell.execute_reply": "2021-11-16T20:00:09.397774Z",
     "shell.execute_reply.started": "2021-11-16T20:00:08.784863Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'files': [{'id': '1jSf6PIeYtXyca5WvG0Bz4U5bRI-7GYrF', 'name': 'ML Webinar 11 17 2021 - R.csv'}]}\n",
      "[{'id': '1jSf6PIeYtXyca5WvG0Bz4U5bRI-7GYrF', 'name': 'ML Webinar 11 17 2021 - R.csv'}]\n",
      "[INFO] [82911130 - nanoHUB:google_imports]: Found file: ML Webinar 11 17 2021 - R.csv (1jSf6PIeYtXyca5WvG0Bz4U5bRI-7GYrF) [82911130.<module>:23]\n",
      "[INFO] [82911130 - nanoHUB:google_imports]: ['1jSf6PIeYtXyca5WvG0Bz4U5bRI-7GYrF'] [82911130.<module>:30]\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_IMPORT_FRESH == True:    \n",
    "    mimetypes = \"\"\"\n",
    "    mimeType = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' or\n",
    "    mimeType = 'application/vnd.ms-excel' or\n",
    "    mimeType = 'text/csv'\n",
    "    \"\"\"\n",
    "    file_ids = []\n",
    "    file_names = []\n",
    "    page_token = None\n",
    "    # q = \"(\" + mimetypes + \") and '\" + folder.get('id') + \"' in parents\"\n",
    "    while True:\n",
    "        query = \"(\" + mimetypes + \") and '\" + subfolder_id + \"' in parents\"\n",
    "        response = service.files().list(\n",
    "            q = query,\n",
    "            spaces='drive',\n",
    "            pageToken=page_token,\n",
    "            fields=\"nextPageToken, files(id, name)\"\n",
    "        ).execute()\n",
    "        print(response)\n",
    "        files = response.get('files', [])\n",
    "        print(files)\n",
    "        for file in files:\n",
    "            log.info('Found file: %s (%s)' % (file.get('name'), file.get('id')))\n",
    "            file_names.append(file['name'])\n",
    "            file_ids.append(file['id'])\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "            \n",
    "    log.info(file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:09.403981Z",
     "iopub.status.busy": "2021-11-16T20:00:09.403600Z",
     "iopub.status.idle": "2021-11-16T20:00:09.413873Z",
     "shell.execute_reply": "2021-11-16T20:00:09.411155Z",
     "shell.execute_reply.started": "2021-11-16T20:00:09.403939Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io \n",
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:09.415623Z",
     "iopub.status.busy": "2021-11-16T20:00:09.415165Z",
     "iopub.status.idle": "2021-11-16T20:00:11.126689Z",
     "shell.execute_reply": "2021-11-16T20:00:11.123909Z",
     "shell.execute_reply.started": "2021-11-16T20:00:09.415565Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [4233136166 - nanoHUB:google_imports]: Downloading ML Webinar 11 17 2021 - R.csv 100%. [4233136166.<module>:11]\n",
      "[INFO] [4233136166 - nanoHUB:google_imports]: Finished downloading files [4233136166.<module>:17]\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_IMPORT_FRESH == True:    \n",
    "\n",
    "    try:\n",
    "        for temp_index,f_tbd_id in enumerate(file_ids):\n",
    "            request = service.files().get_media(fileId=f_tbd_id) #,mimeType='text/csv') #if not .csv, then do .export()\n",
    "            fh = io.BytesIO()\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while done is False:\n",
    "                status, done = downloader.next_chunk()\n",
    "                log.info(\"Downloading %s %d%%.\" % (file_names[temp_index], int(status.progress() * 100)))\n",
    "\n",
    "            # The file has been downloaded into RAM, now save it in a file\n",
    "            # https://stackoverflow.com/questions/60111361/how-to-download-a-file-from-google-drive-using-python-and-the-drive-api-v3\n",
    "            fh.seek(0)\n",
    "\n",
    "        log.info(\"Finished downloading files\")\n",
    "    except Exception as e:\n",
    "        log.info(\"Error downloading file: \" + str(e))\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.128221Z",
     "iopub.status.busy": "2021-11-16T20:00:11.128004Z",
     "iopub.status.idle": "2021-11-16T20:00:11.171509Z",
     "shell.execute_reply": "2021-11-16T20:00:11.167022Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.128190Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/saxenap/nanoHUB/.cache/SF_Imports/To_Import\n",
      "[INFO] [4249670119 - nanoHUB:google_imports]: <_io.BufferedWriter name='/home/saxenap/nanoHUB/.cache/SF_Imports/To_Import/ML Webinar 11 17 2021 - R.csv'> [4249670119.<module>:9]\n",
      "[INFO] [4249670119 - nanoHUB:google_imports]: Finished downloading files [4249670119.<module>:15]\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_IMPORT_FRESH == True:     \n",
    "    try:\n",
    "        for temp_index,f_tbd_id in enumerate(file_ids):\n",
    "            print(to_import_dir_path)\n",
    "            download_filepath = Path(to_import_dir_path, file_names[temp_index])\n",
    "            with open(download_filepath, 'wb') as f:\n",
    "                \n",
    "\n",
    "                log.info(f)\n",
    "                shutil.copyfileobj(fh, f) #, length=131072)\n",
    "\n",
    "    except Exception as e:\n",
    "        log.info(\"Error saving file (%s) : \" % download_filepath.absolute() + str(e))\n",
    "\n",
    "    log.info(\"Finished downloading files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.181351Z",
     "iopub.status.busy": "2021-11-16T20:00:11.178909Z",
     "iopub.status.idle": "2021-11-16T20:00:11.191455Z",
     "shell.execute_reply": "2021-11-16T20:00:11.189746Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.181114Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.192996Z",
     "iopub.status.busy": "2021-11-16T20:00:11.192781Z",
     "iopub.status.idle": "2021-11-16T20:00:11.223960Z",
     "shell.execute_reply": "2021-11-16T20:00:11.222138Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.192966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2499228183 - nanoHUB:google_imports]: ['ML Webinar 11 17 2021 - R.csv'] [2499228183.<module>:8]\n",
      "[INFO] [2499228183 - nanoHUB:google_imports]: ['/home/saxenap/nanoHUB/.cache/SF_Imports/To_Import/ML Webinar 11 17 2021 - R.csv'] [2499228183.<module>:9]\n"
     ]
    }
   ],
   "source": [
    "list_files = []\n",
    "to_import_file_paths = []\n",
    "\n",
    "for file_name in os.listdir(to_import_dir_path):\n",
    "    list_files.append(file_name)\n",
    "    to_import_file_paths.append(os.path.join(to_import_dir_path, file_name))\n",
    "\n",
    "log.info(list_files)\n",
    "log.info(to_import_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sequential Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.227442Z",
     "iopub.status.busy": "2021-11-16T20:00:11.226838Z",
     "iopub.status.idle": "2021-11-16T20:00:11.233666Z",
     "shell.execute_reply": "2021-11-16T20:00:11.232423Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.227406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.238368Z",
     "iopub.status.busy": "2021-11-16T20:00:11.236242Z",
     "iopub.status.idle": "2021-11-16T20:00:11.245128Z",
     "shell.execute_reply": "2021-11-16T20:00:11.243547Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.238207Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_files = []\n",
    "fail_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.249047Z",
     "iopub.status.busy": "2021-11-16T20:00:11.248733Z",
     "iopub.status.idle": "2021-11-16T20:00:11.260530Z",
     "shell.execute_reply": "2021-11-16T20:00:11.258858Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.248993Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chardet import detect\n",
    "import cchardet\n",
    "# get file encoding type\n",
    "def get_encoding_type(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.263953Z",
     "iopub.status.busy": "2021-11-16T20:00:11.263562Z",
     "iopub.status.idle": "2021-11-16T20:00:11.358613Z",
     "shell.execute_reply": "2021-11-16T20:00:11.356426Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.263902Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [4054237887 - nanoHUB:google_imports]: \n",
      " [4054237887.<module>:4]\n",
      "[INFO] [4054237887 - nanoHUB:google_imports]: Processing file: /home/saxenap/nanoHUB/.cache/SF_Imports/To_Import/ML Webinar 11 17 2021 - R.csv----> [4054237887.<module>:7]\n",
      "[INFO] [4054237887 - nanoHUB:google_imports]: From Normalizer ----> utf_8 [4054237887.<module>:13]\n"
     ]
    }
   ],
   "source": [
    "from charset_normalizer import CharsetNormalizerMatches as CnM\n",
    "\n",
    "\n",
    "log.info(\"\\n\")\n",
    "for file in to_import_file_paths:\n",
    "\n",
    "    log.info(\"Processing file: \" + str(file) + '---->')\n",
    "\n",
    "    #from_chardet = get_encoding_type(str(file))\n",
    "    #log.info('From chardet ----> ' + from_chardet['encoding'])\n",
    "\n",
    "    from_normalizer = CnM.from_path(str(file)).best().first().encoding\n",
    "    log.info('From Normalizer ----> ' + from_normalizer)\n",
    "\n",
    "\n",
    "\n",
    "from charset_normalizer import from_path\n",
    "\n",
    "def change_encoding(file_path: str):\n",
    "    try:\n",
    "        with open(file_path, 'w') as filetowrite:\n",
    "            filetowrite.write(results)\n",
    "    except IOError as e:\n",
    "        log.info('Sadly, we are unable to perform charset normalization.', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.362759Z",
     "iopub.status.busy": "2021-11-16T20:00:11.360708Z",
     "iopub.status.idle": "2021-11-16T20:00:11.448769Z",
     "shell.execute_reply": "2021-11-16T20:00:11.445070Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.362695Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1391234329 - nanoHUB:google_imports]: Reading /home/saxenap/nanoHUB/.cache/SF_Imports/To_Import/ML Webinar 11 17 2021 - R.csv [1391234329.<module>:8]\n",
      "[INFO] [1391234329 - nanoHUB:google_imports]: File loaded and converted to a DataFrame [1391234329.<module>:19]\n",
      "[INFO] [1391234329 - nanoHUB:google_imports]: Encoding: utf-8, Separator: , [1391234329.<module>:20]\n",
      "   Registration Count            Event ID    Event Key    \\\n",
      "0                     1  207898488745263590  26234766766   \n",
      "1                     2  207898488745263590  26234766766   \n",
      "2                     3  207898488745263590  26234766766   \n",
      "3                     4  207898488745263590  26234766766   \n",
      "4                     5  207898488745263590  26234766766   \n",
      "\n",
      "                                      Program Name    \\\n",
      "0  nanoHUB Hands-on Data Science and Machine Lear...   \n",
      "1  nanoHUB Hands-on Data Science and Machine Lear...   \n",
      "2  nanoHUB Hands-on Data Science and Machine Lear...   \n",
      "3  nanoHUB Hands-on Data Science and Machine Lear...   \n",
      "4  nanoHUB Hands-on Data Science and Machine Lear...   \n",
      "\n",
      "                                        Event Name    \\\n",
      "0  nanoHUB ML workshops: Joe D Kern, Integrating ...   \n",
      "1  nanoHUB ML workshops: Joe D Kern, Integrating ...   \n",
      "2  nanoHUB ML workshops: Joe D Kern, Integrating ...   \n",
      "3  nanoHUB ML workshops: Joe D Kern, Integrating ...   \n",
      "4  nanoHUB ML workshops: Joe D Kern, Integrating ...   \n",
      "\n",
      "                Event Start Date       Event Start Time    \\\n",
      "0  November 17, 2021 New York Time  1:00 pm New York Time   \n",
      "1  November 17, 2021 New York Time  1:00 pm New York Time   \n",
      "2  November 17, 2021 New York Time  1:00 pm New York Time   \n",
      "3  November 17, 2021 New York Time  1:00 pm New York Time   \n",
      "4  November 17, 2021 New York Time  1:00 pm New York Time   \n",
      "\n",
      "        Event End Time   Event/Recording Duration   FirstName    \\\n",
      "0  2:00 pm New York Time                  60.0 mins  ARUN KUMAR   \n",
      "1  2:00 pm New York Time                  60.0 mins     Vishwas   \n",
      "2  2:00 pm New York Time                  60.0 mins    Chijioke   \n",
      "3  2:00 pm New York Time                  60.0 mins      Saejin   \n",
      "4  2:00 pm New York Time                  60.0 mins      Junaid   \n",
      "\n",
      "             LastName                      Email   Invited   Registered    \\\n",
      "0  MANNODI KANAKKITHODI        amannodi@purdue.edu       Yes          Yes   \n",
      "1                  Goel         vishwasg@umich.edu        No          Yes   \n",
      "2                   Eze  nwoye.chijioke@unn.edu.ng        No          Yes   \n",
      "3                    Oh          saejinoh@ucsb.edu        No          Yes   \n",
      "4                   Ali   junaidali.ciit@gmail.com        No          Yes   \n",
      "\n",
      "  Attended   Registration Status   Lead Source ID    \\\n",
      "0         No              Approved                    \n",
      "1         No              Approved                    \n",
      "2         No              Approved                    \n",
      "3         No              Approved                    \n",
      "4         No              Approved                    \n",
      "\n",
      "                   Registration Date/Time    Registration ID    \\\n",
      "0  November 12, 2021 12:04 pm New York Time             551033   \n",
      "1   November 11, 2021 6:05 pm New York Time             730281   \n",
      "2   November 12, 2021 2:28 pm New York Time             202052   \n",
      "3  November 12, 2021 11:50 am New York Time             579065   \n",
      "4    October 13, 2021 5:23 am New York Time             582455   \n",
      "\n",
      "   Registration Score   Okay to send email   Title    Number of Employees    \\\n",
      "0                     0                   No                            NaN   \n",
      "1                     0                   No                            NaN   \n",
      "2                     0                   No      Dr                    NaN   \n",
      "3                     0                   No                            NaN   \n",
      "4                     0                   No                            NaN   \n",
      "\n",
      "                               Company   Phone   Address 1   Address 2    \\\n",
      "0                      Purdue University      1-                           \n",
      "1                                             1-                           \n",
      "2          University of Nigeria, Nsukka      1-                           \n",
      "3                                   UCSB      1-                           \n",
      "4  COMSATS University Islamabad Pakistan      1-                           \n",
      "\n",
      "           City   State/Province   Postal/Zip Code    \\\n",
      "0     W LAFAYETTE               IN                     \n",
      "1       Ann Arbor                                      \n",
      "2           Enugu            Enugu                     \n",
      "3   Santa Barbara               CA                     \n",
      "4  Rawalpindi-RWP           Punjab                     \n",
      "\n",
      "           Country/Region   How did you hear about this webinar?    \\\n",
      "0  United States of America                                    NaN   \n",
      "1  United States of America                     nanoHUB Newsletter   \n",
      "2                   Nigeria                                  email   \n",
      "3  United States of America                                  email   \n",
      "4                  Pakistan                     nanoHUB Newsletter   \n",
      "\n",
      "  What is your primary motivation for attending this tutorial?    \\\n",
      "0                                                NaN               \n",
      "1                 To gain skills for my own research               \n",
      "2                 To gain skills for my own research               \n",
      "3  To gain skills for my own research|Personal In...               \n",
      "4                                  Personal Interest               \n",
      "\n",
      "  Are you running nanoHUB in a classroom?    \\\n",
      "0                                       NaN   \n",
      "1                                        No   \n",
      "2                                        No   \n",
      "3                                        No   \n",
      "4                                       NaN   \n",
      "\n",
      "  We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?    \\\n",
      "0                                                NaN                                                                                                                          \n",
      "1                                                NaN                                                                                                                          \n",
      "2                                         Email list                                                                                                                          \n",
      "3                                   Slack|Email list                                                                                                                          \n",
      "4          Slack|Microsoft Teams|nanoHUB Group Forum                                                                                                                          \n",
      "\n",
      "   Which of the following do you use?    \n",
      "0                                   NaN  \n",
      "1                                   NaN  \n",
      "2                                   NaN  \n",
      "3                                   NaN  \n",
      "4                                   NaN  \n"
     ]
    }
   ],
   "source": [
    "possible_encodings = [\n",
    "    'utf-8', 'cp1252', 'utf-16', 'cp1254', 'cp775', 'utf-8-sig', 'iso-8859-1', 'unicode_escape', 'gbk','latin1'\n",
    "]\n",
    "possible_separators = [',', '\\t']\n",
    "possible_engines = ['c', 'python']\n",
    "\n",
    "for file in to_import_file_paths:\n",
    "    log.info(\"Reading %s\" % file)\n",
    "    idf = pd.DataFrame()\n",
    "    f_type = file.split('.')[-1]\n",
    "    \n",
    "    decoded = False\n",
    "    for encoding in possible_encodings:\n",
    "        if decoded == False:\n",
    "            for sep in possible_separators:\n",
    "                if decoded == False:\n",
    "                    try:\n",
    "                        idf = pd.read_csv(file, encoding=encoding, sep=sep)\n",
    "                        log.info(\"File loaded and converted to a DataFrame\")\n",
    "                        log.info(\"Encoding: %s, Separator: %s\" % (encoding, sep))\n",
    "                        decoded = True\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "    print(idf.head())\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-16T20:00:11.456212Z",
     "iopub.status.busy": "2021-11-16T20:00:11.455759Z",
     "iopub.status.idle": "2021-11-16T20:02:14.593814Z",
     "shell.execute_reply": "2021-11-16T20:02:14.592195Z",
     "shell.execute_reply.started": "2021-11-16T20:00:11.456160Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [3489145242 - nanoHUB:google_imports]: Reading /home/saxenap/nanoHUB/.cache/SF_Imports/To_Import/ML Webinar 11 17 2021 - R.csv [3489145242.<module>:3]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: File type csv [3489145242.<module>:6]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: Loaded csv with encoding utf-8. [3489145242.<module>:11]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: Index(['Registration Count  ', 'Event ID  ', 'Event Key  ', 'Program Name  ',\n",
      "       'Event Name  ', 'Event Start Date  ', 'Event Start Time  ',\n",
      "       'Event End Time  ', 'Event/Recording Duration  ', 'FirstName  ',\n",
      "       'LastName  ', 'Email  ', 'Invited  ', 'Registered  ', 'Attended  ',\n",
      "       'Registration Status  ', 'Lead Source ID  ', 'Registration Date/Time  ',\n",
      "       'Registration ID  ', 'Registration Score  ', 'Okay to send email  ',\n",
      "       'Title  ', 'Number of Employees  ', 'Company  ', 'Phone  ',\n",
      "       'Address 1  ', 'Address 2  ', 'City  ', 'State/Province  ',\n",
      "       'Postal/Zip Code  ', 'Country/Region  ',\n",
      "       'How did you hear about this webinar?  ',\n",
      "       'What is your primary motivation for attending this tutorial?  ',\n",
      "       'Are you running nanoHUB in a classroom?  ',\n",
      "       'We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?  ',\n",
      "       'Which of the following do you use?  '],\n",
      "      dtype='object') [3489145242.<module>:73]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: Index(['Registration Count', 'Event ID', 'Event Key', 'Program Name',\n",
      "       'Event Name', 'Event Start Date', 'Event Start Time', 'Event End Time',\n",
      "       'Event/Recording Duration', 'FirstName', 'LastName', 'Email', 'Invited',\n",
      "       'Registered', 'Attended', 'Registration Status', 'Lead Source ID',\n",
      "       'Registration Date/Time', 'Registration ID', 'Registration Score',\n",
      "       'Okay to send email', 'Title', 'Number of Employees', 'Company',\n",
      "       'Phone', 'Address 1', 'Address 2', 'City', 'State/Province',\n",
      "       'Postal/Zip Code', 'Country/Region',\n",
      "       'How did you hear about this webinar?',\n",
      "       'What is your primary motivation for attending this tutorial?',\n",
      "       'Are you running nanoHUB in a classroom?',\n",
      "       'We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?',\n",
      "       'Which of the following do you use?'],\n",
      "      dtype='object') [3489145242.<module>:77]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: no name [3489145242.<module>:112]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: Index(['Registration Count', 'Event ID', 'Event Key', 'Program Name',\n",
      "       'Event Name', 'Event Start Date', 'Event Start Time', 'Event End Time',\n",
      "       'Event/Recording Duration', 'firstname', 'lastname', 'Email', 'Invited',\n",
      "       'Registered', 'Attended', 'Registration Status', 'Lead Source ID',\n",
      "       'Registration Date/Time', 'Registration ID', 'Registration Score',\n",
      "       'Okay to send email', 'Title', 'Number of Employees', 'Company',\n",
      "       'Phone', 'Address 1', 'Address 2', 'City', 'State/Province',\n",
      "       'Postal/Zip Code', 'Country/Region',\n",
      "       'How did you hear about this webinar?',\n",
      "       'What is your primary motivation for attending this tutorial?',\n",
      "       'Are you running nanoHUB in a classroom?',\n",
      "       'We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?',\n",
      "       'Which of the following do you use?', 'Venue__c'],\n",
      "      dtype='object') [3489145242.<module>:125]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: 299 [3489145242.<module>:136]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: 0 [3489145242.<module>:137]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b5A7oAAE\n",
      "{\"id\":\"7505w00000b5A7oAAE\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-11-16T20:00:13.000+0000\",\"systemModstamp\":\"2021-11-16T20:00:13.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":0,\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000b5A7oAAE\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-11-16T20:00:13.000+0000\",\"systemModstamp\":\"2021-11-16T20:00:13.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":17133,\"retries\":0,\"totalProcessingTime\":1826}\n",
      "{\"id\":\"7505w00000b5A7oAAE\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-11-16T20:00:13.000+0000\",\"systemModstamp\":\"2021-11-16T20:00:38.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":264883,\"retries\":0,\"totalProcessingTime\":21697}\n",
      "[Success] Bulk job completed successfully.\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]:                  Email                  Id  \\\n",
      "0  amannodi@purdue.edu  0035w00003FVt59AAD   \n",
      "1   vishwasg@umich.edu  0035w000034JyhkAAC   \n",
      "\n",
      "                                            Venue__c  nanoHUB_user_ID__c  \n",
      "0  ML111120_A;Spring ML Webinar Series Attendee;M...            303955.0  \n",
      "1  Citrine Material Science workshop - R;/home/sa...            217370.0   [3489145242.<module>:177]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]:    Registration Count            Event ID    Event Key  \\\n",
      "0                   1  207898488745263590  26234766766   \n",
      "1                   2  207898488745263590  26234766766   \n",
      "\n",
      "                                        Program Name  \\\n",
      "0  nanoHUB Hands-on Data Science and Machine Lear...   \n",
      "1  nanoHUB Hands-on Data Science and Machine Lear...   \n",
      "\n",
      "                                          Event Name  \\\n",
      "0  nanoHUB ML workshops: Joe D Kern, Integrating ...   \n",
      "1  nanoHUB ML workshops: Joe D Kern, Integrating ...   \n",
      "\n",
      "                  Event Start Date       Event Start Time  \\\n",
      "0  November 17, 2021 New York Time  1:00 pm New York Time   \n",
      "1  November 17, 2021 New York Time  1:00 pm New York Time   \n",
      "\n",
      "          Event End Time Event/Recording Duration   firstname  \\\n",
      "0  2:00 pm New York Time                60.0 mins  ARUN KUMAR   \n",
      "1  2:00 pm New York Time                60.0 mins     Vishwas   \n",
      "\n",
      "               lastname                Email Invited Registered Attended  \\\n",
      "0  MANNODI KANAKKITHODI  amannodi@purdue.edu     Yes        Yes       No   \n",
      "1                  Goel   vishwasg@umich.edu      No        Yes       No   \n",
      "\n",
      "  Registration Status Lead Source ID  \\\n",
      "0            Approved                  \n",
      "1            Approved                  \n",
      "\n",
      "                     Registration Date/Time  Registration ID  \\\n",
      "0  November 12, 2021 12:04 pm New York Time           551033   \n",
      "1   November 11, 2021 6:05 pm New York Time           730281   \n",
      "\n",
      "   Registration Score Okay to send email Title  Number of Employees  \\\n",
      "0                   0                 No                        NaN   \n",
      "1                   0                 No                        NaN   \n",
      "\n",
      "             Company Phone Address 1 Address 2         City State/Province  \\\n",
      "0  Purdue University    1-                      W LAFAYETTE             IN   \n",
      "1                       1-                        Ann Arbor                  \n",
      "\n",
      "  Postal/Zip Code            Country/Region  \\\n",
      "0                  United States of America   \n",
      "1                  United States of America   \n",
      "\n",
      "  How did you hear about this webinar?  \\\n",
      "0                                  NaN   \n",
      "1                   nanoHUB Newsletter   \n",
      "\n",
      "  What is your primary motivation for attending this tutorial?  \\\n",
      "0                                                NaN             \n",
      "1                 To gain skills for my own research             \n",
      "\n",
      "  Are you running nanoHUB in a classroom?  \\\n",
      "0                                     NaN   \n",
      "1                                      No   \n",
      "\n",
      "  We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?  \\\n",
      "0                                                NaN                                                                                                                        \n",
      "1                                                NaN                                                                                                                        \n",
      "\n",
      "   Which of the following do you use?                Venue__c  \n",
      "0                                 NaN  /home/saxenap/nanoHUB/  \n",
      "1                                 NaN  /home/saxenap/nanoHUB/   [3489145242.<module>:178]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]:                        Email                  Id  \\\n",
      "0        amannodi@purdue.edu  0035w00003FVt59AAD   \n",
      "1         vishwasg@umich.edu  0035w000034JyhkAAC   \n",
      "2  nwoye.chijioke@unn.edu.ng  0035w00003TJw0UAAT   \n",
      "3   junaidali.ciit@gmail.com  0035w000034JN7ZAAW   \n",
      "4    shadmani26133@gmail.com  0035w000034IsXWAA0   \n",
      "\n",
      "                                            Venue__c  nanoHUB_user_ID__c  \n",
      "0  ML111120_A;Spring ML Webinar Series Attendee;M...            303955.0  \n",
      "1  Citrine Material Science workshop - R;/home/sa...            217370.0  \n",
      "2      /home/saxenap/nanoHUB/;/home/saxenap/nanoHUB/            333581.0  \n",
      "3  Citrine Material Science Workshop - S;Long-ter...             22106.0  \n",
      "4  nanoBIO WWS 12/1/2020 attendee;/home/saxenap/n...            278041.0   [3489145242.<module>:190]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b5A91AAE\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000b5A91AAE\n",
      "[Success] Closing job successful. Job ID = 7505w00000b5A91AAE\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b5A9LAAU\n",
      "{\"id\":\"7505w00000b5A9LAAU\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-11-16T20:01:28.000+0000\",\"systemModstamp\":\"2021-11-16T20:01:28.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000b5A9LAAU\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-11-16T20:01:28.000+0000\",\"systemModstamp\":\"2021-11-16T20:01:30.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":0,\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000b5A9LAAU\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-11-16T20:01:28.000+0000\",\"systemModstamp\":\"2021-11-16T20:01:51.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":11534,\"retries\":0,\"totalProcessingTime\":897}\n",
      "[Success] Bulk job completed successfully.\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: Empty DataFrame\n",
      "Columns: [Registration Count, Event ID, Event Key, Program Name, Event Name, Event Start Date, Event Start Time, Event End Time, Event/Recording Duration, firstname, lastname, Email, Invited, Registered, Attended, Registration Status, Lead Source ID, Registration Date/Time, Registration ID, Registration Score, Okay to send email, Title, Number of Employees, Company, Phone, Address 1, Address 2, City, State/Province, Postal/Zip Code, Country/Region, How did you hear about this webinar?, What is your primary motivation for attending this tutorial?, Are you running nanoHUB in a classroom?, We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?, Which of the following do you use?, Venue__c, SF_indexer__c]\n",
      "Index: [] [3489145242.<module>:279]\n",
      "[INFO] [3489145242 - nanoHUB:google_imports]: company exist, using it [3489145242.<module>:284]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b5AAJAA2\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000b5AAJAA2\n",
      "[Success] Closing job successful. Job ID = 7505w00000b5AAJAA2\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b5AAdAAM\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000b5AAdAAM\n",
      "[Success] Closing job successful. Job ID = 7505w00000b5AAdAAM\n",
      "['/home/saxenap/nanoHUB/.cache/SF_Imports/To_Import/ML Webinar 11 17 2021 - R.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for file in to_import_file_paths:\n",
    "    idf = pd.DataFrame()\n",
    "    log.info(\"Reading %s\" % file)\n",
    "    try:\n",
    "        f_type = file.split('.')[-1]\n",
    "        log.info(\"File type %s\" % f_type)\n",
    "        #pandas import # add exception handling - UnicodeError\n",
    "        if f_type == 'csv':\n",
    "            try:\n",
    "                idf = pd.read_csv(file, encoding='utf-8')\n",
    "                log.info('Loaded csv with encoding utf-8.')\n",
    "            except:\n",
    "                try:\n",
    "                    idf = pd.read_csv(file, encoding='cp1252')\n",
    "                    log.info('Loaded csv with encoding cp1252 now.')\n",
    "                except:\n",
    "                    try:\n",
    "                        idf = pd.read_csv(file, encoding='utf-16')\n",
    "                        log.info('Loaded csv with encoding utf-16 now.')\n",
    "                    except:\n",
    "                        try:\n",
    "                            idf = pd.read_csv(file, encoding='cp1254')\n",
    "                            log.info('Loaded csv with encoding cp1254 now.')\n",
    "                        except:\n",
    "                            try:\n",
    "                                idf = pd.read_csv(file, encoding='cp775')\n",
    "                                log.info('Loaded csv with encoding cp775 now.')\n",
    "                            except:\n",
    "                                try:\n",
    "                                    idf = pd.read_csv(file, encoding='cp1252',sep='\\t')\n",
    "                                    log.info('Loaded csv with encoding cp1252 now.')\n",
    "                                except:\n",
    "                                    try:\n",
    "                                        idf = pd.read_csv(file, encoding='utf-16',sep='\\t')\n",
    "                                        log.info('Loaded csv with encoding utf-16 now.')\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            idf = pd.read_csv(file, encoding='cp1254',sep='\\t')\n",
    "                                            log.info('Loaded csv with encoding cp1254 now.')\n",
    "                                        except:\n",
    "                                            try:\n",
    "                                                idf = pd.read_csv(file, encoding='cp775',sep='\\t')\n",
    "                                                log.info('Loaded csv with encoding cp775 now.')\n",
    "                                            except:\n",
    "                                                log.info('CSV import failed. Possible encoding issues with file %s.' % file)\n",
    "                                                try:\n",
    "                                                    idf = pd.read_csv(file, encoding='cp775',sep=',')\n",
    "                                                    log.info('Loaded csv with encoding cp775 now.')\n",
    "                                                except:\n",
    "                                                    log.info('CSV import failed. Possible encoding issues with file %s.' % file)\n",
    "                                                    try:\n",
    "                                                        idf = pd.read_csv(file, engine='python')\n",
    "                                                        log.info('Loaded csv with python engine now.')\n",
    "                                                    except:\n",
    "                                                        log.info('CSV import failed. Possible encoding issues with file %s.' % file)\n",
    "                                                        try:\n",
    "                                                            idf = pd.read_csv(file, engine='python',sep='\\t')\n",
    "                                                            log.info('Loaded csv with python engine now.')\n",
    "                                                        except:\n",
    "                                                            log.info('CSV import failed. Possible encoding issues with file %s.' % file)\n",
    "                                                            raise TypeError\n",
    "        elif f_type == 'xlsx' or f_type == 'xls':\n",
    "            try:\n",
    "                xl = pd.ExcelFile(file)\n",
    "                log.info(\"sheet names: \" + xl.sheet_names)# see all sheet names\n",
    "                sheet_names = xl.parse(xl.sheet_names) #this already performs an import\n",
    "                idf = pd.read_excel(file,sheet_name=xl.sheet_names[0],header=0)#,skiprows=1)\n",
    "            except:\n",
    "                log.info('error bad lines, csv/xls/xlsx import failed')\n",
    "                raise TypeError\n",
    "\n",
    "        ## remove leading and trailing spaces ## add str.strip spaces for all columns and rename\n",
    "        log.info(idf.columns)\n",
    "        prev_idf_cols = idf.columns\n",
    "        idf_cols = [i.strip() for i in idf.columns]\n",
    "        idf.columns = idf.columns.str.strip()\n",
    "        log.info(idf.columns)\n",
    "        idf = idf.rename(columns={j:idf_cols[i] for i,j in enumerate(prev_idf_cols)})\n",
    "    #     display(idf.columns)\n",
    "    #     # log.info(prev_idf_cols)\n",
    "    #     # log.info(idf_cols)\n",
    "\n",
    "        # if engagement venue does not exist, then create a flag with that entry\n",
    "        try:\n",
    "            #if engagement venue is specified\n",
    "            log.info(idf['Engagement Venue'])\n",
    "            log.info(idf['First Name'])\n",
    "            idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "            # idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "        except:\n",
    "            #decide the event\n",
    "            #event_extract = xl.sheet_names[0]\n",
    "            #idf['Engagement Venue'] = event_extract\n",
    "            idf['Venue__c'] = file.split('.')[0]\n",
    "\n",
    "            try:\n",
    "                #name extract\n",
    "                names = idf['Name'].to_list()\n",
    "                from copy import deepcopy?\n",
    "                fname = deepcopy(names)\n",
    "                lname = deepcopy(names)\n",
    "                for ind, val in enumerate(names):\n",
    "                    val = val.split(' ')\n",
    "                    fname[ind] = val[-1]\n",
    "                    lname[ind] = val[0]\n",
    "\n",
    "                idf['firstname'] = fname\n",
    "                idf['lastname'] = lname\n",
    "                name_flag = True\n",
    "            except:\n",
    "                name_flag = False\n",
    "                log.info('no name')\n",
    "\n",
    "        #rename columns\n",
    "        idf = idf.rename(columns={'email':'Email','EMAIL':'Email','E-mail Address':'Email',\\\n",
    "                    'Email Address':'Email','Recipient Email':'Email'})\n",
    "        idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "        idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname','FirstName':'firstname','LastName':'lastname'})\n",
    "\n",
    "        idf = idf.rename(columns={0:'Email'})\n",
    "        if name_flag == True:\n",
    "            idf = idf.drop(columns='Name')#['NAME','LAST NAME','FIRST NAME'])            \n",
    "        idf = idf.dropna(subset=['Email'])\n",
    "\n",
    "        log.info(idf.columns)\n",
    "        #email check rows\n",
    "        grows = []\n",
    "        brows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            if '@' in val:\n",
    "                grows.append(ind)\n",
    "            else:\n",
    "                brows.append(ind)    \n",
    "        idf = idf.iloc[grows,:].reset_index().iloc[:,1:]        \n",
    "\n",
    "        log.info(len(grows))\n",
    "        log.info(len(brows))\n",
    "\n",
    "        ## Import in contacts\n",
    "        os_name = os.name\n",
    "        sys_name = platform.system() #Linux, Darwin, Windows    \n",
    "\n",
    "        # salesforce queries for contact data\n",
    "        # deciding the queries\n",
    "        import_df_cols = deepcopy(idf.columns)\n",
    "        nh_id_flag = False\n",
    "        email_flag = False\n",
    "        if 'nanoHUB_user_ID__c' in import_df_cols:\n",
    "            nh_id_flag = True\n",
    "\n",
    "        if 'Email' in import_df_cols:\n",
    "            email_flag = True    \n",
    "\n",
    "        if nh_id_flag == True and email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)\n",
    "        elif email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)    \n",
    "\n",
    "        # find all existing contacts\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        grows = []\n",
    "        brows = [] #dont need the sf_bad_rows as send to leads\n",
    "        sf_grows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                grows.append(ind)\n",
    "                sf_grows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                brows.append(ind)   \n",
    "\n",
    "        # pull the matching SF entries and the matching import df entries\n",
    "        sf_df_match = sf_df.iloc[sf_grows,:].reset_index().iloc[:,1:]\n",
    "        idf_match = idf.iloc[grows,:].reset_index().iloc[:,1:]\n",
    "        lead_df = idf.iloc[brows,:].reset_index().iloc[:,1:] #use this in next section\n",
    "\n",
    "        log.info(sf_df_match.head(2))\n",
    "        log.info(idf_match.head(2))\n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                val.append(idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val\n",
    "\n",
    "        log.info(sf_df_match.head(5))\n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind]=';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## encoding correction for dashes\n",
    "        venues = sf_df_match['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        sf_df_match = sf_df_match.drop_duplicates()\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        sf_df_match = sf_df_match[['Email','Venue__c','Id']]\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## send to SF\n",
    "        # rebuild api object\n",
    "        db_s_c = deepcopy(db_s)\n",
    "\n",
    "        # send data to SF\n",
    "        db_s_c.object_id = 'Contact'\n",
    "        # db_s_c.external_id = 'nanoHUB_user_ID__c'\n",
    "        db_s_c.external_id = 'Id'\n",
    "\n",
    "        db_s_c.send_data(sf_df_match)\n",
    "\n",
    "        ## find leads and send them to SF as well\n",
    "        #pull all current leads\n",
    "        sf_df = db_s.query_data('SELECT Id, Email, Venue__c, SF_indexer__c FROM Lead')    \n",
    "        # find the max sf_indexer\n",
    "        indexers = sf_df['SF_indexer__c'].fillna(0).to_list()\n",
    "        max_ind = max(indexers)    \n",
    "\n",
    "        # find all existing leads\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        m_rows = []\n",
    "        nm_rows = [] #don't need sf no match rows\n",
    "        sf_mrows = []    \n",
    "\n",
    "        for ind,val in enumerate(lead_df['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                m_rows.append(ind)\n",
    "                sf_mrows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                nm_rows.append(ind)    \n",
    "\n",
    "        # filter the matches\n",
    "        sf_df_match = sf_df.iloc[sf_mrows,:].reset_index().iloc[:,1:]\n",
    "        join_idf = lead_df.iloc[m_rows,:].reset_index().iloc[:,1:]\n",
    "        new_idf = lead_df.iloc[nm_rows,:].reset_index().iloc[:,1:]    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                #if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #    val.remove('MSE Summer Webinar Series 2020')\n",
    "                #    if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #        val.remove('MSE Summer Webinar Series 2020')\n",
    "                val.append(join_idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = join_idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val    \n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind] = ';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        # assign new SF_indexers for the new leads\n",
    "        new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "        new_idf['SF_indexer__c'] = range(int(max_ind)+1,new_max_ind+1)    \n",
    "\n",
    "        # need non-empty company field\n",
    "        new_idf['Company'] = '-'    \n",
    "\n",
    "        # ensure 'â\\x80\\x93' has been replaced\n",
    "        sf_df_match['Venue__c'] = sf_df_match['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "        new_idf['Venue__c'] = new_idf['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "\n",
    "        sf_df_match['Company'] = '-'\n",
    "        new_idf['Company'] = '-'\n",
    "        log.info(new_idf)\n",
    "        # sys.exit()\n",
    "\n",
    "        # populate the company fields for sf_df_match and new_idf by comparing email addresses\n",
    "        if 'Company' in idf.columns:\n",
    "            log.info(\"company exist, using it\")\n",
    "            # comparison for sf_df_match\n",
    "\n",
    "            sf_df_match_comp_ind = sf_df_match.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in sf_df_match['Email'].to_list():\n",
    "                    sf_df_match_ind = sf_df_match['Email'].to_list().index(val)\n",
    "                    sf_df_match.iloc[sf_df_match_ind,sf_df_match_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "\n",
    "            # sf_df_match sf_indexer if not available\n",
    "            sf_df_match_sf_ind = sf_df_match.columns.to_list().index('SF_indexer__c')\n",
    "            new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "            for ind,val in enumerate(sf_df_match['SF_indexer__c'].to_list()):\n",
    "                if type(val) != int and type(val) != float:\n",
    "                    sf_df_match.iloc[ind,sf_df_match_sf_ind] = new_max_ind+1\n",
    "                    new_max_ind += 1\n",
    "\n",
    "            new_idf_comp_ind = new_idf.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in new_idf['Email'].to_list():\n",
    "                    new_idf_ind = new_idf['Email'].to_list().index(val)\n",
    "                    new_idf.iloc[new_idf_ind,new_idf_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "        else:\n",
    "            log.info('no company in import list, set to -')    \n",
    "        \n",
    "        if sf_df_match.shape[0] == 0 and new_idf.shape[0] == 0: \n",
    "            log.info('no leads to import, they were all contacts')\n",
    "        else:\n",
    "            sf_df_match = sf_df_match.fillna('-')\n",
    "            new_idf = new_idf.fillna('-')\n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "            except:\n",
    "                log.info('names are good')          \n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                new_idf['Company'] = '-'\n",
    "            except: # names are not present\n",
    "                try:\n",
    "                    tempnames = new_idf['Faculty'].apply(lambda x: x.split(' '))\n",
    "                    tf_names = [i[0] for i in tempnames]\n",
    "                    tl_names = [i[-1] if len(i[-1]) > 0 else '-' for i in tempnames]\n",
    "                    new_idf['firstname'] = tf_names\n",
    "                    new_idf['lastname'] = tl_names\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'    \n",
    "                except:\n",
    "                    tempnames = new_idf['Name'].to_list()\n",
    "                    temp_fname = []\n",
    "                    temp_lname = []\n",
    "                    for t_ind,t_val in enumerate(tempnames):\n",
    "                        t_val = t_val.split(' ')\n",
    "                        temp_fname.append(t_val[0])\n",
    "                        if len(t_val[-1]) > 0:\n",
    "                            temp_lname.append(t_val[-1])\n",
    "                        else:\n",
    "                            temp_lname.append('-')\n",
    "\n",
    "                    new_idf['firstname'] = temp_fname\n",
    "                    new_idf['lastname'] = temp_lname\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'\n",
    "\n",
    "            #drop duplicate rows\n",
    "            sf_df_match = sf_df_match.drop_duplicates(subset='SF_indexer__c')\n",
    "            new_idf = new_idf.drop_duplicates()    \n",
    "\n",
    "            sf_df_match['Company'] = sf_df_match['Company'].replace('  ','-')\n",
    "            sf_df_match = sf_df_match.drop(columns='SF_indexer__c')\n",
    "\n",
    "            #send the matching ones\n",
    "            db_s_l1 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l1.object_id = 'Lead'\n",
    "            # db_s_l1.external_id = 'SF_indexer__c'\n",
    "            db_s_l1.external_id = 'Id'\n",
    "\n",
    "            db_s_l1.send_data(sf_df_match)\n",
    "\n",
    "            #send the new ones\n",
    "            db_s_l2 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l2.object_id = 'Lead'\n",
    "            db_s_l2.external_id = 'SF_indexer__c'\n",
    "\n",
    "            db_s_l2.send_data(new_idf)\n",
    "\n",
    "        success_files.append(file)\n",
    "        print(success_files)\n",
    "\n",
    "    except Exception as e:\n",
    "        log.info(\"Error with file: \" + str(e))\n",
    "        fail_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:02:14.597303Z",
     "iopub.status.busy": "2021-11-16T20:02:14.595817Z",
     "iopub.status.idle": "2021-11-16T20:02:14.607040Z",
     "shell.execute_reply": "2021-11-16T20:02:14.605360Z",
     "shell.execute_reply.started": "2021-11-16T20:02:14.597229Z"
    }
   },
   "outputs": [],
   "source": [
    "## move success files from To_Import to Imported on Google Drive \n",
    "## move failure files from To_Import to Import_Issues on GDrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:02:14.610140Z",
     "iopub.status.busy": "2021-11-16T20:02:14.609597Z",
     "iopub.status.idle": "2021-11-16T20:02:14.642838Z",
     "shell.execute_reply": "2021-11-16T20:02:14.640503Z",
     "shell.execute_reply.started": "2021-11-16T20:02:14.610107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2124149726 - nanoHUB:google_imports]: success files:  [2124149726.<module>:1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/saxenap/nanoHUB/.cache/SF_Imports/To_Import/ML Webinar 11 17 2021 - R.csv']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.info(\"success files: \")\n",
    "success_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:02:14.646529Z",
     "iopub.status.busy": "2021-11-16T20:02:14.645140Z",
     "iopub.status.idle": "2021-11-16T20:02:14.667159Z",
     "shell.execute_reply": "2021-11-16T20:02:14.665764Z",
     "shell.execute_reply.started": "2021-11-16T20:02:14.646465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1178869520 - nanoHUB:google_imports]: fail files:  [1178869520.<module>:1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.info(\"fail files: \")\n",
    "fail_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:02:14.671473Z",
     "iopub.status.busy": "2021-11-16T20:02:14.669962Z",
     "iopub.status.idle": "2021-11-16T20:02:14.682765Z",
     "shell.execute_reply": "2021-11-16T20:02:14.680600Z",
     "shell.execute_reply.started": "2021-11-16T20:02:14.671350Z"
    }
   },
   "outputs": [],
   "source": [
    "tbd_imp_names = file_names\n",
    "tbd_imp_ids = file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T20:02:14.759565Z",
     "iopub.status.busy": "2021-11-16T20:02:14.758403Z",
     "iopub.status.idle": "2021-11-16T20:02:14.773764Z",
     "shell.execute_reply": "2021-11-16T20:02:14.771010Z",
     "shell.execute_reply.started": "2021-11-16T20:02:14.759308Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1jSf6PIeYtXyca5WvG0Bz4U5bRI-7GYrF\n",
      "['1jSf6PIeYtXyca5WvG0Bz4U5bRI-7GYrF']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## match success and failures to their appropriate ids\n",
    "success_fids = []\n",
    "failure_fids = []\n",
    "\n",
    "for i in success_files:\n",
    "    file_name = Path(i).name \n",
    "    t_index = tbd_imp_names.index(file_name) #np.where(i in tbd_imp_names)\n",
    "    print(t_index)\n",
    "    success_fids.append(tbd_imp_ids[t_index])#[0][0]])\n",
    "    print(tbd_imp_ids[t_index])\n",
    "\n",
    "for i in fail_files:\n",
    "    file_name = Path(i).name \n",
    "    t_index = tbd_imp_names.index(file_name) #np.where(i in tbd_imp_names)\n",
    "    failure_fids.append(tbd_imp_ids[t_index])#[0][0]])\n",
    "    \n",
    "print(success_fids)\n",
    "print(failure_fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"success file ids: \")\n",
    "success_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"failure file ids: \")\n",
    "failure_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start with success (To_Import -> Imported)\n",
    "imported_folder_id = get_subfolder_id(service, folder_id, IMPORTED_FOLDER_NAME)\n",
    "log.info('Found imported sub-folder: %s (%s)' % (IMPORTED_FOLDER_NAME, imported_folder_id))\n",
    "\n",
    "for sid in success_fids:\n",
    "    log.info('Deleting success file with id: ' + sid)\n",
    "    change_folder_for_file(sid, subfolder_id, imported_folder_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now do failures (To_Import -> Import_Issues)\n",
    "# failure_folder_id = get_subfolder_id(service, folder_id, FAILURES_FOLDER_NAME)\n",
    "# log.info('Found failure subfolder: %s (%s)' % (FAILURES_FOLDER_NAME, failure_folder_id))\n",
    "#\n",
    "# for fid in failure_fids:\n",
    "#     log.info('Deleting failed file with id: ' + fid)\n",
    "#     change_folder_for_file(fid, subfolder_id, failure_folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove files from local directory\n",
    "for file in list_files:\n",
    "    os.remove(import_dir + '/' +file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
