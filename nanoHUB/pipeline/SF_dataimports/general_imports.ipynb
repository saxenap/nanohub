{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general_import.py transposed to a .ipynb file for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:11:55.160260Z",
     "iopub.status.busy": "2021-12-09T18:11:55.159791Z",
     "iopub.status.idle": "2021-12-09T18:11:55.184242Z",
     "shell.execute_reply": "2021-12-09T18:11:55.179729Z",
     "shell.execute_reply.started": "2021-12-09T18:11:55.160160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:11:57.572941Z",
     "iopub.status.busy": "2021-12-09T18:11:57.571616Z",
     "iopub.status.idle": "2021-12-09T18:12:02.229039Z",
     "shell.execute_reply": "2021-12-09T18:12:02.226060Z",
     "shell.execute_reply.started": "2021-12-09T18:11:57.572771Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mnanoHUB - Serving Students, Researchers & Instructors\u001b[0m\n",
      "Obtained Salesforce access token ...... True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from nanoHUB.application import Application\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "from apiclient import errors\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from httplib2 import Http\n",
    "from nanoHUB.logger import logger\n",
    "application = Application.get_instance()\n",
    "# nanohub_db = application.new_db_engine('nanohub')\n",
    "# nanohub_metrics_db = application.new_db_engine('nanohub_metrics')\n",
    "# wang159_myrmekes_db = application.new_db_engine('wang159_myrmekes')\n",
    "\n",
    "salesforce = application.new_salesforce_engine()\n",
    "db_s = salesforce\n",
    "log = logger('nanoHUB:google_imports')\n",
    "\n",
    "\n",
    "SHOULD_IMPORT_FRESH = False\n",
    "SHOULD_IMPORT_FRESH = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GDrive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:02.234329Z",
     "iopub.status.busy": "2021-12-09T18:12:02.233538Z",
     "iopub.status.idle": "2021-12-09T18:12:02.293289Z",
     "shell.execute_reply": "2021-12-09T18:12:02.290310Z",
     "shell.execute_reply.started": "2021-12-09T18:12:02.234283Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import os\n",
    "import errno\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "## stuff that's rather hard to find from documentation\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "\n",
    "service = application.new_google_api_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:04.788555Z",
     "iopub.status.busy": "2021-12-09T18:12:04.788288Z",
     "iopub.status.idle": "2021-12-09T18:12:04.802748Z",
     "shell.execute_reply": "2021-12-09T18:12:04.797641Z",
     "shell.execute_reply.started": "2021-12-09T18:12:04.788514Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'Salesforce_Imports'\n",
    "TO_IMPORT_FOLDER_NAME = 'To_Import'\n",
    "FAILURES_FOLDER_NAME = 'Import_Issues'\n",
    "SUCCESS_FOLDER_NAME = 'Imported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:09.501460Z",
     "iopub.status.busy": "2021-12-09T18:12:09.499970Z",
     "iopub.status.idle": "2021-12-09T18:12:09.526465Z",
     "shell.execute_reply": "2021-12-09T18:12:09.524799Z",
     "shell.execute_reply.started": "2021-12-09T18:12:09.501404Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_dir = Path(os.getenv('APP_DIR') + '/.cache/')\n",
    "\n",
    "import_dir_path = Path.joinpath(import_dir, FOLDER_NAME)\n",
    "import_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "to_import_dir_path = Path.joinpath(import_dir_path, TO_IMPORT_FOLDER_NAME)\n",
    "to_import_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "failures_import_dir_path = Path.joinpath(import_dir_path, FAILURES_FOLDER_NAME)\n",
    "failures_import_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "success_dir_path = Path.joinpath(import_dir_path, SUCCESS_FOLDER_NAME)\n",
    "success_dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:13.603797Z",
     "iopub.status.busy": "2021-12-09T18:12:13.601787Z",
     "iopub.status.idle": "2021-12-09T18:12:13.620651Z",
     "shell.execute_reply": "2021-12-09T18:12:13.617295Z",
     "shell.execute_reply.started": "2021-12-09T18:12:13.603730Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_folder_id(service, folder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + folder_name + \"'\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def get_subfolder_id(service, parent_folder_id: str, subfolder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + subfolder_name + \"'\" + \" and '\" + parent_folder_id + \"' in parents\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def change_folder_for_file(file_id: str, old_folder_id: str, new_folder_id: str):\n",
    "    return service.files().update(\n",
    "        fileId=file_id,\n",
    "        removeParents=old_folder_id,\n",
    "        addParents=new_folder_id,\n",
    "        fields='id, parents'\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:18.224533Z",
     "iopub.status.busy": "2021-12-09T18:12:18.224147Z",
     "iopub.status.idle": "2021-12-09T18:12:19.376293Z",
     "shell.execute_reply": "2021-12-09T18:12:19.372623Z",
     "shell.execute_reply.started": "2021-12-09T18:12:18.224486Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_id = get_folder_id(service, FOLDER_NAME)\n",
    "log.debug('Found folder: %s (%s)' % (FOLDER_NAME, folder_id))\n",
    "    \n",
    "subfolder_id = get_subfolder_id(service, folder_id, TO_IMPORT_FOLDER_NAME)\n",
    "log.debug('Found subfolder: %s (%s)' % (TO_IMPORT_FOLDER_NAME, subfolder_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:22.735650Z",
     "iopub.status.busy": "2021-12-09T18:12:22.735329Z",
     "iopub.status.idle": "2021-12-09T18:12:23.036790Z",
     "shell.execute_reply": "2021-12-09T18:12:23.034098Z",
     "shell.execute_reply.started": "2021-12-09T18:12:22.735620Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [3985917202 - nanoHUB:google_imports]: Found file: Recitation 2 2021 - A.csv (13l2ZnO3zwSOuDCIMNSpAx_oRsNE9-1au) [3985917202.<module>:21]\n",
      "[INFO] [3985917202 - nanoHUB:google_imports]: Found file: Recitation 2 2021 - R.csv (1UvpwDQAuHJyhau5W0jAl6S7P9uA5q9FY) [3985917202.<module>:21]\n"
     ]
    }
   ],
   "source": [
    "mimetypes = \"\"\"\n",
    "    mimeType = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' or\n",
    "    mimeType = 'application/vnd.ms-excel' or\n",
    "    mimeType = 'text/csv'\n",
    "    \"\"\"\n",
    "file_ids = []\n",
    "file_names = []\n",
    "page_token = None\n",
    "# q = \"(\" + mimetypes + \") and '\" + folder.get('id') + \"' in parents\"\n",
    "while True:\n",
    "    query = \"(\" + mimetypes + \") and '\" + subfolder_id + \"' in parents\"\n",
    "    response = service.files().list(\n",
    "        q = query,\n",
    "        spaces='drive',\n",
    "        pageToken=page_token,\n",
    "        fields=\"nextPageToken, files(id, name)\"\n",
    "    ).execute()\n",
    "    log.debug(response)\n",
    "    files = response.get('files', [])\n",
    "    for file in files:\n",
    "        log.info('Found file: %s (%s)' % (file.get('name'), file.get('id')))\n",
    "        file_names.append(file['name'])\n",
    "        file_ids.append(file['id'])\n",
    "    page_token = response.get('nextPageToken', None)\n",
    "    if page_token is None:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:39.642723Z",
     "iopub.status.busy": "2021-12-09T18:12:39.642112Z",
     "iopub.status.idle": "2021-12-09T18:12:39.654863Z",
     "shell.execute_reply": "2021-12-09T18:12:39.652271Z",
     "shell.execute_reply.started": "2021-12-09T18:12:39.642490Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io \n",
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:40.781128Z",
     "iopub.status.busy": "2021-12-09T18:12:40.780713Z",
     "iopub.status.idle": "2021-12-09T18:12:41.639721Z",
     "shell.execute_reply": "2021-12-09T18:12:41.638244Z",
     "shell.execute_reply.started": "2021-12-09T18:12:40.781087Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_files = []\n",
    "to_import_file_paths = []\n",
    "\n",
    "for filename in os.listdir(to_import_dir_path):\n",
    "    file_path = os.path.join(to_import_dir_path, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "        log.debug(\"Removed existing file %s\" % file_path)\n",
    "    except Exception as e:\n",
    "        log.error('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "try:\n",
    "    for temp_index,f_tbd_id in enumerate(file_ids):\n",
    "        request = service.files().get_media(fileId=f_tbd_id) #,mimeType='text/csv') #if not .csv, then do .export()\n",
    "        fh = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "            log.debug(\"Downloading %s %d%%.\" % (file_names[temp_index], int(status.progress() * 100)))\n",
    "\n",
    "        # The file has been downloaded into RAM, now save it in a file\n",
    "        # https://stackoverflow.com/questions/60111361/how-to-download-a-file-from-google-drive-using-python-and-the-drive-api-v3\n",
    "        fh.seek(0)\n",
    "\n",
    "        download_filepath = Path(to_import_dir_path, file_names[temp_index])\n",
    "        with open(download_filepath, 'wb') as f:\n",
    "            shutil.copyfileobj(fh, f)\n",
    "\n",
    "        list_files.append(file_names[temp_index])\n",
    "        to_import_file_paths.append(download_filepath)\n",
    "\n",
    "    log.debug(\"Finished downloading files\")\n",
    "except Exception as e:\n",
    "    log.error(\"Error downloading file: \" + str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:42.055060Z",
     "iopub.status.busy": "2021-12-09T18:12:42.053416Z",
     "iopub.status.idle": "2021-12-09T18:12:42.063733Z",
     "shell.execute_reply": "2021-12-09T18:12:42.061200Z",
     "shell.execute_reply.started": "2021-12-09T18:12:42.054972Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sequential Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:44.493678Z",
     "iopub.status.busy": "2021-12-09T18:12:44.492936Z",
     "iopub.status.idle": "2021-12-09T18:12:44.502959Z",
     "shell.execute_reply": "2021-12-09T18:12:44.502007Z",
     "shell.execute_reply.started": "2021-12-09T18:12:44.493580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_files = []\n",
    "fail_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:45.516157Z",
     "iopub.status.busy": "2021-12-09T18:12:45.515847Z",
     "iopub.status.idle": "2021-12-09T18:12:45.526993Z",
     "shell.execute_reply": "2021-12-09T18:12:45.523660Z",
     "shell.execute_reply.started": "2021-12-09T18:12:45.516120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chardet import detect\n",
    "import cchardet\n",
    "# get file encoding type\n",
    "def get_encoding_type(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:46.346783Z",
     "iopub.status.busy": "2021-12-09T18:12:46.345153Z",
     "iopub.status.idle": "2021-12-09T18:12:46.575055Z",
     "shell.execute_reply": "2021-12-09T18:12:46.572107Z",
     "shell.execute_reply.started": "2021-12-09T18:12:46.346722Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [api - charset_normalizer]: Detected a SIG or BOM mark on first 3 byte(s). Priority +1 given for utf_8. [api.from_bytes:159]\n",
      "[INFO] [api - charset_normalizer]: Code page utf_8 is a multi byte encoding table and it appear that at least one character was encoded using n-bytes. [api.from_bytes:257]\n",
      "[INFO] [api - charset_normalizer]: utf_8 passed initial chaos probing. Mean measured chaos is 1.720000 % [api.from_bytes:352]\n",
      "[INFO] [api - charset_normalizer]: We detected language [('English', 1.0), ('Swedish', 1.0), ('Indonesian', 1.0), ('Dutch', 0.9833), ('French', 0.9167), ('Finnish', 0.9167)] using utf_8 [api.from_bytes:382]\n",
      "[INFO] [api - charset_normalizer]: utf_8 is most likely the one. Stopping the process. [api.from_bytes:417]\n",
      "[INFO] [api - charset_normalizer]: Detected a SIG or BOM mark on first 3 byte(s). Priority +1 given for utf_8. [api.from_bytes:159]\n",
      "[INFO] [api - charset_normalizer]: Code page utf_8 is a multi byte encoding table and it appear that at least one character was encoded using n-bytes. [api.from_bytes:257]\n",
      "[INFO] [api - charset_normalizer]: utf_8 passed initial chaos probing. Mean measured chaos is 1.020000 % [api.from_bytes:352]\n",
      "[INFO] [api - charset_normalizer]: We detected language [('English', 1.0), ('Norwegian', 1.0), ('Dutch', 0.9917), ('Finnish', 0.9791), ('Hungarian', 0.9583)] using utf_8 [api.from_bytes:382]\n",
      "[INFO] [api - charset_normalizer]: utf_8 is most likely the one. Stopping the process. [api.from_bytes:417]\n"
     ]
    }
   ],
   "source": [
    "from charset_normalizer import CharsetNormalizerMatches as CnM\n",
    "\n",
    "for file in to_import_file_paths:\n",
    "\n",
    "    log.debug(\"Processing file: \" + str(file) + '---->')\n",
    "\n",
    "    #from_chardet = get_encoding_type(str(file))\n",
    "    #log.info('From chardet ----> ' + from_chardet['encoding'])\n",
    "\n",
    "    from_normalizer = CnM.from_path(str(file)).best().first().encoding\n",
    "    log.debug('From Normalizer ----> ' + from_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:47.596310Z",
     "iopub.status.busy": "2021-12-09T18:12:47.594862Z",
     "iopub.status.idle": "2021-12-09T18:12:47.607192Z",
     "shell.execute_reply": "2021-12-09T18:12:47.605979Z",
     "shell.execute_reply.started": "2021-12-09T18:12:47.596247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from charset_normalizer import from_path\n",
    "\n",
    "def change_encoding(file_path: str):\n",
    "    try:\n",
    "        with open(file_path, 'w') as filetowrite:\n",
    "            filetowrite.write(results)\n",
    "    except IOError as e:\n",
    "        log.debug('Sadly, we are unable to perform charset normalization.', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:48.860419Z",
     "iopub.status.busy": "2021-12-09T18:12:48.860057Z",
     "iopub.status.idle": "2021-12-09T18:12:48.878806Z",
     "shell.execute_reply": "2021-12-09T18:12:48.876874Z",
     "shell.execute_reply.started": "2021-12-09T18:12:48.860366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file_to_df(file_path: str) -> pd.DataFrame:\n",
    "    possible_encodings = [\n",
    "        'utf-8', 'cp1252', 'utf-16', 'cp1254', 'cp775', 'utf-8-sig', 'iso-8859-1', 'unicode_escape', 'gbk','latin1'\n",
    "    ]\n",
    "    possible_separators = [',', '\\t']\n",
    "    possible_engines = ['c', 'python']\n",
    "\n",
    "    for encoding in possible_encodings:\n",
    "        for sep in possible_separators:\n",
    "            for engine in possible_engines:\n",
    "                try:\n",
    "                    idf = pd.read_csv(file_path, encoding=encoding, sep=sep, engine=engine)\n",
    "                    log.info(\"File with Encoding: %s and Separator: %s processed using engine %s\" % (encoding, sep, engine))\n",
    "                    return idf\n",
    "                except Exception as e:\n",
    "                    log.debug(\"Decoding failed with %s, %s, %s\" % (encoding, sep, engine))\n",
    "                    pass\n",
    "    log.info(\"Either the file (%s) is empty or unable to decode.\" % file_path)\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:51.632615Z",
     "iopub.status.busy": "2021-12-09T18:12:51.630184Z",
     "iopub.status.idle": "2021-12-09T18:12:51.662402Z",
     "shell.execute_reply": "2021-12-09T18:12:51.659963Z",
     "shell.execute_reply.started": "2021-12-09T18:12:51.632523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_excel_file(file):\n",
    "    try:\n",
    "        xl = pd.ExcelFile(file)\n",
    "        log.debug(\"sheet names: \" + xl.sheet_names)# see all sheet names\n",
    "        sheet_names = xl.parse(xl.sheet_names) #this already performs an import\n",
    "        idf = pd.read_excel(file,sheet_name=xl.sheet_names[0],header=0)#,skiprows=1)\n",
    "    except:\n",
    "        log.error('error bad lines, csv/xls/xlsx import failed')\n",
    "        raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-09T18:12:53.855851Z",
     "iopub.status.busy": "2021-12-09T18:12:53.853694Z",
     "iopub.status.idle": "2021-12-09T18:14:34.125069Z",
     "shell.execute_reply": "2021-12-09T18:14:34.122152Z",
     "shell.execute_reply.started": "2021-12-09T18:12:53.855792Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [141495456 - nanoHUB:google_imports]: Reading /home/saxenap/nanoHUB/.cache/Salesforce_Imports/To_Import/Recitation 2 2021 - A.csv [141495456.<module>:6]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: File type .csv [141495456.<module>:9]\n",
      "[INFO] [34983876 - nanoHUB:google_imports]: File with Encoding: utf-8 and Separator: , processed using engine c [34983876.read_file_to_df:13]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: no name [141495456.<module>:57]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of good emails found: 46 [141495456.<module>:80]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of bad emails found: 0 [141495456.<module>:81]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb6lAAA\n",
      "{\"id\":\"7505w00000bnb6lAAA\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:12:54.000+0000\",\"systemModstamp\":\"2021-12-09T18:12:54.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000bnb6lAAA\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:12:54.000+0000\",\"systemModstamp\":\"2021-12-09T18:12:59.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":17394,\"retries\":0,\"totalProcessingTime\":1180}\n",
      "{\"id\":\"7505w00000bnb6lAAA\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:12:54.000+0000\",\"systemModstamp\":\"2021-12-09T18:12:59.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":17394,\"retries\":0,\"totalProcessingTime\":1180}\n",
      "{\"id\":\"7505w00000bnb6lAAA\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:12:54.000+0000\",\"systemModstamp\":\"2021-12-09T18:13:17.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":266723,\"retries\":0,\"totalProcessingTime\":16730}\n",
      "[Success] Bulk job completed successfully.\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of contacts: 33 [141495456.<module>:157]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb6qAAA\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000bnb6qAAA\n",
      "[Success] Closing job successful. Job ID = 7505w00000bnb6qAAA\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb6vAAA\n",
      "{\"id\":\"7505w00000bnb6vAAA\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:13:34.000+0000\",\"systemModstamp\":\"2021-12-09T18:13:35.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000bnb6vAAA\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:13:34.000+0000\",\"systemModstamp\":\"2021-12-09T18:13:37.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":12510,\"retries\":0,\"totalProcessingTime\":1152}\n",
      "[Success] Bulk job completed successfully.\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: company exist, using it [141495456.<module>:229]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of existing leads: 6 [141495456.<module>:309]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb75AAA\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000bnb75AAA\n",
      "[Success] Closing job successful. Job ID = 7505w00000bnb75AAA\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of new leads: 6 [141495456.<module>:319]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb7AAAQ\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000bnb7AAAQ\n",
      "[Success] Closing job successful. Job ID = 7505w00000bnb7AAAQ\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Reading /home/saxenap/nanoHUB/.cache/Salesforce_Imports/To_Import/Recitation 2 2021 - R.csv [141495456.<module>:6]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: File type .csv [141495456.<module>:9]\n",
      "[INFO] [34983876 - nanoHUB:google_imports]: File with Encoding: utf-8 and Separator: , processed using engine c [34983876.read_file_to_df:13]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: no name [141495456.<module>:57]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of good emails found: 100 [141495456.<module>:80]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of bad emails found: 0 [141495456.<module>:81]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb7FAAQ\n",
      "{\"id\":\"7505w00000bnb7FAAQ\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:13:50.000+0000\",\"systemModstamp\":\"2021-12-09T18:13:50.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000bnb7FAAQ\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:13:50.000+0000\",\"systemModstamp\":\"2021-12-09T18:13:50.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":17395,\"retries\":0,\"totalProcessingTime\":1292}\n",
      "{\"id\":\"7505w00000bnb7FAAQ\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:13:50.000+0000\",\"systemModstamp\":\"2021-12-09T18:14:04.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":266724,\"retries\":0,\"totalProcessingTime\":13993}\n",
      "[Success] Bulk job completed successfully.\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of contacts: 63 [141495456.<module>:157]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb7UAAQ\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000bnb7UAAQ\n",
      "[Success] Closing job successful. Job ID = 7505w00000bnb7UAAQ\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb7ZAAQ\n",
      "{\"id\":\"7505w00000bnb7ZAAQ\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:14:20.000+0000\",\"systemModstamp\":\"2021-12-09T18:14:20.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000bnb7ZAAQ\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-12-09T18:14:20.000+0000\",\"systemModstamp\":\"2021-12-09T18:14:22.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":12516,\"retries\":0,\"totalProcessingTime\":1097}\n",
      "[Success] Bulk job completed successfully.\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: company exist, using it [141495456.<module>:229]\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of existing leads: 13 [141495456.<module>:309]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb7eAAA\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000bnb7eAAA\n",
      "[Success] Closing job successful. Job ID = 7505w00000bnb7eAAA\n",
      "[INFO] [141495456 - nanoHUB:google_imports]: Number of new leads: 13 [141495456.<module>:319]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000bnb7jAAA\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000bnb7jAAA\n",
      "[Success] Closing job successful. Job ID = 7505w00000bnb7jAAA\n"
     ]
    }
   ],
   "source": [
    "class EmptyOrEncodingIssue(TypeError):\n",
    "    \"\"\"Either the file is empty or was unable to be read properly.\"\"\"\n",
    "\n",
    "for file in to_import_file_paths:\n",
    "    idf = pd.DataFrame()\n",
    "    log.info(\"Reading %s\" % file)\n",
    "    try:\n",
    "        f_type = file.suffix\n",
    "        log.info(\"File type %s\" % f_type)\n",
    "        #pandas import # add exception handling - UnicodeError\n",
    "        if f_type == '.csv':\n",
    "            log.debug(\"csv file type identified\")\n",
    "            idf = read_file_to_df(file)\n",
    "            if idf.empty:\n",
    "                raise EmptyOrEncodingIssue\n",
    "        elif f_type == '.xlsx' or f_type == '.xls':\n",
    "            process_excel_file(file)\n",
    "\n",
    "        ## remove leading and trailing spaces ## add str.strip spaces for all columns and rename\n",
    "        log.debug(idf.columns)\n",
    "        prev_idf_cols = idf.columns\n",
    "        idf_cols = [i.strip() for i in idf.columns]\n",
    "        idf.columns = idf.columns.str.strip()\n",
    "        idf = idf.rename(columns={j:idf_cols[i] for i,j in enumerate(prev_idf_cols)})\n",
    "\n",
    "        # if engagement venue does not exist, then create a flag with that entry\n",
    "        try:\n",
    "            #if engagement venue is specified\n",
    "            log.debug(idf['Engagement Venue'])\n",
    "            log.debug(idf['First Name'])\n",
    "            idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "            # idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "        except:\n",
    "            #decide the event\n",
    "            #event_extract = xl.sheet_names[0]\n",
    "            #idf['Engagement Venue'] = event_extract\n",
    "            file_name = Path(file).name\n",
    "            idf['Venue__c'] = file_name.split('.')[0]\n",
    "\n",
    "            try:\n",
    "                #name extract\n",
    "                names = idf['Name'].to_list()\n",
    "                #display(idf)\n",
    "                \n",
    "                fname = deepcopy(names)\n",
    "                lname = deepcopy(names)\n",
    "                for ind, val in enumerate(names):\n",
    "                    val = val.split(' ')\n",
    "                    fname[ind] = val[-1]\n",
    "                    lname[ind] = val[0]\n",
    "\n",
    "                idf['firstname'] = fname\n",
    "                idf['lastname'] = lname\n",
    "                name_flag = True\n",
    "            except:\n",
    "                name_flag = False\n",
    "                log.info('no name')\n",
    "\n",
    "        #rename columns\n",
    "        idf = idf.rename(columns={'email':'Email','EMAIL':'Email','E-mail Address':'Email',\\\n",
    "                    'Email Address':'Email','Recipient Email':'Email'})\n",
    "        idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "        idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname','FirstName':'firstname','LastName':'lastname'})\n",
    "\n",
    "        idf = idf.rename(columns={0:'Email'})\n",
    "        if name_flag == True:\n",
    "            idf = idf.drop(columns='Name')#['NAME','LAST NAME','FIRST NAME'])            \n",
    "        idf = idf.dropna(subset=['Email'])\n",
    "\n",
    "        #email check rows\n",
    "        grows = []\n",
    "        brows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            if '@' in val:\n",
    "                grows.append(ind)\n",
    "            else:\n",
    "                brows.append(ind)    \n",
    "        idf = idf.iloc[grows,:].reset_index().iloc[:,1:]        \n",
    "\n",
    "        log.info(\"Number of good emails found: %d\" %len(grows))\n",
    "        log.info(\"Number of bad emails found: %d\" % len(brows))\n",
    "\n",
    "        ## Import in contacts\n",
    "        os_name = os.name\n",
    "        sys_name = platform.system() #Linux, Darwin, Windows    \n",
    "\n",
    "        # salesforce queries for contact data\n",
    "        # deciding the queries\n",
    "        import_df_cols = deepcopy(idf.columns)\n",
    "        nh_id_flag = False\n",
    "        email_flag = False\n",
    "        if 'nanoHUB_user_ID__c' in import_df_cols:\n",
    "            nh_id_flag = True\n",
    "\n",
    "        if 'Email' in import_df_cols:\n",
    "            email_flag = True    \n",
    "\n",
    "        if nh_id_flag == True and email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)\n",
    "        elif email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)    \n",
    "\n",
    "        # find all existing contacts\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        grows = []\n",
    "        brows = [] #dont need the sf_bad_rows as send to leads\n",
    "        sf_grows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                grows.append(ind)\n",
    "                sf_grows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                brows.append(ind)   \n",
    "\n",
    "        # pull the matching SF entries and the matching import df entries\n",
    "        sf_df_match = sf_df.iloc[sf_grows,:].reset_index().iloc[:,1:]\n",
    "        idf_match = idf.iloc[grows,:].reset_index().iloc[:,1:]\n",
    "        lead_df = idf.iloc[brows,:].reset_index().iloc[:,1:] #use this in next section\n",
    "\n",
    "        #log.debug(sf_df_match.head())\n",
    "        #log.debug(idf_match.head())\n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                val.append(idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val\n",
    "\n",
    "        #log.debug(sf_df_match.head())\n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind]=';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## encoding correction for dashes\n",
    "        venues = sf_df_match['Venue__c'].apply(lambda x: x.replace('창\\x80\\x93','-'))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        sf_df_match = sf_df_match.drop_duplicates()\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        sf_df_match = sf_df_match[['Email','Venue__c','Id']]\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## send to SF\n",
    "        # rebuild api object\n",
    "        db_s_c = deepcopy(db_s)\n",
    "\n",
    "        log.info(\"Number of contacts: %d\" % len(sf_df_match.index))\n",
    "        # send data to SF\n",
    "        db_s_c.object_id = 'Contact'\n",
    "        # db_s_c.external_id = 'nanoHUB_user_ID__c'\n",
    "        db_s_c.external_id = 'Id'\n",
    "\n",
    "        db_s_c.send_data(sf_df_match)\n",
    "\n",
    "        ## find leads and send them to SF as well\n",
    "        #pull all current leads\n",
    "        sf_df = db_s.query_data('SELECT Id, Email, Venue__c, SF_indexer__c FROM Lead')    \n",
    "        # find the max sf_indexer\n",
    "        indexers = sf_df['SF_indexer__c'].fillna(0).to_list()\n",
    "        max_ind = max(indexers)    \n",
    "\n",
    "        # find all existing leads\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        m_rows = []\n",
    "        nm_rows = [] #don't need sf no match rows\n",
    "        sf_mrows = []    \n",
    "\n",
    "        for ind,val in enumerate(lead_df['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                m_rows.append(ind)\n",
    "                sf_mrows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                nm_rows.append(ind)    \n",
    "\n",
    "        # filter the matches\n",
    "        sf_df_match = sf_df.iloc[sf_mrows,:].reset_index().iloc[:,1:]\n",
    "        join_idf = lead_df.iloc[m_rows,:].reset_index().iloc[:,1:]\n",
    "        new_idf = lead_df.iloc[nm_rows,:].reset_index().iloc[:,1:]    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                #if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #    val.remove('MSE Summer Webinar Series 2020')\n",
    "                #    if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #        val.remove('MSE Summer Webinar Series 2020')\n",
    "                val.append(join_idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = join_idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val    \n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind] = ';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        # assign new SF_indexers for the new leads\n",
    "        new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "        new_idf['SF_indexer__c'] = range(int(max_ind)+1,new_max_ind+1)    \n",
    "\n",
    "        # need non-empty company field\n",
    "        new_idf['Company'] = '-'    \n",
    "\n",
    "        # ensure '창\\x80\\x93' has been replaced\n",
    "        sf_df_match['Venue__c'] = sf_df_match['Venue__c'].apply(lambda x: x.replace('창\\x80\\x93','-'))\n",
    "        new_idf['Venue__c'] = new_idf['Venue__c'].apply(lambda x: x.replace('창\\x80\\x93','-'))\n",
    "\n",
    "        sf_df_match['Company'] = '-'\n",
    "        new_idf['Company'] = '-'\n",
    "        #log.debug(new_idf)\n",
    "        # sys.exit()\n",
    "\n",
    "        # populate the company fields for sf_df_match and new_idf by comparing email addresses\n",
    "        if 'Company' in idf.columns:\n",
    "            log.info(\"company exist, using it\")\n",
    "            # comparison for sf_df_match\n",
    "\n",
    "            sf_df_match_comp_ind = sf_df_match.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in sf_df_match['Email'].to_list():\n",
    "                    sf_df_match_ind = sf_df_match['Email'].to_list().index(val)\n",
    "                    sf_df_match.iloc[sf_df_match_ind,sf_df_match_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "\n",
    "            # sf_df_match sf_indexer if not available\n",
    "            sf_df_match_sf_ind = sf_df_match.columns.to_list().index('SF_indexer__c')\n",
    "            new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "            for ind,val in enumerate(sf_df_match['SF_indexer__c'].to_list()):\n",
    "                if type(val) != int and type(val) != float:\n",
    "                    sf_df_match.iloc[ind,sf_df_match_sf_ind] = new_max_ind+1\n",
    "                    new_max_ind += 1\n",
    "\n",
    "            new_idf_comp_ind = new_idf.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in new_idf['Email'].to_list():\n",
    "                    new_idf_ind = new_idf['Email'].to_list().index(val)\n",
    "                    new_idf.iloc[new_idf_ind,new_idf_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "        else:\n",
    "            log.info('no company in import list, set to -')    \n",
    "        \n",
    "        if sf_df_match.shape[0] == 0 and new_idf.shape[0] == 0: \n",
    "            log.info('no leads to import, they were all contacts')\n",
    "        else:\n",
    "            sf_df_match = sf_df_match.fillna('-')\n",
    "            new_idf = new_idf.fillna('-')\n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "            except:\n",
    "                log.info('names are good')          \n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                new_idf['Company'] = '-'\n",
    "            except: # names are not present\n",
    "                try:\n",
    "                    tempnames = new_idf['Faculty'].apply(lambda x: x.split(' '))\n",
    "                    tf_names = [i[0] for i in tempnames]\n",
    "                    tl_names = [i[-1] if len(i[-1]) > 0 else '-' for i in tempnames]\n",
    "                    new_idf['firstname'] = tf_names\n",
    "                    new_idf['lastname'] = tl_names\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'    \n",
    "                except:\n",
    "                    tempnames = new_idf['Name'].to_list()\n",
    "                    temp_fname = []\n",
    "                    temp_lname = []\n",
    "                    for t_ind,t_val in enumerate(tempnames):\n",
    "                        t_val = t_val.split(' ')\n",
    "                        temp_fname.append(t_val[0])\n",
    "                        if len(t_val[-1]) > 0:\n",
    "                            temp_lname.append(t_val[-1])\n",
    "                        else:\n",
    "                            temp_lname.append('-')\n",
    "\n",
    "                    new_idf['firstname'] = temp_fname\n",
    "                    new_idf['lastname'] = temp_lname\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'\n",
    "\n",
    "            #drop duplicate rows\n",
    "            sf_df_match = sf_df_match.drop_duplicates(subset='SF_indexer__c')\n",
    "            new_idf = new_idf.drop_duplicates()    \n",
    "\n",
    "            sf_df_match['Company'] = sf_df_match['Company'].replace('  ','-')\n",
    "            sf_df_match = sf_df_match.drop(columns='SF_indexer__c')\n",
    "\n",
    "            #send the matching ones\n",
    "            db_s_l1 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l1.object_id = 'Lead'\n",
    "            # db_s_l1.external_id = 'SF_indexer__c'\n",
    "            db_s_l1.external_id = 'Id'\n",
    "            \n",
    "            log.info(\"Number of existing leads: %d\" % len(new_idf.index))\n",
    "            db_s_l1.send_data(sf_df_match)\n",
    "\n",
    "            #send the new ones\n",
    "            db_s_l2 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l2.object_id = 'Lead'\n",
    "            db_s_l2.external_id = 'SF_indexer__c'\n",
    "            \n",
    "            log.info(\"Number of new leads: %d\" % len(new_idf.index))\n",
    "            db_s_l2.send_data(new_idf)\n",
    "\n",
    "        success_files.append(file)\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(\"Error with file: \" + str(e))\n",
    "        fail_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:14:34.131686Z",
     "iopub.status.busy": "2021-12-09T18:14:34.128756Z",
     "iopub.status.idle": "2021-12-09T18:14:34.147095Z",
     "shell.execute_reply": "2021-12-09T18:14:34.145331Z",
     "shell.execute_reply.started": "2021-12-09T18:14:34.131626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tbd_imp_names = file_names\n",
    "tbd_imp_ids = file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T18:14:34.148717Z",
     "iopub.status.busy": "2021-12-09T18:14:34.148413Z",
     "iopub.status.idle": "2021-12-09T18:14:36.559623Z",
     "shell.execute_reply": "2021-12-09T18:14:36.554601Z",
     "shell.execute_reply.started": "2021-12-09T18:14:34.148676Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1136129925 - nanoHUB:google_imports]: Success in proccesing file: Recitation 2 2021 - A.csv with id: 13l2ZnO3zwSOuDCIMNSpAx_oRsNE9-1au [1136129925.<module>:12]\n",
      "[INFO] [1136129925 - nanoHUB:google_imports]: Success in proccesing file: Recitation 2 2021 - R.csv with id: 1UvpwDQAuHJyhau5W0jAl6S7P9uA5q9FY [1136129925.<module>:12]\n"
     ]
    }
   ],
   "source": [
    "imported_folder_id = get_subfolder_id(service, folder_id, SUCCESS_FOLDER_NAME)\n",
    "failure_folder_id = get_subfolder_id(service, folder_id, FAILURES_FOLDER_NAME)\n",
    "\n",
    "if not success_files and not fail_files:\n",
    "    log.info(\"No files found to process.\")\n",
    "\n",
    "else: \n",
    "    for i in success_files:\n",
    "        file_name = Path(i).name \n",
    "        t_index = tbd_imp_names.index(file_name) \n",
    "        file_id = tbd_imp_ids[t_index]\n",
    "        log.info(\"Success in proccesing file: %s with id: %s\" % (file_name, file_id))\n",
    "        change_folder_for_file(file_id, subfolder_id, imported_folder_id)\n",
    "        i.rename(Path(success_dir_path) / i.name)\n",
    "\n",
    "    for i in fail_files:\n",
    "        file_name = Path(i).name \n",
    "        t_index = tbd_imp_names.index(file_name)\n",
    "        file_id = tbd_imp_ids[t_index]\n",
    "        log.info(\"Failure in proccesing file: %s with id: %s\" % (file_name, file_id))\n",
    "        change_folder_for_file(fid, subfolder_id, failure_folder_id)\n",
    "        i.rename(Path(failures_import_dir_path) / i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
