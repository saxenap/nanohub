{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general_import.py transposed to a .ipynb file for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "```````````````````````````````$$$$$``````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "`````````````````````````````$$$$`$$$$````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "`````````````````$$$$$```````$$`````$$$```````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````````````$$```$$``````$$$$``$$$````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````````````$$$$$$$```````$$$$$$``````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "``````$$$$````````$$$$$```````$$$``````$$$$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````$$$$`$$$$```````$$$$$$$$$$$$$``````$$``$$$````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````$$`````$$$```$$$$$$$$$`$$$$$$$$``$$$$$$$$$````````````````````````````````````````````````````````$$$```````$$$$``$$$```````$$$```$$$$$$$$$$$`````\n",
      "````$$````$$$$``$$$$$``````````$$$$$$$$$$$$$$````````````````````````````````````````````````````````$$$$```````$$$$`$$$$```````$$$$`$$$$$$$$$$$$$$```\n",
      "`````$$$$$$$`$$$$$$```````````````$$$$```````````$$$$$$$``````$$$$$$$$`````$$$$$$$```````$$$$$$$`````$$$$```````$$$$`$$$$```````$$$$`$$$$``````$$$$```\n",
      "``````````````$$$`````$$$$$$$$`````$$$$````````$$$$$$$$$$$``$$$$$$$$$$$``$$$$$$$$$$$```$$$$$$$$$$$```$$$$``````$$$$$`$$$$```````$$$$`$$$$`````$$$$$```\n",
      "``````````````$$$`````$$````$$`````$$$$````````$$$`````$$$$````````$$$$$`$$$$````$$$$`$$$$`````$$$$``$$$$$$$$$$$$$$$`$$$$```````$$$$`$$$$$$$$$$$$$$```\n",
      "`````$$$$$```$$$$`````$$````$$`````$$$$````````$$$`````$$$$``$$$$$$$$$$$`$$$$````$$$$`$$$$`````$$$$``$$$$```````$$$$`$$$$```````$$$$`$$$$``````$$$$$``\n",
      "```$$$``$$$$$$$$$`````$$````$$`````$$$$````````$$$`````$$$$`$$$$````$$$$`$$$$````$$$$`$$$$`````$$$$``$$$$```````$$$$`$$$$$``````$$$``$$$$```````$$$$``\n",
      "````$$$`$$$````$$$````````````````$$$$`````````$$$`````$$$$`$$$$$$$$$$$``$$$$````$$$$``$$$$$$$$$$$```$$$$```````$$$$``$$$$$$$$$$$$$``$$$$$$$$$$$$$$```\n",
      "``````$$$```````$$$$$``````````$$$$$$``````````$$$`````$$$$``$$$$$$$$$```$$$$````$$$$````$$$$$$$`````$$$$```````$$$$````$$$$$$$$$````$$$$$$$$$$$$$````\n",
      "````````````````$$$$$$$$$```$$$$$$$$``````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "```````````$$$$$$$``$$$$$$$$$$$$```$$$$$$$$```````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````$$$$$$$$$````````$$$````````$$$``$$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "```````$$$````$$$````````$$$```````$$$````$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````$$````$$$``````$$$$$$$``````$$$``$$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "`````````$$$$$$````````$$$``$$````````$$$$````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````````````````````$$$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "\n",
      "Serving Students, Researchers & Instructors\n",
      "\n",
      "Obtained Salesforce access token ...... True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from nanoHUB.application import Application\n",
    "\n",
    "application = Application.get_instance()\n",
    "# nanohub_db = application.new_db_engine('nanohub')\n",
    "# nanohub_metrics_db = application.new_db_engine('nanohub_metrics')\n",
    "# wang159_myrmekes_db = application.new_db_engine('wang159_myrmekes')\n",
    "\n",
    "salesforce = application.new_salesforce_engine()\n",
    "db_s = salesforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GDrive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "## stuff that's rather hard to find from documentation\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.install', 'https://www.googleapis.com/auth/drive', \\\n",
    "'https://www.googleapis.com/auth/docs', 'https://www.googleapis.com/auth/drive.readonly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = None \n",
    "# The file token.json stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists(cwd+'/credentials/token.json'):\n",
    "    creds = Credentials.from_authorized_user_file(cwd+'/credentials/token.json', SCOPES)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(cwd+'/credentials/oauth_credentials.json', SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(cwd+'/credentials/token.json', 'w') as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "# creds = service_account.Credentials.from_service_account_file(cwd+'/credentials/service_acc.json', scopes=SCOPES) \n",
    "# creds = service_account.Credentials.from_service_account_file(cwd+'/credentials/service_acc2.json', scopes=SCOPES) \n",
    "service = build('drive', 'v3', credentials=creds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCN_master_id = '1OjlcHKkyKe9Uw_3iTFxIUafdtD1NNXKC' \n",
    "sf_import_master_id = '1Hg6UQmf5z2ZRqOwVnOHMqjLwZkZN-jOh' \n",
    "tbd_import_id = '1gIbrfPnNs8TZrhTarhWOaCl8Bts5PNX8' \n",
    "failure_id = '1WMU4A6IMQhtIZ4rehpFiKiyllc718U_1' \n",
    "success_id = '1lf2vTUWiZHLEhfzM8m3Tym4j9UGgdhMZ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd_imp_files = service.files().list(q=\"'\"+tbd_import_id + \"' in parents\",\n",
    "                                      spaces='drive',\n",
    "                                      fields='nextPageToken, files(id, name)').execute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'files': [{'id': '1e0n-6Kl1pUcIGajvd_QcCbr_Xuk2HdZB',\n",
       "   'name': 'Erin R1 Faculty Scrape 2021.csv'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbd_imp_files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd_imp_ids = [] \n",
    "tbd_imp_fnames = [] \n",
    "for temp_file_dict in tbd_imp_files['files']:\n",
    "    tbd_imp_ids.append(temp_file_dict['id']) \n",
    "    tbd_imp_fnames.append(temp_file_dict['name']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "for temp_index,f_tbd_id in enumerate(tbd_imp_ids): \n",
    "    request = service.files().get_media(fileId=f_tbd_id) #,mimeType='text/csv') #if not .csv, then do .export()\n",
    "    fh = io.BytesIO() \n",
    "    downloader = MediaIoBaseDownload(fh, request) \n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "        \n",
    "    # The file has been downloaded into RAM, now save it in a file\n",
    "    # https://stackoverflow.com/questions/60111361/how-to-download-a-file-from-google-drive-using-python-and-the-drive-api-v3\n",
    "    fh.seek(0)\n",
    "    with open(cwd+'/To_Import/'+tbd_imp_fnames[temp_index], 'wb') as f:\n",
    "        shutil.copyfileobj(fh, f) #, length=131072)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Erin R1 Faculty Scrape 2021.csv']\n"
     ]
    }
   ],
   "source": [
    "list_files = [name for name in os.listdir(cwd+'/To_Import')] #if os.path.isfile(name)] \n",
    "print(list_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sequential Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_files = []\n",
    "fail_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no name\n",
      "815\n",
      "76\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000ZGmNPAA1\n",
      "{\"id\":\"7505w00000ZGmNPAA1\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-08-15T20:46:08.000+0000\",\"systemModstamp\":\"2021-08-15T20:46:08.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":803,\"retries\":0,\"totalProcessingTime\":382}\n",
      "{\"id\":\"7505w00000ZGmNPAA1\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-08-15T20:46:08.000+0000\",\"systemModstamp\":\"2021-08-15T20:46:08.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":803,\"retries\":0,\"totalProcessingTime\":382}\n",
      "{\"id\":\"7505w00000ZGmNPAA1\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-08-15T20:46:08.000+0000\",\"systemModstamp\":\"2021-08-15T20:46:08.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":803,\"retries\":0,\"totalProcessingTime\":382}\n",
      "{\"id\":\"7505w00000ZGmNPAA1\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-08-15T20:46:08.000+0000\",\"systemModstamp\":\"2021-08-15T20:46:08.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":803,\"retries\":0,\"totalProcessingTime\":382}\n",
      "{\"id\":\"7505w00000ZGmNPAA1\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-08-15T20:46:08.000+0000\",\"systemModstamp\":\"2021-08-15T20:46:55.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":253074,\"retries\":0,\"totalProcessingTime\":46058}\n",
      "[Success] Bulk job completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/731167127.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sf_df_match['Venue__c'][ind] = val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] Bulk job creation successful. Job ID = 7505w00000ZGmNZAA1\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000ZGmNZAA1\n",
      "[Success] Closing job successful. Job ID = 7505w00000ZGmNZAA1\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000ZGllMAAT\n",
      "{\"id\":\"7505w00000ZGllMAAT\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-08-15T20:47:10.000+0000\",\"systemModstamp\":\"2021-08-15T20:47:10.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000ZGllMAAT\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-08-15T20:47:10.000+0000\",\"systemModstamp\":\"2021-08-15T20:47:13.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":9633,\"retries\":0,\"totalProcessingTime\":1367}\n",
      "[Success] Bulk job completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/731167127.py:209: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sf_df_match['Venue__c'][ind] = val\n",
      "/tmp/ipykernel_20/731167127.py:296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_idf['Company'] = '-'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no company in import list, set to -\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000ZGmNeAAL\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000ZGmNeAAL\n",
      "[Success] Closing job successful. Job ID = 7505w00000ZGmNeAAL\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000ZGmJnAAL\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000ZGmJnAAL\n",
      "[Success] Closing job successful. Job ID = 7505w00000ZGmJnAAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nil\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Deep copy operation on arbitrary Python objects.\n",
       "\n",
       "See the module's __doc__ string for more info.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/lib/python3.8/copy.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file in list_files:\n",
    "    try:\n",
    "        f_type = file.split('.')[-1] \n",
    "\n",
    "        #pandas import # add exception handling - UnicodeError\n",
    "        if f_type == 'csv':\n",
    "            try:\n",
    "                idf = pd.read_csv(cwd+'/To_Import/'+file,encoding='cp1252')\n",
    "            except:\n",
    "                try:\n",
    "                    idf = pd.read_csv(cwd+'/To_Import/'+file,encoding='utf-16')\n",
    "                except:\n",
    "                    try:\n",
    "                        idf = pd.read_csv(cwd+'/To_Import/'+file,encoding='cp1252',sep='\\t')\n",
    "                    except:\n",
    "                        try:\n",
    "                            idf = pd.read_csv(cwd+'/To_Import/'+file,encoding='utf-16',sep='\\t')\n",
    "                        except:\n",
    "                            print('error_bad_lines')\n",
    "        elif f_type == 'xlsx' or f_type == 'xls':\n",
    "            xl = pd.ExcelFile(cwd+'/To_Import/'+file)\n",
    "            print(xl.sheet_names)# see all sheet names\n",
    "            sheet_names = xl.parse(xl.sheet_names) #this already performs an import\n",
    "            idf = pd.read_excel(cwd+'/To_Import/'+file,sheet_name=xl.sheet_names[0],header=0)#,skiprows=1)\n",
    "    #     print(idf.head(2))    \n",
    "\n",
    "        ## remove leading and trailing spaces ## add str.strip spaces for all columns and rename\n",
    "        prev_idf_cols = idf.columns\n",
    "        idf_cols = [i.strip() for i in idf.columns]\n",
    "\n",
    "        idf = idf.rename(columns={j:idf_cols[i] for i,j in enumerate(prev_idf_cols)})\n",
    "    #     display(idf.columns)\n",
    "    #     # print(prev_idf_cols)\n",
    "    #     # print(idf_cols)\n",
    "\n",
    "        # if engagement venue does not exist, then create a flag with that entry\n",
    "        try:\n",
    "            #if engagement venue is specified\n",
    "            print(idf['Engagement Venue'])\n",
    "            print(idf['First Name'])\n",
    "            idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "            idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "        except:\n",
    "            #decide the event\n",
    "            #event_extract = xl.sheet_names[0]\n",
    "            #idf['Engagement Venue'] = event_extract\n",
    "            idf['Venue__c'] = file.split('.')[0]\n",
    "\n",
    "            try:\n",
    "                #name extract\n",
    "                names = idf['Name'].to_list()\n",
    "                from copy import deepcopy?\n",
    "                fname = deepcopy(names)\n",
    "                lname = deepcopy(names)\n",
    "                for ind, val in enumerate(names):\n",
    "                    val = val.split(' ')\n",
    "                    fname[ind] = val[-1]\n",
    "                    lname[ind] = val[0]\n",
    "\n",
    "                idf['firstname'] = fname\n",
    "                idf['lastname'] = lname\n",
    "                name_flag = True\n",
    "            except:\n",
    "                name_flag = False\n",
    "                print('no name')\n",
    "\n",
    "        #rename columns\n",
    "        idf = idf.rename(columns={'email':'Email','EMAIL':'Email','E-mail Address':'Email',\\\n",
    "                    'Email Address':'Email','Recipient Email':'Email'})\n",
    "        idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "        idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "        idf = idf.rename(columns={0:'Email'})\n",
    "        if name_flag == True:\n",
    "            idf = idf.drop(columns='Name')#['NAME','LAST NAME','FIRST NAME'])            \n",
    "        idf = idf.dropna(subset=['Email'])\n",
    "\n",
    "        #email check rows\n",
    "        grows = []\n",
    "        brows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            if '@' in val:\n",
    "                grows.append(ind)\n",
    "            else:\n",
    "                brows.append(ind)    \n",
    "        idf = idf.iloc[grows,:].reset_index().iloc[:,1:]        \n",
    "\n",
    "        print(len(grows))\n",
    "        print(len(brows))\n",
    "\n",
    "        ## Import in contacts\n",
    "        os_name = os.name\n",
    "        sys_name = platform.system() #Linux, Darwin, Windows    \n",
    "\n",
    "        # salesforce queries for contact data\n",
    "        # deciding the queries\n",
    "        import_df_cols = deepcopy(idf.columns)\n",
    "        nh_id_flag = False\n",
    "        email_flag = False\n",
    "        if 'nanoHUB_user_ID__c' in import_df_cols:\n",
    "            nh_id_flag = True\n",
    "\n",
    "        if 'Email' in import_df_cols:\n",
    "            email_flag = True    \n",
    "\n",
    "        if nh_id_flag == True and email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)\n",
    "        elif email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)    \n",
    "\n",
    "        # find all existing contacts\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        grows = []\n",
    "        brows = [] #dont need the sf_bad_rows as send to leads\n",
    "        sf_grows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                grows.append(ind)\n",
    "                sf_grows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                brows.append(ind)   \n",
    "\n",
    "        # pull the matching SF entries and the matching import df entries\n",
    "        sf_df_match = sf_df.iloc[sf_grows,:].reset_index().iloc[:,1:]\n",
    "        idf_match = idf.iloc[grows,:].reset_index().iloc[:,1:]\n",
    "        lead_df = idf.iloc[brows,:].reset_index().iloc[:,1:] #use this in next section\n",
    "\n",
    "    #     print(sf_df_match.head(2))\n",
    "    #     print(idf_match.head(2))    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                val.append(idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val\n",
    "\n",
    "    #     print(sf_df_match.head(5))\n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind]=';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## encoding correction for dashes\n",
    "        venues = sf_df_match['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        sf_df_match = sf_df_match.drop_duplicates()\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        sf_df_match = sf_df_match[['Email','Venue__c','Id']]\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## send to SF\n",
    "        # rebuild api object\n",
    "        db_s_c = deepcopy(db_s)\n",
    "\n",
    "        # send data to SF\n",
    "        db_s_c.object_id = 'Contact'\n",
    "        # db_s_c.external_id = 'nanoHUB_user_ID__c'\n",
    "        db_s_c.external_id = 'Id'\n",
    "\n",
    "        db_s_c.send_data(sf_df_match)\n",
    "\n",
    "        ## find leads and send them to SF as well\n",
    "        #pull all current leads\n",
    "        sf_df = db_s.query_data('SELECT Id, Email, Venue__c, SF_indexer__c FROM Lead')    \n",
    "        # find the max sf_indexer\n",
    "        indexers = sf_df['SF_indexer__c'].fillna(0).to_list()\n",
    "        max_ind = max(indexers)    \n",
    "\n",
    "        # find all existing leads\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        m_rows = []\n",
    "        nm_rows = [] #don't need sf no match rows\n",
    "        sf_mrows = []    \n",
    "\n",
    "        for ind,val in enumerate(lead_df['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                m_rows.append(ind)\n",
    "                sf_mrows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                nm_rows.append(ind)    \n",
    "\n",
    "        # filter the matches\n",
    "        sf_df_match = sf_df.iloc[sf_mrows,:].reset_index().iloc[:,1:]\n",
    "        join_idf = lead_df.iloc[m_rows,:].reset_index().iloc[:,1:]\n",
    "        new_idf = lead_df.iloc[nm_rows,:].reset_index().iloc[:,1:]    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                #if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #    val.remove('MSE Summer Webinar Series 2020')\n",
    "                #    if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #        val.remove('MSE Summer Webinar Series 2020')\n",
    "                val.append(join_idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = join_idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val    \n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind] = ';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        # assign new SF_indexers for the new leads\n",
    "        new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "        new_idf['SF_indexer__c'] = range(int(max_ind)+1,new_max_ind+1)    \n",
    "\n",
    "        # need non-empty company field\n",
    "        new_idf['Company'] = '-'    \n",
    "\n",
    "        # ensure 'â\\x80\\x93' has been replaced\n",
    "        sf_df_match['Venue__c'] = sf_df_match['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "        new_idf['Venue__c'] = new_idf['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "\n",
    "        sf_df_match['Company'] = '-'\n",
    "        new_idf['Company'] = '-'   \n",
    "\n",
    "        # populate the company fields for sf_df_match and new_idf by comparing email addresses\n",
    "        if 'Company' in idf.columns:\n",
    "            print(\"company exist, using it\")\n",
    "            # comparison for sf_df_match\n",
    "\n",
    "            sf_df_match_comp_ind = sf_df_match.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in sf_df_match['Email'].to_list():\n",
    "                    sf_df_match_ind = sf_df_match['Email'].to_list().index(val)\n",
    "                    sf_df_match.iloc[sf_df_match_ind,sf_df_match_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "\n",
    "            # sf_df_match sf_indexer if not available\n",
    "            sf_df_match_sf_ind = sf_df_match.columns.to_list().index('SF_indexer__c')\n",
    "            new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "            for ind,val in enumerate(sf_df_match['SF_indexer__c'].to_list()):\n",
    "                if type(val) != int and type(val) != float:\n",
    "                    sf_df_match.iloc[ind,sf_df_match_sf_ind] = new_max_ind+1\n",
    "                    new_max_ind += 1\n",
    "\n",
    "            new_idf_comp_ind = new_idf.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in new_idf['Email'].to_list():\n",
    "                    new_idf_ind = new_idf['Email'].to_list().index(val)\n",
    "                    new_idf.iloc[new_idf_ind,new_idf_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "        else:\n",
    "            print('no company in import list, set to -')    \n",
    "        \n",
    "        if sf_df_match.shape[0] == 0 and new_idf.shape[0] == 0: \n",
    "            print('no leads to import, they were all contacts')\n",
    "        else:\n",
    "            sf_df_match = sf_df_match.fillna('-')\n",
    "            new_idf = new_idf.fillna('-')\n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf.rename(columns={'FirstName':'firstname','LastName':'lastname'})\n",
    "            except:\n",
    "                print('names are good')          \n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                new_idf['Company'] = '-'\n",
    "            except: # names are not present\n",
    "                try:\n",
    "                    tempnames = new_idf['Faculty'].apply(lambda x: x.split(' '))\n",
    "                    tf_names = [i[0] for i in tempnames]\n",
    "                    tl_names = [i[-1] if len(i[-1]) > 0 else '-' for i in tempnames]\n",
    "                    new_idf['firstname'] = tf_names\n",
    "                    new_idf['lastname'] = tl_names\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'    \n",
    "                except:\n",
    "                    tempnames = new_idf['Name'].to_list()\n",
    "                    temp_fname = []\n",
    "                    temp_lname = []\n",
    "                    for t_ind,t_val in enumerate(tempnames):\n",
    "                        t_val = t_val.split(' ')\n",
    "                        temp_fname.append(t_val[0])\n",
    "                        if len(t_val[-1]) > 0:\n",
    "                            temp_lname.append(t_val[-1])\n",
    "                        else:\n",
    "                            temp_lname.append('-')\n",
    "\n",
    "                    new_idf['firstname'] = temp_fname\n",
    "                    new_idf['lastname'] = temp_lname\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'\n",
    "\n",
    "            #drop duplicate rows\n",
    "            sf_df_match = sf_df_match.drop_duplicates(subset='SF_indexer__c')\n",
    "            new_idf = new_idf.drop_duplicates()    \n",
    "\n",
    "            sf_df_match['Company'] = sf_df_match['Company'].replace('  ','-')\n",
    "            sf_df_match = sf_df_match.drop(columns='SF_indexer__c')\n",
    "\n",
    "            #send the matching ones\n",
    "            db_s_l1 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l1.object_id = 'Lead'\n",
    "            # db_s_l1.external_id = 'SF_indexer__c'\n",
    "            db_s_l1.external_id = 'Id'\n",
    "\n",
    "            db_s_l1.send_data(sf_df_match)    \n",
    "\n",
    "            #send the new ones\n",
    "            db_s_l2 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l2.object_id = 'Lead'\n",
    "            db_s_l2.external_id = 'SF_indexer__c'\n",
    "\n",
    "            db_s_l2.send_data(new_idf)    \n",
    "\n",
    "        success_files.append(file)\n",
    "    except:\n",
    "        fail_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move success files from To_Import to Imported on Google Drive \n",
    "## move failure files from To_Import to Import_Issues on GDrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Erin R1 Faculty Scrape 2021.csv']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd_imp_list = tbd_imp_files['files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd_imp_names = [i['name'] for i in tbd_imp_list] \n",
    "tbd_imp_ids = [i['id'] for i in tbd_imp_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## match success and failures to their appropriate ids\n",
    "success_fids = []\n",
    "failure_fids = []\n",
    "\n",
    "for i in success_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    success_fids.append(tbd_imp_ids[t_index])#[0][0]])\n",
    "\n",
    "for i in fail_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    failure_fids.append(tbd_imp_ids[t_index])#[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1e0n-6Kl1pUcIGajvd_QcCbr_Xuk2HdZB']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failure_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start with success (To_Import -> Imported)\n",
    "for sid in success_fids:\n",
    "    file_mv = service.files().get(fileId=sid,\n",
    "                                     fields='parents').execute()\n",
    "    previous_parents = \",\".join(file_mv.get('parents'))\n",
    "    \n",
    "    file_mv_complete = service.files().update(fileId=sid,\n",
    "                                        addParents=success_id,\n",
    "                                        removeParents=previous_parents,\n",
    "                                        fields='id, parents').execute()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now do failures (To_Import -> Import_Issues)\n",
    "for fid in fail_files:\n",
    "    file_mv = service.files().get(fileId=fid,\n",
    "                                     fields='parents').execute()\n",
    "    previous_parents = \",\".join(file_mv.get('parents'))\n",
    "    \n",
    "    file_mv_complete = service.files().update(fileId=fid,\n",
    "                                        addParents=failure_id,\n",
    "                                        removeParents=previous_parents,\n",
    "                                        fields='id, parents').execute()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove files from local directory\n",
    "for file in list_files:\n",
    "    os.remove(cwd+'/To_Import/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
