{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general_import.py transposed to a .ipynb file for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:28.643129Z",
     "iopub.status.busy": "2021-10-28T23:28:28.642494Z",
     "iopub.status.idle": "2021-10-28T23:28:28.655859Z",
     "shell.execute_reply": "2021-10-28T23:28:28.654582Z",
     "shell.execute_reply.started": "2021-10-28T23:28:28.642999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:29.116578Z",
     "iopub.status.busy": "2021-10-28T23:28:29.116333Z",
     "iopub.status.idle": "2021-10-28T23:28:30.620776Z",
     "shell.execute_reply": "2021-10-28T23:28:30.618964Z",
     "shell.execute_reply.started": "2021-10-28T23:28:29.116553Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mnanoHUB - Serving Students, Researchers & Instructors\u001B[0m\n",
      "Obtained Salesforce access token ...... True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from nanoHUB.application import Application\n",
    "from googleapiclient.discovery import build\n",
    "from apiclient import errors\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from httplib2 import Http\n",
    "from nanoHUB.logger import logger\n",
    "application = Application.get_instance()\n",
    "# nanohub_db = application.new_db_engine('nanohub')\n",
    "# nanohub_metrics_db = application.new_db_engine('nanohub_metrics')\n",
    "# wang159_myrmekes_db = application.new_db_engine('wang159_myrmekes')\n",
    "\n",
    "salesforce = application.new_salesforce_engine()\n",
    "db_s = salesforce\n",
    "log = logger('nanoHUB:google_imports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GDrive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:30.624185Z",
     "iopub.status.busy": "2021-10-28T23:28:30.623136Z",
     "iopub.status.idle": "2021-10-28T23:28:30.635880Z",
     "shell.execute_reply": "2021-10-28T23:28:30.634235Z",
     "shell.execute_reply.started": "2021-10-28T23:28:30.624141Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import os\n",
    "import errno\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "## stuff that's rather hard to find from documentation\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:30.639684Z",
     "iopub.status.busy": "2021-10-28T23:28:30.638952Z",
     "iopub.status.idle": "2021-10-28T23:28:30.669163Z",
     "shell.execute_reply": "2021-10-28T23:28:30.667605Z",
     "shell.execute_reply.started": "2021-10-28T23:28:30.639649Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'Salesforce_Imports'\n",
    "TO_IMPORT_FOLDER_NAME = 'To_Import'\n",
    "FAILURES_FOLDER_NAME = 'Import_Issues'\n",
    "IMPORTED_FOLDER_NAME = 'Imported'\n",
    "\n",
    "service = application.new_google_api_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:40.851895Z",
     "iopub.status.busy": "2021-10-28T23:28:40.850643Z",
     "iopub.status.idle": "2021-10-28T23:28:40.875945Z",
     "shell.execute_reply": "2021-10-28T23:28:40.873877Z",
     "shell.execute_reply.started": "2021-10-28T23:28:40.851697Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import_dir = os.getenv('APP_DIR') + '/.cache/SF_Imports'\n",
    "Path(import_dir + '/' + FOLDER_NAME).mkdir(parents=True, exist_ok=True)\n",
    "Path(import_dir + '/' + TO_IMPORT_FOLDER_NAME).mkdir(parents=True, exist_ok=True)\n",
    "Path(import_dir + '/' + FAILURES_FOLDER_NAME).mkdir(parents=True, exist_ok=True)\n",
    "Path(import_dir + '/' + IMPORTED_FOLDER_NAME).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:43.126063Z",
     "iopub.status.busy": "2021-10-28T23:28:43.125644Z",
     "iopub.status.idle": "2021-10-28T23:28:43.141563Z",
     "shell.execute_reply": "2021-10-28T23:28:43.139633Z",
     "shell.execute_reply.started": "2021-10-28T23:28:43.126017Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_folder_id(service, folder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + folder_name + \"'\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def get_subfolder_id(service, parent_folder_id: str, subfolder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + subfolder_name + \"'\" + \" and '\" + parent_folder_id + \"' in parents\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def change_folder_for_file(file_id: str, old_folder_id: str, new_folder_id: str):\n",
    "    return service.files().update(\n",
    "        fileId=file_id,\n",
    "        removeParents=old_folder_id,\n",
    "        addParents=new_folder_id,\n",
    "        fields='id, parents'\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:44.789687Z",
     "iopub.status.busy": "2021-10-28T23:28:44.789344Z",
     "iopub.status.idle": "2021-10-28T23:28:45.378164Z",
     "shell.execute_reply": "2021-10-28T23:28:45.376771Z",
     "shell.execute_reply.started": "2021-10-28T23:28:44.789643Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [191429788 - nanoHUB:google_imports]: Found folder: Salesforce_Imports (1xwtRsGEcBNW7LItcgoekxxZADk7Ghy-d) [191429788.<module>:2]\n"
     ]
    }
   ],
   "source": [
    "folder_id = get_folder_id(service, FOLDER_NAME)\n",
    "log.info('Found folder: %s (%s)' % (FOLDER_NAME, folder_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:47.941797Z",
     "iopub.status.busy": "2021-10-28T23:28:47.940151Z",
     "iopub.status.idle": "2021-10-28T23:28:48.179464Z",
     "shell.execute_reply": "2021-10-28T23:28:48.177375Z",
     "shell.execute_reply.started": "2021-10-28T23:28:47.941724Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [285691732 - nanoHUB:google_imports]: Found subfolder: To_Import (1vjnRjaPcEeK82Ye3_BmjMEmXU51Y9FL-) [285691732.<module>:2]\n"
     ]
    }
   ],
   "source": [
    "subfolder_id = get_subfolder_id(service, folder_id, TO_IMPORT_FOLDER_NAME)\n",
    "log.info('Found subfolder: %s (%s)' % (TO_IMPORT_FOLDER_NAME, subfolder_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:53.517665Z",
     "iopub.status.busy": "2021-10-28T23:28:53.517398Z",
     "iopub.status.idle": "2021-10-28T23:28:53.752750Z",
     "shell.execute_reply": "2021-10-28T23:28:53.751287Z",
     "shell.execute_reply.started": "2021-10-28T23:28:53.517629Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2186677928 - nanoHUB:google_imports]: Found file: ML Webinar 11 03 21 - R.csv (1AlQZK8mRPRDX-WuvzwMM58-9aTPatl7r) [2186677928.<module>:20]\n"
     ]
    }
   ],
   "source": [
    "mimetypes = \"\"\"\n",
    "mimeType = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' or\n",
    "mimeType = 'application/vnd.ms-excel' or\n",
    "mimeType = 'text/csv'\n",
    "\"\"\"\n",
    "file_ids = []\n",
    "file_names = []\n",
    "page_token = None\n",
    "# q = \"(\" + mimetypes + \") and '\" + folder.get('id') + \"' in parents\"\n",
    "while True:\n",
    "    query = \"(\" + mimetypes + \") and '\" + subfolder_id + \"' in parents\"\n",
    "    response = service.files().list(\n",
    "        q = query,\n",
    "        spaces='drive',\n",
    "        pageToken=page_token,\n",
    "        fields=\"nextPageToken, files(id, name)\"\n",
    "    ).execute()\n",
    "    files = response.get('files', [])\n",
    "    for file in files:\n",
    "        log.info('Found file: %s (%s)' % (file.get('name'), file.get('id')))\n",
    "        file_names.append(file['name'])\n",
    "        file_ids.append(file['id'])\n",
    "    page_token = response.get('nextPageToken', None)\n",
    "    if page_token is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:58.333713Z",
     "iopub.status.busy": "2021-10-28T23:28:58.332815Z",
     "iopub.status.idle": "2021-10-28T23:28:58.342876Z",
     "shell.execute_reply": "2021-10-28T23:28:58.340051Z",
     "shell.execute_reply.started": "2021-10-28T23:28:58.333664Z"
    }
   },
   "outputs": [],
   "source": [
    "import io \n",
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:28:59.154070Z",
     "iopub.status.busy": "2021-10-28T23:28:59.152808Z",
     "iopub.status.idle": "2021-10-28T23:28:59.644989Z",
     "shell.execute_reply": "2021-10-28T23:28:59.643021Z",
     "shell.execute_reply.started": "2021-10-28T23:28:59.153981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2159645865 - nanoHUB:google_imports]: Download 100%. [2159645865.<module>:9]\n",
      "[INFO] [2159645865 - nanoHUB:google_imports]: <_io.BufferedWriter name='/home/saxenap/nanoHUB/.cache/SF_Imports/ML Webinar 11 03 21 - R.csv'> [2159645865.<module>:23]\n",
      "[INFO] [2159645865 - nanoHUB:google_imports]: Finished downloading files [2159645865.<module>:26]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for temp_index,f_tbd_id in enumerate(file_ids):\n",
    "        request = service.files().get_media(fileId=f_tbd_id) #,mimeType='text/csv') #if not .csv, then do .export()\n",
    "        fh = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "            log.info(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "\n",
    "        # The file has been downloaded into RAM, now save it in a file\n",
    "        # https://stackoverflow.com/questions/60111361/how-to-download-a-file-from-google-drive-using-python-and-the-drive-api-v3\n",
    "        fh.seek(0)\n",
    "        download_filepath = import_dir + '/' + file_names[temp_index]\n",
    "        if not os.path.exists(os.path.dirname(download_filepath)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(download_filepath))\n",
    "            except OSError as exc:\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "        with open(download_filepath, 'wb') as f:\n",
    "            log.info(f)\n",
    "            shutil.copyfileobj(fh, f) #, length=131072)\n",
    "\n",
    "    log.info(\"Finished downloading files\")\n",
    "except Exception as e:\n",
    "    log.info(\"Error downloading file: \" + str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:29:01.861233Z",
     "iopub.status.busy": "2021-10-28T23:29:01.860911Z",
     "iopub.status.idle": "2021-10-28T23:29:01.872339Z",
     "shell.execute_reply": "2021-10-28T23:29:01.870470Z",
     "shell.execute_reply.started": "2021-10-28T23:29:01.861194Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:29:02.830550Z",
     "iopub.status.busy": "2021-10-28T23:29:02.830055Z",
     "iopub.status.idle": "2021-10-28T23:29:02.855733Z",
     "shell.execute_reply": "2021-10-28T23:29:02.851354Z",
     "shell.execute_reply.started": "2021-10-28T23:29:02.830462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2122226748 - nanoHUB:google_imports]: ['Import_Issues', 'Imported', 'ML Bayesian Optimization Webinar â€“ A.csv', 'ML Webinar 11 03 21 - R.csv', 'Salesforce_Imports', 'To_Import'] [2122226748.<module>:2]\n"
     ]
    }
   ],
   "source": [
    "list_files = [name for name in os.listdir(import_dir)] #if os.path.isfile(name)]\n",
    "log.info(list_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sequential Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:29:04.455849Z",
     "iopub.status.busy": "2021-10-28T23:29:04.455513Z",
     "iopub.status.idle": "2021-10-28T23:29:04.465766Z",
     "shell.execute_reply": "2021-10-28T23:29:04.462506Z",
     "shell.execute_reply.started": "2021-10-28T23:29:04.455803Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:29:05.295077Z",
     "iopub.status.busy": "2021-10-28T23:29:05.294842Z",
     "iopub.status.idle": "2021-10-28T23:29:05.304074Z",
     "shell.execute_reply": "2021-10-28T23:29:05.301235Z",
     "shell.execute_reply.started": "2021-10-28T23:29:05.295047Z"
    }
   },
   "outputs": [],
   "source": [
    "success_files = []\n",
    "fail_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:29:05.896141Z",
     "iopub.status.busy": "2021-10-28T23:29:05.895893Z",
     "iopub.status.idle": "2021-10-28T23:29:05.906484Z",
     "shell.execute_reply": "2021-10-28T23:29:05.903705Z",
     "shell.execute_reply.started": "2021-10-28T23:29:05.896112Z"
    }
   },
   "outputs": [],
   "source": [
    "from chardet import detect\n",
    "import cchardet\n",
    "# get file encoding type\n",
    "def get_encoding_type(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-28T23:29:06.723236Z",
     "iopub.status.busy": "2021-10-28T23:29:06.722978Z",
     "iopub.status.idle": "2021-10-28T23:29:07.235258Z",
     "shell.execute_reply": "2021-10-28T23:29:07.233681Z",
     "shell.execute_reply.started": "2021-10-28T23:29:06.723190Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1881040374 - nanoHUB:google_imports]: \n",
      " [1881040374.<module>:4]\n",
      "[INFO] [1881040374 - nanoHUB:google_imports]: Processing file: ML Webinar 11 03 21 - R.csv----> [1881040374.<module>:7]\n",
      "[INFO] [1881040374 - nanoHUB:google_imports]: From chardet ----> Windows-1252 [1881040374.<module>:10]\n",
      "[INFO] [1881040374 - nanoHUB:google_imports]: From Normalizer ----> cp1250 [1881040374.<module>:13]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from charset_normalizer import CharsetNormalizerMatches as CnM\n",
    "\n",
    "\n",
    "log.info(\"\\n\")\n",
    "for file in file_names:\n",
    "    file_path = import_dir + '/' + file\n",
    "    log.info(\"Processing file: \" + file + '---->')\n",
    "\n",
    "    from_chardet = get_encoding_type(file_path)\n",
    "    log.info('From chardet ----> ' + from_chardet['encoding'])\n",
    "\n",
    "    from_normalizer = CnM.from_path(file_path).best().first().encoding\n",
    "    log.info('From Normalizer ----> ' + from_normalizer)\n",
    "\n",
    "\n",
    "\n",
    "from charset_normalizer import from_path\n",
    "\n",
    "def change_encoding(file_path: str):\n",
    "    try:\n",
    "        results = from_path(file_path)\n",
    "        with open(file_path, 'w') as filetowrite:\n",
    "            filetowrite.write(results)\n",
    "    except IOError as e:\n",
    "        log.info('Sadly, we are unable to perform charset normalization.', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-28T23:29:07.961673Z",
     "iopub.status.busy": "2021-10-28T23:29:07.960731Z",
     "iopub.status.idle": "2021-10-28T23:30:26.161368Z",
     "shell.execute_reply": "2021-10-28T23:30:26.160061Z",
     "shell.execute_reply.started": "2021-10-28T23:29:07.961606Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [790826062 - nanoHUB:google_imports]: Reading ML Webinar 11 03 21 - R.csv [790826062.<module>:3]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: File type csv [790826062.<module>:6]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: Loaded csv with encoding cp1252 now. [790826062.<module>:15]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: Index(['FirstName  ', 'LastName  ', 'Email  ', 'Invited  ', 'Registered  ',\n",
      "       'Attended  ', 'Registration Status  ', 'Lead Source ID  ',\n",
      "       'Registration Date/Time  ', 'Registration ID  ', 'Registration Score  ',\n",
      "       'Okay to send email  ', 'Title  ', 'Number of Employees  ', 'Company  ',\n",
      "       'Phone  ', 'Address 1  ', 'Address 2  ', 'City  ', 'State/Province  ',\n",
      "       'Postal/Zip Code  ', 'Country/Region  ',\n",
      "       'How did you hear about this webinar?  ',\n",
      "       'What is your primary motivation for attending this tutorial?  ',\n",
      "       'Are you running nanoHUB in a classroom?  ',\n",
      "       'We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?  ',\n",
      "       'Which of the following do you use?  '],\n",
      "      dtype='object') [790826062.<module>:58]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: Index(['FirstName', 'LastName', 'Email', 'Invited', 'Registered', 'Attended',\n",
      "       'Registration Status', 'Lead Source ID', 'Registration Date/Time',\n",
      "       'Registration ID', 'Registration Score', 'Okay to send email', 'Title',\n",
      "       'Number of Employees', 'Company', 'Phone', 'Address 1', 'Address 2',\n",
      "       'City', 'State/Province', 'Postal/Zip Code', 'Country/Region',\n",
      "       'How did you hear about this webinar?',\n",
      "       'What is your primary motivation for attending this tutorial?',\n",
      "       'Are you running nanoHUB in a classroom?',\n",
      "       'We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?',\n",
      "       'Which of the following do you use?'],\n",
      "      dtype='object') [790826062.<module>:62]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: no name [790826062.<module>:97]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: Index(['firstname', 'lastname', 'Email', 'Invited', 'Registered', 'Attended',\n",
      "       'Registration Status', 'Lead Source ID', 'Registration Date/Time',\n",
      "       'Registration ID', 'Registration Score', 'Okay to send email', 'Title',\n",
      "       'Number of Employees', 'Company', 'Phone', 'Address 1', 'Address 2',\n",
      "       'City', 'State/Province', 'Postal/Zip Code', 'Country/Region',\n",
      "       'How did you hear about this webinar?',\n",
      "       'What is your primary motivation for attending this tutorial?',\n",
      "       'Are you running nanoHUB in a classroom?',\n",
      "       'We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?',\n",
      "       'Which of the following do you use?', 'Venue__c'],\n",
      "      dtype='object') [790826062.<module>:110]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: 193 [790826062.<module>:121]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: 0 [790826062.<module>:122]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b2lJbAAI\n",
      "{\"id\":\"7505w00000b2lJbAAI\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-10-28T23:29:08.000+0000\",\"systemModstamp\":\"2021-10-28T23:29:09.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000b2lJbAAI\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-10-28T23:29:08.000+0000\",\"systemModstamp\":\"2021-10-28T23:29:09.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000b2lJbAAI\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-10-28T23:29:08.000+0000\",\"systemModstamp\":\"2021-10-28T23:29:09.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000b2lJbAAI\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-10-28T23:29:08.000+0000\",\"systemModstamp\":\"2021-10-28T23:29:36.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":15116,\"retries\":0,\"totalProcessingTime\":1050}\n",
      "{\"id\":\"7505w00000b2lJbAAI\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-10-28T23:29:08.000+0000\",\"systemModstamp\":\"2021-10-28T23:29:36.000+0000\",\"state\":\"InProgress\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":15116,\"retries\":0,\"totalProcessingTime\":1050}\n",
      "{\"id\":\"7505w00000b2lJbAAI\",\"operation\":\"query\",\"object\":\"Contact\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-10-28T23:29:08.000+0000\",\"systemModstamp\":\"2021-10-28T23:29:54.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":262858,\"retries\":0,\"totalProcessingTime\":15830}\n",
      "[Success] Bulk job completed successfully.\n",
      "[INFO] [790826062 - nanoHUB:google_imports]:                     Email                  Id  \\\n",
      "0     jverduzc@purdue.edu  0035w000034K05AAAS   \n",
      "1  musazay.abdu@gmail.com  0035w00003THDQZAA5   \n",
      "\n",
      "                                            Venue__c  nanoHUB_user_ID__c  \n",
      "0  Citrine Material Science workshop - R;Citrine ...            207041.0  \n",
      "1                            ML Webinar 11 03 21 - R            332248.0   [790826062.<module>:162]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]:      firstname           lastname                   Email Invited Registered  \\\n",
      "0  Juan Carlos  Verduzco Gastelum     jverduzc@purdue.edu      No        Yes   \n",
      "1  Abdurrahman            Musazay  musazay.abdu@gmail.com      No        Yes   \n",
      "\n",
      "  Attended Registration Status Lead Source ID  \\\n",
      "0       No            Approved                  \n",
      "1       No            Approved                  \n",
      "\n",
      "                   Registration Date/Time  Registration ID  \\\n",
      "0  October 14, 2021 4:04 pm New York Time           741121   \n",
      "1  October 12, 2021 9:53 pm New York Time           531550   \n",
      "\n",
      "   Registration Score Okay to send email Title  Number of Employees Company  \\\n",
      "0                   0                 No                        NaN           \n",
      "1                   0                 No                        NaN           \n",
      "\n",
      "  Phone Address 1 Address 2    City State/Province Postal/Zip Code  \\\n",
      "0    1-                                                              \n",
      "1    1-                      Munich                                  \n",
      "\n",
      "             Country/Region How did you hear about this webinar?  \\\n",
      "0  United States of America                   nanoHUB Newsletter   \n",
      "1                   Germany                                email   \n",
      "\n",
      "  What is your primary motivation for attending this tutorial?  \\\n",
      "0  To gain skills for my own research|To learn ab...             \n",
      "1  To gain skills for my own research|Personal In...             \n",
      "\n",
      "  Are you running nanoHUB in a classroom?  \\\n",
      "0                                      No   \n",
      "1                                     Yes   \n",
      "\n",
      "  We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?  \\\n",
      "0                                              Slack                                                                                                                        \n",
      "1                                 Discord|Email list                                                                                                                        \n",
      "\n",
      "   Which of the following do you use?                 Venue__c  \n",
      "0                                 NaN  ML Webinar 11 03 21 - R  \n",
      "1                                 NaN  ML Webinar 11 03 21 - R   [790826062.<module>:163]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]:                       Email                  Id  \\\n",
      "0       jverduzc@purdue.edu  0035w000034K05AAAS   \n",
      "1    musazay.abdu@gmail.com  0035w00003THDQZAA5   \n",
      "2     moha.rahjoo@gmail.com  0035w00003TJw0pAAD   \n",
      "3           m.rabie@gmx.net  0035w00003BRU25AAH   \n",
      "4  siddhipatil.rp@gmail.com  0035w00003R9lPfAAJ   \n",
      "\n",
      "                                            Venue__c  nanoHUB_user_ID__c  \n",
      "0  Citrine Material Science workshop - R;Citrine ...            207041.0  \n",
      "1    ML Webinar 11 03 21 - R;ML Webinar 11 03 21 - R            332248.0  \n",
      "2    ML Webinar 11 03 21 - R;ML Webinar 11 03 21 - R            333589.0  \n",
      "3  Citrine Material Science Workshop - S;ML Webin...            289988.0  \n",
      "4  ML Aug 2021 Webinar - A;ML Webinar 062121 - A;...            329178.0   [790826062.<module>:175]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b2lJlAAI\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000b2lJlAAI\n",
      "[Success] Closing job successful. Job ID = 7505w00000b2lJlAAI\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b2lJqAAI\n",
      "{\"id\":\"7505w00000b2lJqAAI\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-10-28T23:30:11.000+0000\",\"systemModstamp\":\"2021-10-28T23:30:11.000+0000\",\"state\":\"UploadComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"retries\":0,\"totalProcessingTime\":0}\n",
      "{\"id\":\"7505w00000b2lJqAAI\",\"operation\":\"query\",\"object\":\"Lead\",\"createdById\":\"0055w00000DM5bOAAT\",\"createdDate\":\"2021-10-28T23:30:11.000+0000\",\"systemModstamp\":\"2021-10-28T23:30:13.000+0000\",\"state\":\"JobComplete\",\"concurrencyMode\":\"Parallel\",\"contentType\":\"CSV\",\"apiVersion\":47.0,\"jobType\":\"V2Query\",\"lineEnding\":\"LF\",\"columnDelimiter\":\"COMMA\",\"numberRecordsProcessed\":9826,\"retries\":0,\"totalProcessingTime\":1083}\n",
      "[Success] Bulk job completed successfully.\n",
      "[INFO] [790826062 - nanoHUB:google_imports]:        firstname    lastname                       Email Invited Registered  \\\n",
      "0        Tanya A     Faltens         tafaltens@gmail.com     Yes        Yes   \n",
      "1           Erin     Hartman    emrcherry46947@gmail.com      No        Yes   \n",
      "2           Kang        Wang  kang.wang18@imperial.ac.uk      No        Yes   \n",
      "3       Danielle    Alverson           dalverson@ufl.edu      No        Yes   \n",
      "4             Xi          Xu              xx220@ic.ac.uk      No        Yes   \n",
      "..           ...         ...                         ...     ...        ...   \n",
      "56  Dinesh Varma    Mudunuri    varmadinesh339@gmail.com      No        Yes   \n",
      "57        Meryem      Lamari      meryemlamari@gmail.com      No        Yes   \n",
      "58     Oleksandr  Mashchenko             mashco00@jcu.cz      No        Yes   \n",
      "59       Hyunsoo         Lee       hyunsol@g.clemson.edu      No        Yes   \n",
      "60          Geng        Yuan            gxy102@miami.edu      No        Yes   \n",
      "\n",
      "   Attended Registration Status Lead Source ID  \\\n",
      "0        No            Approved                  \n",
      "1        No            Approved                  \n",
      "2        No            Approved                  \n",
      "3        No            Approved                  \n",
      "4        No            Approved                  \n",
      "..      ...                 ...            ...   \n",
      "56       No            Approved                  \n",
      "57       No            Approved                  \n",
      "58       No            Approved                  \n",
      "59       No            Approved                  \n",
      "60       No            Approved                  \n",
      "\n",
      "                     Registration Date/Time  Registration ID  \\\n",
      "0    October 11, 2021 2:52 pm New York Time           504711   \n",
      "1    October 11, 2021 2:36 pm New York Time           628222   \n",
      "2    October 21, 2021 1:09 pm New York Time           846900   \n",
      "3   October 17, 2021 11:26 am New York Time           210255   \n",
      "4    October 21, 2021 9:18 am New York Time           601556   \n",
      "..                                      ...              ...   \n",
      "56   October 12, 2021 5:56 pm New York Time           226325   \n",
      "57  October 21, 2021 10:56 am New York Time           766654   \n",
      "58   October 13, 2021 5:29 am New York Time           870159   \n",
      "59   October 12, 2021 4:25 pm New York Time           608399   \n",
      "60   October 12, 2021 4:32 pm New York Time           731622   \n",
      "\n",
      "    Registration Score Okay to send email  \\\n",
      "0                    0                 No   \n",
      "1                    0                 No   \n",
      "2                    0                 No   \n",
      "3                    0                 No   \n",
      "4                    0                 No   \n",
      "..                 ...                ...   \n",
      "56                   0                 No   \n",
      "57                   0                 No   \n",
      "58                   0                 No   \n",
      "59                   0                 No   \n",
      "60                   0                 No   \n",
      "\n",
      "                                   Title  Number of Employees Company Phone  \\\n",
      "0   Educational Content Creation Manager                  NaN       -    1-   \n",
      "1                                  Admin                  NaN       -    1-   \n",
      "2                                                         NaN       -    1-   \n",
      "3                                                         NaN       -    1-   \n",
      "4                                  Miss                   NaN       -    1-   \n",
      "..                                   ...                  ...     ...   ...   \n",
      "56                               Student                  NaN       -    1-   \n",
      "57                                  Miss                  NaN       -    1-   \n",
      "58                                   MSc                  NaN       -    1-   \n",
      "59                                                        NaN       -    1-   \n",
      "60                                                        NaN       -    1-   \n",
      "\n",
      "   Address 1 Address 2            City  State/Province Postal/Zip Code  \\\n",
      "0                       West Lafayette              IN                   \n",
      "1                       West Lafayette              IN                   \n",
      "2                               London          London                   \n",
      "3                          Gainesville         Florida                   \n",
      "4                                                                        \n",
      "..       ...       ...             ...             ...             ...   \n",
      "56                              Sydney             Nsw                   \n",
      "57                             London                                    \n",
      "58                                                                       \n",
      "59                                      South Carolina                   \n",
      "60                                                                       \n",
      "\n",
      "              Country/Region How did you hear about this webinar?  \\\n",
      "0   United States of America                     From a colleague   \n",
      "1   United States of America                     From a colleague   \n",
      "2             United Kingdom                                email   \n",
      "3   United States of America                     From a colleague   \n",
      "4             United Kingdom                                email   \n",
      "..                       ...                                  ...   \n",
      "56                 Australia                   nanoHUB Newsletter   \n",
      "57            United Kingdom                     From a colleague   \n",
      "58            Czech Republic                   nanoHUB Newsletter   \n",
      "59  United States of America                                email   \n",
      "60  United States of America                     From a colleague   \n",
      "\n",
      "   What is your primary motivation for attending this tutorial?  \\\n",
      "0                                   Personal Interest             \n",
      "1   Personal Interest|To learn about this specific...             \n",
      "2                  To gain skills for my own research             \n",
      "3   To gain skills for my own research|To learn ab...             \n",
      "4                  To gain skills for my own research             \n",
      "..                                                ...             \n",
      "56  To gain skills for my own research|To gain ski...             \n",
      "57  To gain skills for my own research|To gain ski...             \n",
      "58  To gain skills for my own research|To gain ski...             \n",
      "59  To gain skills for my own research|Personal In...             \n",
      "60                    To learn about Machine Learning             \n",
      "\n",
      "   Are you running nanoHUB in a classroom?  \\\n",
      "0                                       No   \n",
      "1                                       No   \n",
      "2                                       No   \n",
      "3                                       No   \n",
      "4                                       No   \n",
      "..                                     ...   \n",
      "56                                      No   \n",
      "57                                      No   \n",
      "58                                      No   \n",
      "59                                      No   \n",
      "60                                      No   \n",
      "\n",
      "   We are looking for ways to continue discussion after the training. Which of the following communication methods would you prefer to use (you can select more than one)?  \\\n",
      "0                                 nanoHUB Group Forum                                                                                                                        \n",
      "1                                               Slack                                                                                                                        \n",
      "2                                         Google chat                                                                                                                        \n",
      "3                                               Slack                                                                                                                        \n",
      "4                                     Microsoft Teams                                                                                                                        \n",
      "..                                                ...                                                                                                                        \n",
      "56                                    Microsoft Teams                                                                                                                        \n",
      "57                                    Microsoft Teams                                                                                                                        \n",
      "58                                 Discord|Email list                                                                                                                        \n",
      "59                     nanoHUB Group Forum|Email list                                                                                                                        \n",
      "60                          Slack|nanoHUB Group Forum                                                                                                                        \n",
      "\n",
      "    Which of the following do you use?                 Venue__c  SF_indexer__c  \n",
      "0                                  NaN  ML Webinar 11 03 21 - R           6155  \n",
      "1                                  NaN  ML Webinar 11 03 21 - R           6156  \n",
      "2                                  NaN  ML Webinar 11 03 21 - R           6157  \n",
      "3                                  NaN  ML Webinar 11 03 21 - R           6158  \n",
      "4                                  NaN  ML Webinar 11 03 21 - R           6159  \n",
      "..                                 ...                      ...            ...  \n",
      "56                                 NaN  ML Webinar 11 03 21 - R           6211  \n",
      "57                                 NaN  ML Webinar 11 03 21 - R           6212  \n",
      "58                                 NaN  ML Webinar 11 03 21 - R           6213  \n",
      "59                                 NaN  ML Webinar 11 03 21 - R           6214  \n",
      "60                                 NaN  ML Webinar 11 03 21 - R           6215  \n",
      "\n",
      "[61 rows x 29 columns] [790826062.<module>:264]\n",
      "[INFO] [790826062 - nanoHUB:google_imports]: company exist, using it [790826062.<module>:269]\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b2lK0AAI\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000b2lK0AAI\n",
      "[Success] Closing job successful. Job ID = 7505w00000b2lK0AAI\n",
      "[Success] Bulk job creation successful. Job ID = 7505w00000b2lK5AAI\n",
      "hello\n",
      "[Success] CSV upload successful. Job ID = 7505w00000b2lK5AAI\n",
      "[Success] Closing job successful. Job ID = 7505w00000b2lK5AAI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in file_names:\n",
    "    idf = pd.DataFrame()\n",
    "    log.info(\"Reading %s\" % file)\n",
    "    try:\n",
    "        f_type = file.split('.')[-1]\n",
    "        log.info(\"File type %s\" % f_type)\n",
    "        #pandas import # add exception handling - UnicodeError\n",
    "        if f_type == 'csv':\n",
    "            try:\n",
    "                idf = pd.read_csv(import_dir + '/' +file,encoding='utf-8')\n",
    "                log.info('Loaded csv with encoding utf-8.')\n",
    "            except:\n",
    "                try:\n",
    "                    idf = pd.read_csv(import_dir + '/' +file,encoding='cp1252')\n",
    "                    log.info('Loaded csv with encoding cp1252 now.')\n",
    "                except:\n",
    "                    try:\n",
    "                        idf = pd.read_csv(import_dir + '/' +file,encoding='utf-16')\n",
    "                        log.info('Loaded csv with encoding utf-16 now.')\n",
    "                    except:\n",
    "                        try:\n",
    "                            idf = pd.read_csv(import_dir + '/' +file,encoding='cp1254')\n",
    "                            log.info('Loaded csv with encoding cp1254 now.')\n",
    "                        except:\n",
    "                            try:\n",
    "                                idf = pd.read_csv(import_dir + '/' +file,encoding='cp775')\n",
    "                                log.info('Loaded csv with encoding cp775 now.')\n",
    "                            except:\n",
    "                                try:\n",
    "                                    idf = pd.read_csv(import_dir + '/' +file,encoding='cp1252',sep='\\t')\n",
    "                                    log.info('Loaded csv with encoding cp1252 now.')\n",
    "                                except:\n",
    "                                    try:\n",
    "                                        idf = pd.read_csv(import_dir + '/' +file,encoding='utf-16',sep='\\t')\n",
    "                                        log.info('Loaded csv with encoding utf-16 now.')\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            idf = pd.read_csv(import_dir + '/' +file,encoding='cp1254',sep='\\t')\n",
    "                                            log.info('Loaded csv with encoding cp1254 now.')\n",
    "                                        except:\n",
    "                                            try:\n",
    "                                                idf = pd.read_csv(import_dir + '/' +file,encoding='cp775',sep='\\t')\n",
    "                                                log.info('Loaded csv with encoding cp775 now.')\n",
    "                                            except:\n",
    "                                                log.info('CSV import failed. Possible encoding issues with file %s.' % file)\n",
    "                                                raise TypeError\n",
    "        elif f_type == 'xlsx' or f_type == 'xls':\n",
    "            try:\n",
    "                xl = pd.ExcelFile(import_dir + '/' +file)\n",
    "                log.info(\"sheet names: \" + xl.sheet_names)# see all sheet names\n",
    "                sheet_names = xl.parse(xl.sheet_names) #this already performs an import\n",
    "                idf = pd.read_excel(import_dir + '/' +file,sheet_name=xl.sheet_names[0],header=0)#,skiprows=1)\n",
    "            except:\n",
    "                log.info('error bad lines, csv/xls/xlsx import failed')\n",
    "                raise TypeError\n",
    "\n",
    "        ## remove leading and trailing spaces ## add str.strip spaces for all columns and rename\n",
    "        log.info(idf.columns)\n",
    "        prev_idf_cols = idf.columns\n",
    "        idf_cols = [i.strip() for i in idf.columns]\n",
    "        idf.columns = idf.columns.str.strip()\n",
    "        log.info(idf.columns)\n",
    "        idf = idf.rename(columns={j:idf_cols[i] for i,j in enumerate(prev_idf_cols)})\n",
    "    #     display(idf.columns)\n",
    "    #     # log.info(prev_idf_cols)\n",
    "    #     # log.info(idf_cols)\n",
    "\n",
    "        # if engagement venue does not exist, then create a flag with that entry\n",
    "        try:\n",
    "            #if engagement venue is specified\n",
    "            log.info(idf['Engagement Venue'])\n",
    "            log.info(idf['First Name'])\n",
    "            idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "            # idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "        except:\n",
    "            #decide the event\n",
    "            #event_extract = xl.sheet_names[0]\n",
    "            #idf['Engagement Venue'] = event_extract\n",
    "            idf['Venue__c'] = file.split('.')[0]\n",
    "\n",
    "            try:\n",
    "                #name extract\n",
    "                names = idf['Name'].to_list()\n",
    "                from copy import deepcopy?\n",
    "                fname = deepcopy(names)\n",
    "                lname = deepcopy(names)\n",
    "                for ind, val in enumerate(names):\n",
    "                    val = val.split(' ')\n",
    "                    fname[ind] = val[-1]\n",
    "                    lname[ind] = val[0]\n",
    "\n",
    "                idf['firstname'] = fname\n",
    "                idf['lastname'] = lname\n",
    "                name_flag = True\n",
    "            except:\n",
    "                name_flag = False\n",
    "                log.info('no name')\n",
    "\n",
    "        #rename columns\n",
    "        idf = idf.rename(columns={'email':'Email','EMAIL':'Email','E-mail Address':'Email',\\\n",
    "                    'Email Address':'Email','Recipient Email':'Email'})\n",
    "        idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "        idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname','FirstName':'firstname','LastName':'lastname'})\n",
    "\n",
    "        idf = idf.rename(columns={0:'Email'})\n",
    "        if name_flag == True:\n",
    "            idf = idf.drop(columns='Name')#['NAME','LAST NAME','FIRST NAME'])            \n",
    "        idf = idf.dropna(subset=['Email'])\n",
    "\n",
    "        log.info(idf.columns)\n",
    "        #email check rows\n",
    "        grows = []\n",
    "        brows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            if '@' in val:\n",
    "                grows.append(ind)\n",
    "            else:\n",
    "                brows.append(ind)    \n",
    "        idf = idf.iloc[grows,:].reset_index().iloc[:,1:]        \n",
    "\n",
    "        log.info(len(grows))\n",
    "        log.info(len(brows))\n",
    "\n",
    "        ## Import in contacts\n",
    "        os_name = os.name\n",
    "        sys_name = platform.system() #Linux, Darwin, Windows    \n",
    "\n",
    "        # salesforce queries for contact data\n",
    "        # deciding the queries\n",
    "        import_df_cols = deepcopy(idf.columns)\n",
    "        nh_id_flag = False\n",
    "        email_flag = False\n",
    "        if 'nanoHUB_user_ID__c' in import_df_cols:\n",
    "            nh_id_flag = True\n",
    "\n",
    "        if 'Email' in import_df_cols:\n",
    "            email_flag = True    \n",
    "\n",
    "        if nh_id_flag == True and email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)\n",
    "        elif email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)    \n",
    "\n",
    "        # find all existing contacts\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        grows = []\n",
    "        brows = [] #dont need the sf_bad_rows as send to leads\n",
    "        sf_grows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                grows.append(ind)\n",
    "                sf_grows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                brows.append(ind)   \n",
    "\n",
    "        # pull the matching SF entries and the matching import df entries\n",
    "        sf_df_match = sf_df.iloc[sf_grows,:].reset_index().iloc[:,1:]\n",
    "        idf_match = idf.iloc[grows,:].reset_index().iloc[:,1:]\n",
    "        lead_df = idf.iloc[brows,:].reset_index().iloc[:,1:] #use this in next section\n",
    "\n",
    "        log.info(sf_df_match.head(2))\n",
    "        log.info(idf_match.head(2))\n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                val.append(idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val\n",
    "\n",
    "        log.info(sf_df_match.head(5))\n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind]=';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## encoding correction for dashes\n",
    "        venues = sf_df_match['Venue__c'].apply(lambda x: x.replace('Ã¢\\x80\\x93','-'))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        sf_df_match = sf_df_match.drop_duplicates()\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        sf_df_match = sf_df_match[['Email','Venue__c','Id']]\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## send to SF\n",
    "        # rebuild api object\n",
    "        db_s_c = deepcopy(db_s)\n",
    "\n",
    "        # send data to SF\n",
    "        db_s_c.object_id = 'Contact'\n",
    "        # db_s_c.external_id = 'nanoHUB_user_ID__c'\n",
    "        db_s_c.external_id = 'Id'\n",
    "\n",
    "        db_s_c.send_data(sf_df_match)\n",
    "\n",
    "        ## find leads and send them to SF as well\n",
    "        #pull all current leads\n",
    "        sf_df = db_s.query_data('SELECT Id, Email, Venue__c, SF_indexer__c FROM Lead')    \n",
    "        # find the max sf_indexer\n",
    "        indexers = sf_df['SF_indexer__c'].fillna(0).to_list()\n",
    "        max_ind = max(indexers)    \n",
    "\n",
    "        # find all existing leads\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        m_rows = []\n",
    "        nm_rows = [] #don't need sf no match rows\n",
    "        sf_mrows = []    \n",
    "\n",
    "        for ind,val in enumerate(lead_df['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                m_rows.append(ind)\n",
    "                sf_mrows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                nm_rows.append(ind)    \n",
    "\n",
    "        # filter the matches\n",
    "        sf_df_match = sf_df.iloc[sf_mrows,:].reset_index().iloc[:,1:]\n",
    "        join_idf = lead_df.iloc[m_rows,:].reset_index().iloc[:,1:]\n",
    "        new_idf = lead_df.iloc[nm_rows,:].reset_index().iloc[:,1:]    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                #if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #    val.remove('MSE Summer Webinar Series 2020')\n",
    "                #    if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #        val.remove('MSE Summer Webinar Series 2020')\n",
    "                val.append(join_idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = join_idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val    \n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind] = ';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        # assign new SF_indexers for the new leads\n",
    "        new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "        new_idf['SF_indexer__c'] = range(int(max_ind)+1,new_max_ind+1)    \n",
    "\n",
    "        # need non-empty company field\n",
    "        new_idf['Company'] = '-'    \n",
    "\n",
    "        # ensure 'Ã¢\\x80\\x93' has been replaced\n",
    "        sf_df_match['Venue__c'] = sf_df_match['Venue__c'].apply(lambda x: x.replace('Ã¢\\x80\\x93','-'))\n",
    "        new_idf['Venue__c'] = new_idf['Venue__c'].apply(lambda x: x.replace('Ã¢\\x80\\x93','-'))\n",
    "\n",
    "        sf_df_match['Company'] = '-'\n",
    "        new_idf['Company'] = '-'\n",
    "        log.info(new_idf)\n",
    "        # sys.exit()\n",
    "\n",
    "        # populate the company fields for sf_df_match and new_idf by comparing email addresses\n",
    "        if 'Company' in idf.columns:\n",
    "            log.info(\"company exist, using it\")\n",
    "            # comparison for sf_df_match\n",
    "\n",
    "            sf_df_match_comp_ind = sf_df_match.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in sf_df_match['Email'].to_list():\n",
    "                    sf_df_match_ind = sf_df_match['Email'].to_list().index(val)\n",
    "                    sf_df_match.iloc[sf_df_match_ind,sf_df_match_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "\n",
    "            # sf_df_match sf_indexer if not available\n",
    "            sf_df_match_sf_ind = sf_df_match.columns.to_list().index('SF_indexer__c')\n",
    "            new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "            for ind,val in enumerate(sf_df_match['SF_indexer__c'].to_list()):\n",
    "                if type(val) != int and type(val) != float:\n",
    "                    sf_df_match.iloc[ind,sf_df_match_sf_ind] = new_max_ind+1\n",
    "                    new_max_ind += 1\n",
    "\n",
    "            new_idf_comp_ind = new_idf.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in new_idf['Email'].to_list():\n",
    "                    new_idf_ind = new_idf['Email'].to_list().index(val)\n",
    "                    new_idf.iloc[new_idf_ind,new_idf_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "        else:\n",
    "            log.info('no company in import list, set to -')    \n",
    "        \n",
    "        if sf_df_match.shape[0] == 0 and new_idf.shape[0] == 0: \n",
    "            log.info('no leads to import, they were all contacts')\n",
    "        else:\n",
    "            sf_df_match = sf_df_match.fillna('-')\n",
    "            new_idf = new_idf.fillna('-')\n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "            except:\n",
    "                log.info('names are good')          \n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                new_idf['Company'] = '-'\n",
    "            except: # names are not present\n",
    "                try:\n",
    "                    tempnames = new_idf['Faculty'].apply(lambda x: x.split(' '))\n",
    "                    tf_names = [i[0] for i in tempnames]\n",
    "                    tl_names = [i[-1] if len(i[-1]) > 0 else '-' for i in tempnames]\n",
    "                    new_idf['firstname'] = tf_names\n",
    "                    new_idf['lastname'] = tl_names\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'    \n",
    "                except:\n",
    "                    tempnames = new_idf['Name'].to_list()\n",
    "                    temp_fname = []\n",
    "                    temp_lname = []\n",
    "                    for t_ind,t_val in enumerate(tempnames):\n",
    "                        t_val = t_val.split(' ')\n",
    "                        temp_fname.append(t_val[0])\n",
    "                        if len(t_val[-1]) > 0:\n",
    "                            temp_lname.append(t_val[-1])\n",
    "                        else:\n",
    "                            temp_lname.append('-')\n",
    "\n",
    "                    new_idf['firstname'] = temp_fname\n",
    "                    new_idf['lastname'] = temp_lname\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'\n",
    "\n",
    "            #drop duplicate rows\n",
    "            sf_df_match = sf_df_match.drop_duplicates(subset='SF_indexer__c')\n",
    "            new_idf = new_idf.drop_duplicates()    \n",
    "\n",
    "            sf_df_match['Company'] = sf_df_match['Company'].replace('  ','-')\n",
    "            sf_df_match = sf_df_match.drop(columns='SF_indexer__c')\n",
    "\n",
    "            #send the matching ones\n",
    "            db_s_l1 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l1.object_id = 'Lead'\n",
    "            # db_s_l1.external_id = 'SF_indexer__c'\n",
    "            db_s_l1.external_id = 'Id'\n",
    "\n",
    "            db_s_l1.send_data(sf_df_match)\n",
    "\n",
    "            #send the new ones\n",
    "            db_s_l2 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l2.object_id = 'Lead'\n",
    "            db_s_l2.external_id = 'SF_indexer__c'\n",
    "\n",
    "            db_s_l2.send_data(new_idf)\n",
    "\n",
    "        success_files.append(file)\n",
    "    except Exception as e:\n",
    "        log.info(\"Error with file: \" + str(e))\n",
    "        fail_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:30:26.167291Z",
     "iopub.status.busy": "2021-10-28T23:30:26.166463Z",
     "iopub.status.idle": "2021-10-28T23:30:26.174885Z",
     "shell.execute_reply": "2021-10-28T23:30:26.170868Z",
     "shell.execute_reply.started": "2021-10-28T23:30:26.167238Z"
    }
   },
   "outputs": [],
   "source": [
    "## move success files from To_Import to Imported on Google Drive \n",
    "## move failure files from To_Import to Import_Issues on GDrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:30:26.176689Z",
     "iopub.status.busy": "2021-10-28T23:30:26.176266Z",
     "iopub.status.idle": "2021-10-28T23:30:26.196337Z",
     "shell.execute_reply": "2021-10-28T23:30:26.194877Z",
     "shell.execute_reply.started": "2021-10-28T23:30:26.176653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2124149726 - nanoHUB:google_imports]: success files:  [2124149726.<module>:1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ML Webinar 11 03 21 - R.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.info(\"success files: \")\n",
    "success_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:30:26.198462Z",
     "iopub.status.busy": "2021-10-28T23:30:26.198086Z",
     "iopub.status.idle": "2021-10-28T23:30:26.209514Z",
     "shell.execute_reply": "2021-10-28T23:30:26.208376Z",
     "shell.execute_reply.started": "2021-10-28T23:30:26.198420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1178869520 - nanoHUB:google_imports]: fail files:  [1178869520.<module>:1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.info(\"fail files: \")\n",
    "fail_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:30:56.193612Z",
     "iopub.status.busy": "2021-10-28T23:30:56.192819Z",
     "iopub.status.idle": "2021-10-28T23:30:56.198669Z",
     "shell.execute_reply": "2021-10-28T23:30:56.197617Z",
     "shell.execute_reply.started": "2021-10-28T23:30:56.193405Z"
    }
   },
   "outputs": [],
   "source": [
    "tbd_imp_names = file_names\n",
    "tbd_imp_ids = file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:30:57.402571Z",
     "iopub.status.busy": "2021-10-28T23:30:57.402069Z",
     "iopub.status.idle": "2021-10-28T23:30:57.411047Z",
     "shell.execute_reply": "2021-10-28T23:30:57.408081Z",
     "shell.execute_reply.started": "2021-10-28T23:30:57.402522Z"
    }
   },
   "outputs": [],
   "source": [
    "## match success and failures to their appropriate ids\n",
    "success_fids = []\n",
    "failure_fids = []\n",
    "\n",
    "for i in success_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    success_fids.append(tbd_imp_ids[t_index])#[0][0]])\n",
    "\n",
    "for i in fail_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    failure_fids.append(tbd_imp_ids[t_index])#[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:30:59.760465Z",
     "iopub.status.busy": "2021-10-28T23:30:59.759153Z",
     "iopub.status.idle": "2021-10-28T23:30:59.778988Z",
     "shell.execute_reply": "2021-10-28T23:30:59.777386Z",
     "shell.execute_reply.started": "2021-10-28T23:30:59.760405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [3776719133 - nanoHUB:google_imports]: success file ids:  [3776719133.<module>:1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1AlQZK8mRPRDX-WuvzwMM58-9aTPatl7r']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.info(\"success file ids: \")\n",
    "success_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:31:00.640559Z",
     "iopub.status.busy": "2021-10-28T23:31:00.638410Z",
     "iopub.status.idle": "2021-10-28T23:31:00.662656Z",
     "shell.execute_reply": "2021-10-28T23:31:00.661606Z",
     "shell.execute_reply.started": "2021-10-28T23:31:00.640400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2311033698 - nanoHUB:google_imports]: failure file ids:  [2311033698.<module>:1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.info(\"failure file ids: \")\n",
    "failure_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:31:02.108249Z",
     "iopub.status.busy": "2021-10-28T23:31:02.106682Z",
     "iopub.status.idle": "2021-10-28T23:31:03.259874Z",
     "shell.execute_reply": "2021-10-28T23:31:03.257469Z",
     "shell.execute_reply.started": "2021-10-28T23:31:02.108051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [753651360 - nanoHUB:google_imports]: Found imported sub-folder: Imported (1ujcpSpj_Uj00VAENMU4OEm5KJiszBCb5) [753651360.<module>:3]\n",
      "[INFO] [753651360 - nanoHUB:google_imports]: Deleting success file with id: 1AlQZK8mRPRDX-WuvzwMM58-9aTPatl7r [753651360.<module>:6]\n"
     ]
    }
   ],
   "source": [
    "## start with success (To_Import -> Imported)\n",
    "imported_folder_id = get_subfolder_id(service, folder_id, IMPORTED_FOLDER_NAME)\n",
    "log.info('Found imported sub-folder: %s (%s)' % (IMPORTED_FOLDER_NAME, imported_folder_id))\n",
    "\n",
    "for sid in success_fids:\n",
    "    log.info('Deleting success file with id: ' + sid)\n",
    "    change_folder_for_file(sid, subfolder_id, imported_folder_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:31:03.263514Z",
     "iopub.status.busy": "2021-10-28T23:31:03.263091Z",
     "iopub.status.idle": "2021-10-28T23:31:03.275782Z",
     "shell.execute_reply": "2021-10-28T23:31:03.270940Z",
     "shell.execute_reply.started": "2021-10-28T23:31:03.263463Z"
    }
   },
   "outputs": [],
   "source": [
    "## now do failures (To_Import -> Import_Issues)\n",
    "# failure_folder_id = get_subfolder_id(service, folder_id, FAILURES_FOLDER_NAME)\n",
    "# log.info('Found failure subfolder: %s (%s)' % (FAILURES_FOLDER_NAME, failure_folder_id))\n",
    "#\n",
    "# for fid in failure_fids:\n",
    "#     log.info('Deleting failed file with id: ' + fid)\n",
    "#     change_folder_for_file(fid, subfolder_id, failure_folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-28T23:31:04.653880Z",
     "iopub.status.busy": "2021-10-28T23:31:04.653413Z",
     "iopub.status.idle": "2021-10-28T23:31:04.792559Z",
     "shell.execute_reply": "2021-10-28T23:31:04.789927Z",
     "shell.execute_reply.started": "2021-10-28T23:31:04.653832Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/saxenap/nanoHUB/.cache/SF_Imports/Import_Issues'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIsADirectoryError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_70/3916154673.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# remove files from local directory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfile\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlist_files\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimport_dir\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'/'\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mIsADirectoryError\u001B[0m: [Errno 21] Is a directory: '/home/saxenap/nanoHUB/.cache/SF_Imports/Import_Issues'"
     ]
    }
   ],
   "source": [
    "# remove files from local directory\n",
    "for file in list_files:\n",
    "    os.remove(import_dir + '/' +file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}