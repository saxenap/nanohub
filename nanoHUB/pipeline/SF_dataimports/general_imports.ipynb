{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general_import.py transposed to a .ipynb file for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "```````````````````````````````$$$$$``````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "`````````````````````````````$$$$`$$$$````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "`````````````````$$$$$```````$$`````$$$```````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````````````$$```$$``````$$$$``$$$````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````````````$$$$$$$```````$$$$$$``````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "``````$$$$````````$$$$$```````$$$``````$$$$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````$$$$`$$$$```````$$$$$$$$$$$$$``````$$``$$$````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````$$`````$$$```$$$$$$$$$`$$$$$$$$``$$$$$$$$$````````````````````````````````````````````````````````$$$```````$$$$``$$$```````$$$```$$$$$$$$$$$`````\n",
      "````$$````$$$$``$$$$$``````````$$$$$$$$$$$$$$````````````````````````````````````````````````````````$$$$```````$$$$`$$$$```````$$$$`$$$$$$$$$$$$$$```\n",
      "`````$$$$$$$`$$$$$$```````````````$$$$```````````$$$$$$$``````$$$$$$$$`````$$$$$$$```````$$$$$$$`````$$$$```````$$$$`$$$$```````$$$$`$$$$``````$$$$```\n",
      "``````````````$$$`````$$$$$$$$`````$$$$````````$$$$$$$$$$$``$$$$$$$$$$$``$$$$$$$$$$$```$$$$$$$$$$$```$$$$``````$$$$$`$$$$```````$$$$`$$$$`````$$$$$```\n",
      "``````````````$$$`````$$````$$`````$$$$````````$$$`````$$$$````````$$$$$`$$$$````$$$$`$$$$`````$$$$``$$$$$$$$$$$$$$$`$$$$```````$$$$`$$$$$$$$$$$$$$```\n",
      "`````$$$$$```$$$$`````$$````$$`````$$$$````````$$$`````$$$$``$$$$$$$$$$$`$$$$````$$$$`$$$$`````$$$$``$$$$```````$$$$`$$$$```````$$$$`$$$$``````$$$$$``\n",
      "```$$$``$$$$$$$$$`````$$````$$`````$$$$````````$$$`````$$$$`$$$$````$$$$`$$$$````$$$$`$$$$`````$$$$``$$$$```````$$$$`$$$$$``````$$$``$$$$```````$$$$``\n",
      "````$$$`$$$````$$$````````````````$$$$`````````$$$`````$$$$`$$$$$$$$$$$``$$$$````$$$$``$$$$$$$$$$$```$$$$```````$$$$``$$$$$$$$$$$$$``$$$$$$$$$$$$$$```\n",
      "``````$$$```````$$$$$``````````$$$$$$``````````$$$`````$$$$``$$$$$$$$$```$$$$````$$$$````$$$$$$$`````$$$$```````$$$$````$$$$$$$$$````$$$$$$$$$$$$$````\n",
      "````````````````$$$$$$$$$```$$$$$$$$``````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "```````````$$$$$$$``$$$$$$$$$$$$```$$$$$$$$```````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````$$$$$$$$$````````$$$````````$$$``$$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "```````$$$````$$$````````$$$```````$$$````$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````$$````$$$``````$$$$$$$``````$$$``$$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "`````````$$$$$$````````$$$``$$````````$$$$````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "````````````````````````$$$$$`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "\n",
      "Serving Students, Researchers & Instructors\n",
      "\n",
      "Obtained Salesforce access token ...... True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from nanoHUB.application import Application\n",
    "from googleapiclient.discovery import build\n",
    "from apiclient import errors\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from httplib2 import Http\n",
    "from nanoHUB.logger import logger\n",
    "application = Application.get_instance()\n",
    "# nanohub_db = application.new_db_engine('nanohub')\n",
    "# nanohub_metrics_db = application.new_db_engine('nanohub_metrics')\n",
    "# wang159_myrmekes_db = application.new_db_engine('wang159_myrmekes')\n",
    "\n",
    "salesforce = application.new_salesforce_engine()\n",
    "db_s = salesforce\n",
    "log = logger('nanoHUB:google_imports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GDrive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "## stuff that's rather hard to find from documentation\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'Salesforce_Imports'\n",
    "TO_IMPORT_FOLDER_NAME = 'To_Import'\n",
    "TO_IMPORT_FOLDER_NAME = 'Test'\n",
    "FAILURES_FOLDER_NAME = 'Import_Issues'\n",
    "IMPORTED_FOLDER_NAME = 'Imported'\n",
    "\n",
    "import_dir = os.getenv('APP_DIR') + '/.cache/SF_Imports'\n",
    "\n",
    "service = application.new_google_api_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCN_master_id = '1OjlcHKkyKe9Uw_3iTFxIUafdtD1NNXKC' \n",
    "sf_import_master_id = '1Hg6UQmf5z2ZRqOwVnOHMqjLwZkZN-jOh' \n",
    "tbd_import_id = '1gIbrfPnNs8TZrhTarhWOaCl8Bts5PNX8' \n",
    "failure_id = '1WMU4A6IMQhtIZ4rehpFiKiyllc718U_1' \n",
    "success_id = '1lf2vTUWiZHLEhfzM8m3Tym4j9UGgdhMZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_folder_id(service, folder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + folder_name + \"'\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def get_subfolder_id(service, parent_folder_id: str, subfolder_name: str):\n",
    "    response = service.files().list(\n",
    "        q = \"mimeType = 'application/vnd.google-apps.folder' and name = '\" + subfolder_name + \"'\" + \" and '\" + parent_folder_id + \"' in parents\",\n",
    "        spaces='drive',\n",
    "        fields=\"files(id, name)\"\n",
    "    ).execute()\n",
    "    folder = response.get('files', [])[0]\n",
    "    return folder.get('id')\n",
    "\n",
    "def change_folder_for_file(file_id: str, old_folder_id: str, new_folder_id: str):\n",
    "    return service.files().update(\n",
    "        fileId=file_id,\n",
    "        removeParents=old_folder_id,\n",
    "        addParents=new_folder_id,\n",
    "        fields='id, parents'\n",
    "    ).execute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "folder_id = get_folder_id(service, FOLDER_NAME)\n",
    "log.info('Found folder: %s (%s)' % (FOLDER_NAME, folder_id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subfolder_id = get_subfolder_id(service, folder_id, TO_IMPORT_FOLDER_NAME)\n",
    "log.info('Found subfolder: %s (%s)' % (TO_IMPORT_FOLDER_NAME, subfolder_id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mimetypes = \"\"\"\n",
    "mimeType = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' or\n",
    "mimeType = 'application/vnd.ms-excel' or\n",
    "mimeType = 'text/csv'\n",
    "\"\"\"\n",
    "file_ids = []\n",
    "file_names = []\n",
    "page_token = None\n",
    "# q = \"(\" + mimetypes + \") and '\" + folder.get('id') + \"' in parents\"\n",
    "while True:\n",
    "    query = \"(\" + mimetypes + \") and '\" + subfolder_id + \"' in parents\"\n",
    "    response = service.files().list(\n",
    "        q = query,\n",
    "        spaces='drive',\n",
    "        pageToken=page_token,\n",
    "        fields=\"nextPageToken, files(id, name)\"\n",
    "    ).execute()\n",
    "    files = response.get('files', [])\n",
    "    for file in files:\n",
    "        log.info('Found file: %s (%s)' % (file.get('name'), file.get('id')))\n",
    "        file_names.append(file['name'])\n",
    "        file_ids.append(file['id'])\n",
    "    page_token = response.get('nextPageToken', None)\n",
    "    if page_token is None:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for temp_index,f_tbd_id in enumerate(file_ids):\n",
    "    request = service.files().get_media(fileId=f_tbd_id) #,mimeType='text/csv') #if not .csv, then do .export()\n",
    "    fh = io.BytesIO() \n",
    "    downloader = MediaIoBaseDownload(fh, request) \n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "        \n",
    "    # The file has been downloaded into RAM, now save it in a file\n",
    "    # https://stackoverflow.com/questions/60111361/how-to-download-a-file-from-google-drive-using-python-and-the-drive-api-v3\n",
    "    fh.seek(0)\n",
    "    with open(import_dir + '/' + file_names[temp_index], 'wb') as f:\n",
    "        print(f)\n",
    "        shutil.copyfileobj(fh, f) #, length=131072)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ML Debugging Neural Networks - A.csv']\n"
     ]
    }
   ],
   "source": [
    "list_files = [name for name in os.listdir(import_dir)] #if os.path.isfile(name)]\n",
    "print(list_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sequential Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_files = []\n",
    "fail_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_bad_lines, csv import failed\n"
     ]
    }
   ],
   "source": [
    "from chardet import detect\n",
    "import cchardet\n",
    "# get file encoding type\n",
    "def get_encoding_type(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from charset_normalizer import CharsetNormalizerMatches as CnM\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "for file in file_names:\n",
    "    file_path = import_dir + '/' + file\n",
    "    print(\"Processing file: \" + file + '---->')\n",
    "\n",
    "    from_chardet = get_encoding_type(file_path)\n",
    "    print('From chardet ----> ' + from_chardet['encoding'])\n",
    "\n",
    "    from_normalizer = CnM.from_path(file_path).best().first().encoding\n",
    "    print('From Normalizer ----> ' + from_normalizer)\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for file in file_names:\n",
    "    idf = pd.DataFrame()\n",
    "    try:\n",
    "        f_type = file.split('.')[-1] \n",
    "\n",
    "        #pandas import # add exception handling - UnicodeError\n",
    "        if f_type == 'csv':\n",
    "            try:\n",
    "                idf = pd.read_csv(import_dir + '/' +file,encoding='utf-8')\n",
    "                log.info('Loading csv with encoding utf-8 now.')\n",
    "            except:\n",
    "                try:\n",
    "                    idf = pd.read_csv(import_dir + '/' +file,encoding='cp1252')\n",
    "                    log.info('Loading csv with encoding cp1252 now.')\n",
    "                except:\n",
    "                    try:\n",
    "                        idf = pd.read_csv(import_dir + '/' +file,encoding='utf-16')\n",
    "                        log.info('Loading csv with encoding utf-16 now.')\n",
    "                    except:\n",
    "                        try:\n",
    "                            idf = pd.read_csv(import_dir + '/' +file,encoding='cp1252',sep='\\t')\n",
    "                            log.info('Loading csv with encoding cp1252 now.')\n",
    "                        except:\n",
    "                            try:\n",
    "                                idf = pd.read_csv(import_dir + '/' +file,encoding='utf-16',sep='\\t')\n",
    "                                log.info('Loading csv with encoding utf-16 now.')\n",
    "                            except:\n",
    "                                print('error_bad_lines, csv import failed')\n",
    "                                raise TypeError\n",
    "        elif f_type == 'xlsx' or f_type == 'xls':\n",
    "            try:\n",
    "                xl = pd.ExcelFile(import_dir + '/' +file)\n",
    "                log.info(\"sheet names: \" + xl.sheet_names)# see all sheet names\n",
    "                sheet_names = xl.parse(xl.sheet_names) #this already performs an import\n",
    "                idf = pd.read_excel(import_dir + '/' +file,sheet_name=xl.sheet_names[0],header=0)#,skiprows=1)\n",
    "            except:\n",
    "                log.info('error bad lines, csv/xls/xlsx import failed')\n",
    "                raise TypeError\n",
    "\n",
    "        ## remove leading and trailing spaces ## add str.strip spaces for all columns and rename\n",
    "        prev_idf_cols = idf.columns\n",
    "        idf_cols = [i.strip() for i in idf.columns]\n",
    "\n",
    "        idf = idf.rename(columns={j:idf_cols[i] for i,j in enumerate(prev_idf_cols)})\n",
    "    #     display(idf.columns)\n",
    "    #     # print(prev_idf_cols)\n",
    "    #     # print(idf_cols)\n",
    "\n",
    "        # if engagement venue does not exist, then create a flag with that entry\n",
    "        try:\n",
    "            #if engagement venue is specified\n",
    "            log.info(idf['Engagement Venue'])\n",
    "            log.info(idf['First Name'])\n",
    "            idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "            # idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "        except:\n",
    "            #decide the event\n",
    "            #event_extract = xl.sheet_names[0]\n",
    "            #idf['Engagement Venue'] = event_extract\n",
    "            idf['Venue__c'] = file.split('.')[0]\n",
    "\n",
    "            try:\n",
    "                #name extract\n",
    "                names = idf['Name'].to_list()\n",
    "                from copy import deepcopy?\n",
    "                fname = deepcopy(names)\n",
    "                lname = deepcopy(names)\n",
    "                for ind, val in enumerate(names):\n",
    "                    val = val.split(' ')\n",
    "                    fname[ind] = val[-1]\n",
    "                    lname[ind] = val[0]\n",
    "\n",
    "                idf['firstname'] = fname\n",
    "                idf['lastname'] = lname\n",
    "                name_flag = True\n",
    "            except:\n",
    "                name_flag = False\n",
    "                log.info('no name')\n",
    "\n",
    "        #rename columns\n",
    "        idf = idf.rename(columns={'email':'Email','EMAIL':'Email','E-mail Address':'Email',\\\n",
    "                    'Email Address':'Email','Recipient Email':'Email'})\n",
    "        idf = idf.rename(columns={'Engagement Venue':'Venue__c'})\n",
    "        idf = idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "\n",
    "        idf = idf.rename(columns={0:'Email'})\n",
    "        if name_flag == True:\n",
    "            idf = idf.drop(columns='Name')#['NAME','LAST NAME','FIRST NAME'])            \n",
    "        idf = idf.dropna(subset=['Email'])\n",
    "\n",
    "        #email check rows\n",
    "        grows = []\n",
    "        brows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            if '@' in val:\n",
    "                grows.append(ind)\n",
    "            else:\n",
    "                brows.append(ind)    \n",
    "        idf = idf.iloc[grows,:].reset_index().iloc[:,1:]        \n",
    "\n",
    "        print(len(grows))\n",
    "        print(len(brows))\n",
    "\n",
    "        ## Import in contacts\n",
    "        os_name = os.name\n",
    "        sys_name = platform.system() #Linux, Darwin, Windows    \n",
    "\n",
    "        # salesforce queries for contact data\n",
    "        # deciding the queries\n",
    "        import_df_cols = deepcopy(idf.columns)\n",
    "        nh_id_flag = False\n",
    "        email_flag = False\n",
    "        if 'nanoHUB_user_ID__c' in import_df_cols:\n",
    "            nh_id_flag = True\n",
    "\n",
    "        if 'Email' in import_df_cols:\n",
    "            email_flag = True    \n",
    "\n",
    "        if nh_id_flag == True and email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)\n",
    "        elif email_flag == True:\n",
    "            sf_df = db_s.query_data('SELECT Id,nanoHUB_user_ID__c, Email, Venue__c FROM Contact')#,sys_name=sys_name)    \n",
    "\n",
    "        # find all existing contacts\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        grows = []\n",
    "        brows = [] #dont need the sf_bad_rows as send to leads\n",
    "        sf_grows = []\n",
    "        for ind,val in enumerate(idf['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                grows.append(ind)\n",
    "                sf_grows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                brows.append(ind)   \n",
    "\n",
    "        # pull the matching SF entries and the matching import df entries\n",
    "        sf_df_match = sf_df.iloc[sf_grows,:].reset_index().iloc[:,1:]\n",
    "        idf_match = idf.iloc[grows,:].reset_index().iloc[:,1:]\n",
    "        lead_df = idf.iloc[brows,:].reset_index().iloc[:,1:] #use this in next section\n",
    "\n",
    "    #     print(sf_df_match.head(2))\n",
    "    #     print(idf_match.head(2))    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                val.append(idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val\n",
    "\n",
    "    #     print(sf_df_match.head(5))\n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind]=';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## encoding correction for dashes\n",
    "        venues = sf_df_match['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        sf_df_match = sf_df_match.drop_duplicates()\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        sf_df_match = sf_df_match[['Email','Venue__c','Id']]\n",
    "    #     display(sf_df_match.head(5))\n",
    "\n",
    "        ## send to SF\n",
    "        # rebuild api object\n",
    "        db_s_c = deepcopy(db_s)\n",
    "\n",
    "        # send data to SF\n",
    "        db_s_c.object_id = 'Contact'\n",
    "        # db_s_c.external_id = 'nanoHUB_user_ID__c'\n",
    "        db_s_c.external_id = 'Id'\n",
    "\n",
    "        db_s_c.send_data(sf_df_match)\n",
    "\n",
    "        ## find leads and send them to SF as well\n",
    "        #pull all current leads\n",
    "        sf_df = db_s.query_data('SELECT Id, Email, Venue__c, SF_indexer__c FROM Lead')    \n",
    "        # find the max sf_indexer\n",
    "        indexers = sf_df['SF_indexer__c'].fillna(0).to_list()\n",
    "        max_ind = max(indexers)    \n",
    "\n",
    "        # find all existing leads\n",
    "        sf_emails = sf_df['Email'].to_list()\n",
    "        m_rows = []\n",
    "        nm_rows = [] #don't need sf no match rows\n",
    "        sf_mrows = []    \n",
    "\n",
    "        for ind,val in enumerate(lead_df['Email'].to_list()):\n",
    "            val = val.strip()\n",
    "            if val in sf_emails:\n",
    "                m_rows.append(ind)\n",
    "                sf_mrows.append(sf_emails.index(val))\n",
    "            else:\n",
    "                nm_rows.append(ind)    \n",
    "\n",
    "        # filter the matches\n",
    "        sf_df_match = sf_df.iloc[sf_mrows,:].reset_index().iloc[:,1:]\n",
    "        join_idf = lead_df.iloc[m_rows,:].reset_index().iloc[:,1:]\n",
    "        new_idf = lead_df.iloc[nm_rows,:].reset_index().iloc[:,1:]    \n",
    "\n",
    "        # linear join since the sequence is matching\n",
    "        for ind,val in enumerate(sf_df_match['Venue__c']):\n",
    "            try:\n",
    "                val = val.split(';')\n",
    "                #if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #    val.remove('MSE Summer Webinar Series 2020')\n",
    "                #    if 'MSE Summer Webinar Series 2020' in val:\n",
    "                #        val.remove('MSE Summer Webinar Series 2020')\n",
    "                val.append(join_idf['Venue__c'][ind])\n",
    "                val = ';'.join(val)\n",
    "            except:\n",
    "                val = join_idf['Venue__c'][ind]\n",
    "            sf_df_match['Venue__c'][ind] = val    \n",
    "\n",
    "        ## delete duplicates\n",
    "        venues = sf_df_match['Venue__c'].to_list()\n",
    "        for ind,val in enumerate(venues):\n",
    "            venues[ind] = ';'.join(list(dict.fromkeys(val.split(';'))))\n",
    "        sf_df_match['Venue__c'] = venues\n",
    "\n",
    "        # assign new SF_indexers for the new leads\n",
    "        new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "        new_idf['SF_indexer__c'] = range(int(max_ind)+1,new_max_ind+1)    \n",
    "\n",
    "        # need non-empty company field\n",
    "        new_idf['Company'] = '-'    \n",
    "\n",
    "        # ensure 'â\\x80\\x93' has been replaced\n",
    "        sf_df_match['Venue__c'] = sf_df_match['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "        new_idf['Venue__c'] = new_idf['Venue__c'].apply(lambda x: x.replace('â\\x80\\x93','-'))\n",
    "\n",
    "        sf_df_match['Company'] = '-'\n",
    "        new_idf['Company'] = '-'\n",
    "        print(new_idf)\n",
    "        sys.exit()\n",
    "\n",
    "        # populate the company fields for sf_df_match and new_idf by comparing email addresses\n",
    "        if 'Company' in idf.columns:\n",
    "            print(\"company exist, using it\")\n",
    "            # comparison for sf_df_match\n",
    "\n",
    "            sf_df_match_comp_ind = sf_df_match.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in sf_df_match['Email'].to_list():\n",
    "                    sf_df_match_ind = sf_df_match['Email'].to_list().index(val)\n",
    "                    sf_df_match.iloc[sf_df_match_ind,sf_df_match_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "\n",
    "            # sf_df_match sf_indexer if not available\n",
    "            sf_df_match_sf_ind = sf_df_match.columns.to_list().index('SF_indexer__c')\n",
    "            new_max_ind = int(max_ind+new_idf.shape[0])\n",
    "            for ind,val in enumerate(sf_df_match['SF_indexer__c'].to_list()):\n",
    "                if type(val) != int and type(val) != float:\n",
    "                    sf_df_match.iloc[ind,sf_df_match_sf_ind] = new_max_ind+1\n",
    "                    new_max_ind += 1\n",
    "\n",
    "            new_idf_comp_ind = new_idf.columns.to_list().index('Company')\n",
    "            for ind,val in enumerate(idf['Email'].to_list()):\n",
    "                if val in new_idf['Email'].to_list():\n",
    "                    new_idf_ind = new_idf['Email'].to_list().index(val)\n",
    "                    new_idf.iloc[new_idf_ind,new_idf_comp_ind] = deepcopy(idf['Company'].to_list()[ind])\n",
    "        else:\n",
    "            print('no company in import list, set to -')    \n",
    "        \n",
    "        if sf_df_match.shape[0] == 0 and new_idf.shape[0] == 0: \n",
    "            print('no leads to import, they were all contacts')\n",
    "        else:\n",
    "            sf_df_match = sf_df_match.fillna('-')\n",
    "            new_idf = new_idf.fillna('-')\n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf.rename(columns={'First Name':'firstname','Last Name':'lastname'})\n",
    "            except:\n",
    "                print('names are good')          \n",
    "\n",
    "            try:\n",
    "                new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                new_idf['Company'] = '-'\n",
    "            except: # names are not present\n",
    "                try:\n",
    "                    tempnames = new_idf['Faculty'].apply(lambda x: x.split(' '))\n",
    "                    tf_names = [i[0] for i in tempnames]\n",
    "                    tl_names = [i[-1] if len(i[-1]) > 0 else '-' for i in tempnames]\n",
    "                    new_idf['firstname'] = tf_names\n",
    "                    new_idf['lastname'] = tl_names\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'    \n",
    "                except:\n",
    "                    tempnames = new_idf['Name'].to_list()\n",
    "                    temp_fname = []\n",
    "                    temp_lname = []\n",
    "                    for t_ind,t_val in enumerate(tempnames):\n",
    "                        t_val = t_val.split(' ')\n",
    "                        temp_fname.append(t_val[0])\n",
    "                        if len(t_val[-1]) > 0:\n",
    "                            temp_lname.append(t_val[-1])\n",
    "                        else:\n",
    "                            temp_lname.append('-')\n",
    "\n",
    "                    new_idf['firstname'] = temp_fname\n",
    "                    new_idf['lastname'] = temp_lname\n",
    "                    new_idf = new_idf[['Email','firstname','lastname','SF_indexer__c','Venue__c']]\n",
    "                    new_idf['Company'] = '-'\n",
    "\n",
    "            #drop duplicate rows\n",
    "            sf_df_match = sf_df_match.drop_duplicates(subset='SF_indexer__c')\n",
    "            new_idf = new_idf.drop_duplicates()    \n",
    "\n",
    "            sf_df_match['Company'] = sf_df_match['Company'].replace('  ','-')\n",
    "            sf_df_match = sf_df_match.drop(columns='SF_indexer__c')\n",
    "\n",
    "            #send the matching ones\n",
    "            db_s_l1 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l1.object_id = 'Lead'\n",
    "            # db_s_l1.external_id = 'SF_indexer__c'\n",
    "            db_s_l1.external_id = 'Id'\n",
    "\n",
    "            db_s_l1.send_data(sf_df_match)\n",
    "\n",
    "            #send the new ones\n",
    "            db_s_l2 = deepcopy(db_s)\n",
    "\n",
    "            # send data to SF\n",
    "            db_s_l2.object_id = 'Lead'\n",
    "            db_s_l2.external_id = 'SF_indexer__c'\n",
    "\n",
    "            db_s_l2.send_data(new_idf)\n",
    "\n",
    "        success_files.append(file)\n",
    "    except Exception as e:\n",
    "        log.info(\"Error with file: \" + str(e))\n",
    "        fail_files.append(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move success files from To_Import to Imported on Google Drive \n",
    "## move failure files from To_Import to Import_Issues on GDrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"success files: \")\n",
    "success_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ML Debugging Neural Networks - A.csv']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"fail files: \")\n",
    "fail_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd_imp_names = file_names\n",
    "tbd_imp_ids = file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## match success and failures to their appropriate ids\n",
    "success_fids = []\n",
    "failure_fids = []\n",
    "\n",
    "for i in success_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    success_fids.append(tbd_imp_ids[t_index])#[0][0]])\n",
    "\n",
    "for i in fail_files:\n",
    "    t_index = tbd_imp_names.index(i) #np.where(i in tbd_imp_names)\n",
    "    failure_fids.append(tbd_imp_ids[t_index])#[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"success file ids: \")\n",
    "success_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1eZNoNM8kUIeuAnSaa9opZTS330ljC4Ct']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"failure file ids: \")\n",
    "failure_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start with success (To_Import -> Imported)\n",
    "imported_folder_id = get_subfolder_id(service, folder_id, IMPORTED_FOLDER_NAME)\n",
    "log.info('Found imported sub-folder: %s (%s)' % (IMPORTED_FOLDER_NAME, imported_folder_id))\n",
    "\n",
    "for sid in success_fids:\n",
    "    print('Deleting success file with id: ' + sid)\n",
    "    change_folder_for_file(sid, subfolder_id, imported_folder_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now do failures (To_Import -> Import_Issues)\n",
    "# failure_folder_id = get_subfolder_id(service, folder_id, FAILURES_FOLDER_NAME)\n",
    "# log.info('Found failure subfolder: %s (%s)' % (FAILURES_FOLDER_NAME, failure_folder_id))\n",
    "#\n",
    "# for fid in failure_fids:\n",
    "#     print('Deleting failed file with id: ' + fid)\n",
    "#     change_folder_for_file(fid, subfolder_id, failure_folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove files from local directory\n",
    "for file in list_files:\n",
    "    os.remove(import_dir + '/' +file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}